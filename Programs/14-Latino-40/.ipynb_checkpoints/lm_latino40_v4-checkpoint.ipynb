{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de lenguaje de Latino 40\n",
    "\n",
    "Vamos a hacer el archivo que necesitamos para el reconocimiento con el HTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from NLPUtils import *\n",
    "import re\n",
    "import fasttext\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de lenguaje frecuentista\n",
    "\n",
    "Creamos el archivo `lm_l40_bigram_train_test` dentro de la carpeta `lm_files` con el programa *ngram-count* del *SRILM*. Este archivo contiene la información del modelo de lenguaje estimado para un modelo de bigramas con el set de entrenamiento y de test de la base de datos Latino-40.\n",
    "\n",
    "El archivo `vocab` consta de todas las palabras de la base de datos de test. La instrucción utilizada a continuación cuenta la cantidad de bigramas en un archivo `trainLM2.txt` y con eso estima la probabilidad del modelo de lenguaje. El parámetro `-order 2` indica que se implementa un modelo de bigramas y el parametro `-unkdiscount2` indica que se usa el método de suavizado de Kneser-Ney."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: discount coeff 1 is out of range: -0\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/speechapp/srilm/bin/i686-m64/ngram-count -order 2 -text trainLM2.txt -lm ./lm_files/lm_l40_bigram_train_test -ukndiscount2 -vocab vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la perplejidad sobre el corpus de test para este modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El logaritmo de la perplejidad para el corpus de test es: -1.3650002757790303\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el modelo de lenguaje del método frecuentista:\n",
    "with open('lm_l40_2', 'rb') as file:\n",
    "    lm_file = file.read().decode('iso-8859-1')\n",
    "    \n",
    "def get_log_prob(w1,w2):\n",
    "    match = re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({} {})'.format(w1,w2),lm_file)\n",
    "    if match is not None:\n",
    "        log_prob, _ = match.groups()\n",
    "        return float(log_prob)\n",
    "    match_w1 = re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({})\\t*(\\-?[\\d]*[\\.]?[\\d]*)'.format(w1),lm_file)\n",
    "    match_w2 = re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({})\\t*(\\-?[\\d]*[\\.]?[\\d]*)'.format(w2),lm_file)\n",
    "    return float(match_w1.groups()[0]) + float(match_w2.groups()[2])   \n",
    "        \n",
    "# Juntamos el corpus de test en una sola lista:\n",
    "with open('promptsl40.test','rb') as file:\n",
    "    test_lines = file.readlines()\n",
    "    test_lines = [' '.join(re.findall(r'\\w+',line.decode('iso-8859-1'))[1:]) for line in test_lines]\n",
    "    corpus_test = [['<s>'] + line.split(' ') + ['</s>'] for line in test_lines]\n",
    "    corpus_test = [word for line in corpus_test for word in line]\n",
    "    \n",
    "# Perplejidad para un modelo de bigrama:\n",
    "corpus_len = len(corpus_test)\n",
    "log_p = [get_log_prob(corpus_test[idx-1],corpus_test[idx]) for idx in range(1,corpus_len)]\n",
    "log_p.insert(0,float(re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({})\\t*(\\-?[\\d]*[\\.]?[\\d]*)'.format(corpus_test[0]),lm_file).groups()[0]))\n",
    "print('El logaritmo de la perplejidad para el corpus de test es: {}'.format(sum(log_p)/corpus_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar resultados del reconocimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de lenguaje neuronal\n",
    "\n",
    "Vamos a entrenar un modelo de lenguaje con una red neuronal a partir de las frases de entrenamiento y de test, las cuales se encuentran en el archivo `trainLM2.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los bigramas que tenemos que entrenar:\n",
    "with open('promptsl40.test','rb') as file:\n",
    "    test_lines = file.readlines()\n",
    "    test_lines = [' '.join(re.findall(r'\\w+',line.decode('iso-8859-1'))[1:]) for line in test_lines]\n",
    "corpus_test = [['<s>'] + line.split(' ') + ['</s>'] for line in test_lines]\n",
    "corpus_test = [word for line in corpus_test for word in line]\n",
    "corpus_test_len = len(corpus_test)\n",
    "bigrams = sorted(list(set(['{} {}'.format(corpus_test[t-1],corpus_test[t]) for t in range(1,corpus_test_len)] )))\n",
    "unigrams = sorted(list(set(corpus_test)))\n",
    "\n",
    "# Obtenemos el corpus de entrenamiento:\n",
    "with open('trainLM2.txt', 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    corpus = [['<s>'] + line.decode('iso-8859-1').split(' ')[:-1] + ['</s>'] for line in lines]\n",
    "    \n",
    "corpus = [[token for doc in corpus for token in doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 48612\n",
      "Vocabulary Size: 5924\n",
      "Number of batches: 95\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:1\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 100\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 4446.98486328125\n",
      "Epoch: 1, Batch number: 10, Loss: 4385.72216796875\n",
      "Epoch: 1, Batch number: 20, Loss: 4316.03369140625\n",
      "Epoch: 1, Batch number: 30, Loss: 4267.333984375\n",
      "Epoch: 1, Batch number: 40, Loss: 4206.4677734375\n",
      "Epoch: 1, Batch number: 50, Loss: 4144.2373046875\n",
      "Epoch: 1, Batch number: 60, Loss: 4102.51416015625\n",
      "Epoch: 1, Batch number: 70, Loss: 4000.6240234375\n",
      "Epoch: 1, Batch number: 80, Loss: 3973.108154296875\n",
      "Epoch: 1, Batch number: 90, Loss: 3927.130615234375\n",
      "Epoch: 2, Batch number: 5, Loss: 3808.236083984375\n",
      "Epoch: 2, Batch number: 15, Loss: 3734.701904296875\n",
      "Epoch: 2, Batch number: 25, Loss: 3672.935302734375\n",
      "Epoch: 2, Batch number: 35, Loss: 3601.31396484375\n",
      "Epoch: 2, Batch number: 45, Loss: 3526.304931640625\n",
      "Epoch: 2, Batch number: 55, Loss: 3518.852783203125\n",
      "Epoch: 2, Batch number: 65, Loss: 3398.611572265625\n",
      "Epoch: 2, Batch number: 75, Loss: 3392.111083984375\n",
      "Epoch: 2, Batch number: 85, Loss: 3335.809814453125\n",
      "Epoch: 3, Batch number: 0, Loss: 3221.710205078125\n",
      "Epoch: 3, Batch number: 10, Loss: 3242.22705078125\n",
      "Epoch: 3, Batch number: 20, Loss: 3189.760009765625\n",
      "Epoch: 3, Batch number: 30, Loss: 3222.75341796875\n",
      "Epoch: 3, Batch number: 40, Loss: 3179.460205078125\n",
      "Epoch: 3, Batch number: 50, Loss: 3195.05322265625\n",
      "Epoch: 3, Batch number: 60, Loss: 3130.69287109375\n",
      "Epoch: 3, Batch number: 70, Loss: 3079.05224609375\n",
      "Epoch: 3, Batch number: 80, Loss: 3174.429931640625\n",
      "Epoch: 3, Batch number: 90, Loss: 3131.21875\n",
      "Epoch: 4, Batch number: 5, Loss: 3123.775390625\n",
      "Epoch: 4, Batch number: 15, Loss: 2975.306396484375\n",
      "Epoch: 4, Batch number: 25, Loss: 3015.2734375\n",
      "Epoch: 4, Batch number: 35, Loss: 3048.47802734375\n",
      "Epoch: 4, Batch number: 45, Loss: 3083.011474609375\n",
      "Epoch: 4, Batch number: 55, Loss: 2996.740478515625\n",
      "Epoch: 4, Batch number: 65, Loss: 3065.49169921875\n",
      "Epoch: 4, Batch number: 75, Loss: 3020.802001953125\n",
      "Epoch: 4, Batch number: 85, Loss: 3090.436767578125\n",
      "Epoch: 5, Batch number: 0, Loss: 2971.3408203125\n",
      "Epoch: 5, Batch number: 10, Loss: 2944.947509765625\n",
      "Epoch: 5, Batch number: 20, Loss: 2973.027587890625\n",
      "Epoch: 5, Batch number: 30, Loss: 3000.62841796875\n",
      "Epoch: 5, Batch number: 40, Loss: 3027.241455078125\n",
      "Epoch: 5, Batch number: 50, Loss: 2942.249755859375\n",
      "Epoch: 5, Batch number: 60, Loss: 2959.2978515625\n",
      "Epoch: 5, Batch number: 70, Loss: 2968.147216796875\n",
      "Epoch: 5, Batch number: 80, Loss: 2953.166259765625\n",
      "Epoch: 5, Batch number: 90, Loss: 2781.74853515625\n",
      "Epoch: 6, Batch number: 5, Loss: 2841.11669921875\n",
      "Epoch: 6, Batch number: 15, Loss: 2888.34375\n",
      "Epoch: 6, Batch number: 25, Loss: 2872.53466796875\n",
      "Epoch: 6, Batch number: 35, Loss: 2878.038330078125\n",
      "Epoch: 6, Batch number: 45, Loss: 2954.638671875\n",
      "Epoch: 6, Batch number: 55, Loss: 2969.2919921875\n",
      "Epoch: 6, Batch number: 65, Loss: 2944.235595703125\n",
      "Epoch: 6, Batch number: 75, Loss: 2951.705078125\n",
      "Epoch: 6, Batch number: 85, Loss: 2833.21337890625\n",
      "Epoch: 7, Batch number: 0, Loss: 2753.391845703125\n",
      "Epoch: 7, Batch number: 10, Loss: 2924.013671875\n",
      "Epoch: 7, Batch number: 20, Loss: 2865.525390625\n",
      "Epoch: 7, Batch number: 30, Loss: 2800.77001953125\n",
      "Epoch: 7, Batch number: 40, Loss: 2825.061279296875\n",
      "Epoch: 7, Batch number: 50, Loss: 2811.476318359375\n",
      "Epoch: 7, Batch number: 60, Loss: 2861.71240234375\n",
      "Epoch: 7, Batch number: 70, Loss: 2886.618408203125\n",
      "Epoch: 7, Batch number: 80, Loss: 2835.46728515625\n",
      "Epoch: 7, Batch number: 90, Loss: 2798.17236328125\n",
      "Epoch: 8, Batch number: 5, Loss: 2900.59423828125\n",
      "Epoch: 8, Batch number: 15, Loss: 2739.887939453125\n",
      "Epoch: 8, Batch number: 25, Loss: 2708.3525390625\n",
      "Epoch: 8, Batch number: 35, Loss: 2722.93017578125\n",
      "Epoch: 8, Batch number: 45, Loss: 2728.6455078125\n",
      "Epoch: 8, Batch number: 55, Loss: 2723.56591796875\n",
      "Epoch: 8, Batch number: 65, Loss: 2669.487060546875\n",
      "Epoch: 8, Batch number: 75, Loss: 2874.256103515625\n",
      "Epoch: 8, Batch number: 85, Loss: 2799.92333984375\n",
      "Epoch: 9, Batch number: 0, Loss: 2652.92578125\n",
      "Epoch: 9, Batch number: 10, Loss: 2722.5693359375\n",
      "Epoch: 9, Batch number: 20, Loss: 2652.82421875\n",
      "Epoch: 9, Batch number: 30, Loss: 2585.9375\n",
      "Epoch: 9, Batch number: 40, Loss: 2876.85302734375\n",
      "Epoch: 9, Batch number: 50, Loss: 2785.0576171875\n",
      "Epoch: 9, Batch number: 60, Loss: 2689.4111328125\n",
      "Epoch: 9, Batch number: 70, Loss: 2636.1748046875\n",
      "Epoch: 9, Batch number: 80, Loss: 2653.0810546875\n",
      "Epoch: 9, Batch number: 90, Loss: 2669.203125\n",
      "Epoch: 10, Batch number: 5, Loss: 2641.57177734375\n",
      "Epoch: 10, Batch number: 15, Loss: 2694.90673828125\n",
      "Epoch: 10, Batch number: 25, Loss: 2668.444580078125\n",
      "Epoch: 10, Batch number: 35, Loss: 2707.20947265625\n",
      "Epoch: 10, Batch number: 45, Loss: 2625.614013671875\n",
      "Epoch: 10, Batch number: 55, Loss: 2681.803466796875\n",
      "Epoch: 10, Batch number: 65, Loss: 2698.783447265625\n",
      "Epoch: 10, Batch number: 75, Loss: 2591.754638671875\n",
      "Epoch: 10, Batch number: 85, Loss: 2727.184814453125\n",
      "Epoch: 11, Batch number: 0, Loss: 2534.638671875\n",
      "Epoch: 11, Batch number: 10, Loss: 2428.063720703125\n",
      "Epoch: 11, Batch number: 20, Loss: 2556.227294921875\n",
      "Epoch: 11, Batch number: 30, Loss: 2593.416015625\n",
      "Epoch: 11, Batch number: 40, Loss: 2609.1767578125\n",
      "Epoch: 11, Batch number: 50, Loss: 2577.607666015625\n",
      "Epoch: 11, Batch number: 60, Loss: 2524.7421875\n",
      "Epoch: 11, Batch number: 70, Loss: 2589.8955078125\n",
      "Epoch: 11, Batch number: 80, Loss: 2665.790771484375\n",
      "Epoch: 11, Batch number: 90, Loss: 2594.855224609375\n",
      "Epoch: 12, Batch number: 5, Loss: 2476.8515625\n",
      "Epoch: 12, Batch number: 15, Loss: 2507.000732421875\n",
      "Epoch: 12, Batch number: 25, Loss: 2436.084228515625\n",
      "Epoch: 12, Batch number: 35, Loss: 2504.8955078125\n",
      "Epoch: 12, Batch number: 45, Loss: 2511.7763671875\n",
      "Epoch: 12, Batch number: 55, Loss: 2540.8134765625\n",
      "Epoch: 12, Batch number: 65, Loss: 2457.991943359375\n",
      "Epoch: 12, Batch number: 75, Loss: 2550.093017578125\n",
      "Epoch: 12, Batch number: 85, Loss: 2512.940673828125\n",
      "Epoch: 13, Batch number: 0, Loss: 2569.51953125\n",
      "Epoch: 13, Batch number: 10, Loss: 2424.82373046875\n",
      "Epoch: 13, Batch number: 20, Loss: 2458.572998046875\n",
      "Epoch: 13, Batch number: 30, Loss: 2410.9287109375\n",
      "Epoch: 13, Batch number: 40, Loss: 2535.844970703125\n",
      "Epoch: 13, Batch number: 50, Loss: 2559.98486328125\n",
      "Epoch: 13, Batch number: 60, Loss: 2514.7451171875\n",
      "Epoch: 13, Batch number: 70, Loss: 2529.70947265625\n",
      "Epoch: 13, Batch number: 80, Loss: 2431.498779296875\n",
      "Epoch: 13, Batch number: 90, Loss: 2426.27734375\n",
      "Epoch: 14, Batch number: 5, Loss: 2415.882080078125\n",
      "Epoch: 14, Batch number: 15, Loss: 2458.005859375\n",
      "Epoch: 14, Batch number: 25, Loss: 2465.017822265625\n",
      "Epoch: 14, Batch number: 35, Loss: 2415.822509765625\n",
      "Epoch: 14, Batch number: 45, Loss: 2333.129638671875\n",
      "Epoch: 14, Batch number: 55, Loss: 2475.994873046875\n",
      "Epoch: 14, Batch number: 65, Loss: 2381.870849609375\n",
      "Epoch: 14, Batch number: 75, Loss: 2391.79736328125\n",
      "Epoch: 14, Batch number: 85, Loss: 2322.188720703125\n",
      "Epoch: 15, Batch number: 0, Loss: 2463.9169921875\n",
      "Epoch: 15, Batch number: 10, Loss: 2403.712890625\n",
      "Epoch: 15, Batch number: 20, Loss: 2390.471435546875\n",
      "Epoch: 15, Batch number: 30, Loss: 2393.631103515625\n",
      "Epoch: 15, Batch number: 40, Loss: 2374.243896484375\n",
      "Epoch: 15, Batch number: 50, Loss: 2365.505126953125\n",
      "Epoch: 15, Batch number: 60, Loss: 2299.944580078125\n",
      "Epoch: 15, Batch number: 70, Loss: 2368.549072265625\n",
      "Epoch: 15, Batch number: 80, Loss: 2390.9208984375\n",
      "Epoch: 15, Batch number: 90, Loss: 2415.685546875\n",
      "Epoch: 16, Batch number: 5, Loss: 2354.245361328125\n",
      "Epoch: 16, Batch number: 15, Loss: 2353.578369140625\n",
      "Epoch: 16, Batch number: 25, Loss: 2353.443359375\n",
      "Epoch: 16, Batch number: 35, Loss: 2396.4541015625\n",
      "Epoch: 16, Batch number: 45, Loss: 2321.425537109375\n",
      "Epoch: 16, Batch number: 55, Loss: 2342.129150390625\n",
      "Epoch: 16, Batch number: 65, Loss: 2368.1875\n",
      "Epoch: 16, Batch number: 75, Loss: 2329.387939453125\n",
      "Epoch: 16, Batch number: 85, Loss: 2347.724365234375\n",
      "Epoch: 17, Batch number: 0, Loss: 2391.067626953125\n",
      "Epoch: 17, Batch number: 10, Loss: 2275.7919921875\n",
      "Epoch: 17, Batch number: 20, Loss: 2239.333984375\n",
      "Epoch: 17, Batch number: 30, Loss: 2323.3095703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Batch number: 40, Loss: 2293.56591796875\n",
      "Epoch: 17, Batch number: 50, Loss: 2275.305419921875\n",
      "Epoch: 17, Batch number: 60, Loss: 2247.67431640625\n",
      "Epoch: 17, Batch number: 70, Loss: 2247.184814453125\n",
      "Epoch: 17, Batch number: 80, Loss: 2239.06103515625\n",
      "Epoch: 17, Batch number: 90, Loss: 2185.040771484375\n",
      "Epoch: 18, Batch number: 5, Loss: 2288.3623046875\n",
      "Epoch: 18, Batch number: 15, Loss: 2212.8515625\n",
      "Epoch: 18, Batch number: 25, Loss: 2234.97802734375\n",
      "Epoch: 18, Batch number: 35, Loss: 2204.158203125\n",
      "Epoch: 18, Batch number: 45, Loss: 2224.71142578125\n",
      "Epoch: 18, Batch number: 55, Loss: 2118.0380859375\n",
      "Epoch: 18, Batch number: 65, Loss: 2247.163330078125\n",
      "Epoch: 18, Batch number: 75, Loss: 2224.26513671875\n",
      "Epoch: 18, Batch number: 85, Loss: 2211.038330078125\n",
      "Epoch: 19, Batch number: 0, Loss: 2164.0732421875\n",
      "Epoch: 19, Batch number: 10, Loss: 2207.3828125\n",
      "Epoch: 19, Batch number: 20, Loss: 2152.52490234375\n",
      "Epoch: 19, Batch number: 30, Loss: 2198.706298828125\n",
      "Epoch: 19, Batch number: 40, Loss: 2247.767578125\n",
      "Epoch: 19, Batch number: 50, Loss: 2180.634765625\n",
      "Epoch: 19, Batch number: 60, Loss: 2145.422607421875\n",
      "Epoch: 19, Batch number: 70, Loss: 2130.541259765625\n",
      "Epoch: 19, Batch number: 80, Loss: 2251.057861328125\n",
      "Epoch: 19, Batch number: 90, Loss: 2187.525146484375\n",
      "Epoch: 20, Batch number: 5, Loss: 2175.5927734375\n",
      "Epoch: 20, Batch number: 15, Loss: 2187.73681640625\n",
      "Epoch: 20, Batch number: 25, Loss: 2167.913330078125\n",
      "Epoch: 20, Batch number: 35, Loss: 2054.807861328125\n",
      "Epoch: 20, Batch number: 45, Loss: 2135.658935546875\n",
      "Epoch: 20, Batch number: 55, Loss: 2150.00244140625\n",
      "Epoch: 20, Batch number: 65, Loss: 2105.5146484375\n",
      "Epoch: 20, Batch number: 75, Loss: 2091.216796875\n",
      "Epoch: 20, Batch number: 85, Loss: 2080.788330078125\n",
      "Epoch: 21, Batch number: 0, Loss: 2167.92529296875\n",
      "Epoch: 21, Batch number: 10, Loss: 2126.9912109375\n",
      "Epoch: 21, Batch number: 20, Loss: 2082.6923828125\n",
      "Epoch: 21, Batch number: 30, Loss: 2064.005615234375\n",
      "Epoch: 21, Batch number: 40, Loss: 2138.136962890625\n",
      "Epoch: 21, Batch number: 50, Loss: 2122.68603515625\n",
      "Epoch: 21, Batch number: 60, Loss: 2030.2071533203125\n",
      "Epoch: 21, Batch number: 70, Loss: 2148.052001953125\n",
      "Epoch: 21, Batch number: 80, Loss: 2057.48828125\n",
      "Epoch: 21, Batch number: 90, Loss: 2134.74560546875\n",
      "Epoch: 22, Batch number: 5, Loss: 2051.191162109375\n",
      "Epoch: 22, Batch number: 15, Loss: 2079.08642578125\n",
      "Epoch: 22, Batch number: 25, Loss: 2064.188232421875\n",
      "Epoch: 22, Batch number: 35, Loss: 2000.2938232421875\n",
      "Epoch: 22, Batch number: 45, Loss: 2099.546142578125\n",
      "Epoch: 22, Batch number: 55, Loss: 2021.1785888671875\n",
      "Epoch: 22, Batch number: 65, Loss: 2057.014404296875\n",
      "Epoch: 22, Batch number: 75, Loss: 1997.846435546875\n",
      "Epoch: 22, Batch number: 85, Loss: 2075.138671875\n",
      "Epoch: 23, Batch number: 0, Loss: 1937.6041259765625\n",
      "Epoch: 23, Batch number: 10, Loss: 1963.91162109375\n",
      "Epoch: 23, Batch number: 20, Loss: 1989.6776123046875\n",
      "Epoch: 23, Batch number: 30, Loss: 2006.2716064453125\n",
      "Epoch: 23, Batch number: 40, Loss: 2056.7509765625\n",
      "Epoch: 23, Batch number: 50, Loss: 2040.7095947265625\n",
      "Epoch: 23, Batch number: 60, Loss: 1985.7674560546875\n",
      "Epoch: 23, Batch number: 70, Loss: 2008.396728515625\n",
      "Epoch: 23, Batch number: 80, Loss: 2032.80859375\n",
      "Epoch: 23, Batch number: 90, Loss: 2008.42919921875\n",
      "Epoch: 24, Batch number: 5, Loss: 1915.7413330078125\n",
      "Epoch: 24, Batch number: 15, Loss: 1844.9542236328125\n",
      "Epoch: 24, Batch number: 25, Loss: 2001.643798828125\n",
      "Epoch: 24, Batch number: 35, Loss: 1999.3330078125\n",
      "Epoch: 24, Batch number: 45, Loss: 1887.679443359375\n",
      "Epoch: 24, Batch number: 55, Loss: 1953.603271484375\n",
      "Epoch: 24, Batch number: 65, Loss: 1962.9395751953125\n",
      "Epoch: 24, Batch number: 75, Loss: 1943.7410888671875\n",
      "Epoch: 24, Batch number: 85, Loss: 1966.2200927734375\n",
      "Epoch: 25, Batch number: 0, Loss: 1829.67138671875\n",
      "Epoch: 25, Batch number: 10, Loss: 2025.7518310546875\n",
      "Epoch: 25, Batch number: 20, Loss: 1844.537109375\n",
      "Epoch: 25, Batch number: 30, Loss: 1910.097900390625\n",
      "Epoch: 25, Batch number: 40, Loss: 1880.790771484375\n",
      "Epoch: 25, Batch number: 50, Loss: 1898.7568359375\n",
      "Epoch: 25, Batch number: 60, Loss: 1947.935546875\n",
      "Epoch: 25, Batch number: 70, Loss: 1898.999755859375\n",
      "Epoch: 25, Batch number: 80, Loss: 1875.400634765625\n",
      "Epoch: 25, Batch number: 90, Loss: 1878.217041015625\n",
      "Epoch: 26, Batch number: 5, Loss: 1848.6038818359375\n",
      "Epoch: 26, Batch number: 15, Loss: 1790.266357421875\n",
      "Epoch: 26, Batch number: 25, Loss: 1910.196044921875\n",
      "Epoch: 26, Batch number: 35, Loss: 1878.599365234375\n",
      "Epoch: 26, Batch number: 45, Loss: 1851.8572998046875\n",
      "Epoch: 26, Batch number: 55, Loss: 1866.5831298828125\n",
      "Epoch: 26, Batch number: 65, Loss: 1882.4775390625\n",
      "Epoch: 26, Batch number: 75, Loss: 1971.51220703125\n",
      "Epoch: 26, Batch number: 85, Loss: 1842.2305908203125\n",
      "Epoch: 27, Batch number: 0, Loss: 1824.664306640625\n",
      "Epoch: 27, Batch number: 10, Loss: 1775.730712890625\n",
      "Epoch: 27, Batch number: 20, Loss: 1826.484619140625\n",
      "Epoch: 27, Batch number: 30, Loss: 1811.9661865234375\n",
      "Epoch: 27, Batch number: 40, Loss: 1832.4068603515625\n",
      "Epoch: 27, Batch number: 50, Loss: 1852.5888671875\n",
      "Epoch: 27, Batch number: 60, Loss: 1783.519775390625\n",
      "Epoch: 27, Batch number: 70, Loss: 1797.068603515625\n",
      "Epoch: 27, Batch number: 80, Loss: 1836.6240234375\n",
      "Epoch: 27, Batch number: 90, Loss: 1804.033203125\n",
      "Epoch: 28, Batch number: 5, Loss: 1844.58203125\n",
      "Epoch: 28, Batch number: 15, Loss: 1791.1142578125\n",
      "Epoch: 28, Batch number: 25, Loss: 1736.855712890625\n",
      "Epoch: 28, Batch number: 35, Loss: 1719.6119384765625\n",
      "Epoch: 28, Batch number: 45, Loss: 1728.5535888671875\n",
      "Epoch: 28, Batch number: 55, Loss: 1796.2813720703125\n",
      "Epoch: 28, Batch number: 65, Loss: 1813.212890625\n",
      "Epoch: 28, Batch number: 75, Loss: 1805.64208984375\n",
      "Epoch: 28, Batch number: 85, Loss: 1777.508056640625\n",
      "Epoch: 29, Batch number: 0, Loss: 1745.047607421875\n",
      "Epoch: 29, Batch number: 10, Loss: 1778.5316162109375\n",
      "Epoch: 29, Batch number: 20, Loss: 1799.1788330078125\n",
      "Epoch: 29, Batch number: 30, Loss: 1721.9312744140625\n",
      "Epoch: 29, Batch number: 40, Loss: 1703.563232421875\n",
      "Epoch: 29, Batch number: 50, Loss: 1650.164794921875\n",
      "Epoch: 29, Batch number: 60, Loss: 1760.929443359375\n",
      "Epoch: 29, Batch number: 70, Loss: 1854.3729248046875\n",
      "Epoch: 29, Batch number: 80, Loss: 1700.920166015625\n",
      "Epoch: 29, Batch number: 90, Loss: 1746.6070556640625\n",
      "Epoch: 30, Batch number: 5, Loss: 1639.798095703125\n",
      "Epoch: 30, Batch number: 15, Loss: 1618.2020263671875\n",
      "Epoch: 30, Batch number: 25, Loss: 1643.5313720703125\n",
      "Epoch: 30, Batch number: 35, Loss: 1623.3861083984375\n",
      "Epoch: 30, Batch number: 45, Loss: 1654.9268798828125\n",
      "Epoch: 30, Batch number: 55, Loss: 1621.343994140625\n",
      "Epoch: 30, Batch number: 65, Loss: 1733.028564453125\n",
      "Epoch: 30, Batch number: 75, Loss: 1695.986328125\n",
      "Epoch: 30, Batch number: 85, Loss: 1762.52978515625\n",
      "Epoch: 31, Batch number: 0, Loss: 1656.4576416015625\n",
      "Epoch: 31, Batch number: 10, Loss: 1632.46728515625\n",
      "Epoch: 31, Batch number: 20, Loss: 1647.4229736328125\n",
      "Epoch: 31, Batch number: 30, Loss: 1607.6043701171875\n",
      "Epoch: 31, Batch number: 40, Loss: 1598.8720703125\n",
      "Epoch: 31, Batch number: 50, Loss: 1660.36669921875\n",
      "Epoch: 31, Batch number: 60, Loss: 1641.1168212890625\n",
      "Epoch: 31, Batch number: 70, Loss: 1710.186279296875\n",
      "Epoch: 31, Batch number: 80, Loss: 1668.2698974609375\n",
      "Epoch: 31, Batch number: 90, Loss: 1711.4205322265625\n",
      "Epoch: 32, Batch number: 5, Loss: 1653.3565673828125\n",
      "Epoch: 32, Batch number: 15, Loss: 1611.0980224609375\n",
      "Epoch: 32, Batch number: 25, Loss: 1623.8818359375\n",
      "Epoch: 32, Batch number: 35, Loss: 1685.7542724609375\n",
      "Epoch: 32, Batch number: 45, Loss: 1616.825439453125\n",
      "Epoch: 32, Batch number: 55, Loss: 1589.0128173828125\n",
      "Epoch: 32, Batch number: 65, Loss: 1729.3917236328125\n",
      "Epoch: 32, Batch number: 75, Loss: 1598.1534423828125\n",
      "Epoch: 32, Batch number: 85, Loss: 1605.0946044921875\n",
      "Epoch: 33, Batch number: 0, Loss: 1558.515625\n",
      "Epoch: 33, Batch number: 10, Loss: 1544.294189453125\n",
      "Epoch: 33, Batch number: 20, Loss: 1506.9737548828125\n",
      "Epoch: 33, Batch number: 30, Loss: 1624.4365234375\n",
      "Epoch: 33, Batch number: 40, Loss: 1587.0914306640625\n",
      "Epoch: 33, Batch number: 50, Loss: 1561.6385498046875\n",
      "Epoch: 33, Batch number: 60, Loss: 1705.5955810546875\n",
      "Epoch: 33, Batch number: 70, Loss: 1560.5692138671875\n",
      "Epoch: 33, Batch number: 80, Loss: 1567.8067626953125\n",
      "Exiting training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos:\n",
    "\n",
    "window_size = 8           # Tamaño de la ventana del contexto.\n",
    "cutoff_freq = 0           # Palabras con una frecuencia menor o igual a cutoff_freq son excluídas del vocabulario.\n",
    "batch_size = 512          # Tamaño del batch.\n",
    "\n",
    "model = 'CBOW'            # Método de entrenamiento.\n",
    "embedding_dim = 200       # Dimensión del espacio de los word vectors.\n",
    "device = 'cuda:1'         # Dispositivo sobre el cual se entrena. \n",
    "state_dict = None         # Parámetros pre-entrenados.\n",
    "paralelize = False        # Flag para decirle al programa que use las 2 gpus\n",
    "\n",
    "epochs = 100              # Cantidad de epochs\n",
    "learning_rate = 5e-4      # Tasa de aprendizaje\n",
    "sample_loss_every = 10    # Calcular la loss cada este número\n",
    "algorithm = 'Adam'        # Algoritmo de optimización\n",
    "\n",
    "trainer = Word2vecTrainer(corpus,cutoff_freq=cutoff_freq,window_size=window_size,batch_size=batch_size)\n",
    "trainer.InitModel(model=model, state_dict=state_dict, device=device, paralelize=paralelize, embedding_dim=embedding_dim)\n",
    "trainer.Train(algorithm=algorithm, epochs=epochs, sample_loss_every=sample_loss_every, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo de lenguaje en el archivo `lm_l40_word_vectors` y calculamos la perplejidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El logaritmo de la perplejidad para el corpus de test es: -14.516232729664113\n"
     ]
    }
   ],
   "source": [
    "def compute_probs(trainer,bigrams):\n",
    "    vocab = trainer.dataloader.dataset.vocabulary\n",
    "    forward = lambda x: trainer.model.out(trainer.model.emb(x))\n",
    "    device = trainer.device\n",
    "    log_probs_bigram = []\n",
    "    log_probs_unigram = []\n",
    "    for bigram in bigrams:\n",
    "        w1, w2 = bigram.split(' ')\n",
    "        score = forward(torch.tensor(vocab[w1],device=device))\n",
    "        log_prob = (score[vocab[w2]] - torch.logsumexp(score,dim=0)).item()\n",
    "        log_probs_bigram.append(log_prob)\n",
    "    for unigram in unigram:\n",
    "        score = forward(torch.tensor(vocab[unigram],device=device))\n",
    "        log_prob = (score[vocab[unigram]] - torch.logsumexp(score,dim=0)).item()\n",
    "        log_probs_unigram.append(log_prob)\n",
    "    bigrams_with_probs = ['{:.4f}\\t{}\\n'.format(log_prob,bigram) for log_prob, bigram in zip(log_probs_bigram, bigrams)]\n",
    "    bigrams_with_probs = ['{:.4f}\\t{}\\n'.format(log_prob,bigram) for log_prob, bigram in zip(log_probs_bigram, bigrams)]\n",
    "    text = r\"\"\"\\data\\\n",
    "ngram 1={}\n",
    "ngram 2={}\n",
    "\n",
    "\\1-grams:\n",
    "{}\n",
    "\n",
    "\\2-grams:\n",
    "{}\n",
    "\n",
    "\\end\\\n",
    "\n",
    "\"\"\".format(len(unigrams),len(bigrams),''.join(bigrams_with_probs))\n",
    "    return text\n",
    "    \n",
    "with open('./lm_files/lm_l40_word_vectors', 'wb') as file:\n",
    "    text = compute_probs(trainer,bigrams)\n",
    "    file.write(text.encode('iso-8859-1'))\n",
    "\n",
    "# Perplejidad para un modelo de bigrama:\n",
    "def get_log_prob(w1,w2):\n",
    "    match = re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({} {})'.format(w1,w2),text)\n",
    "    if match is not None:\n",
    "        log_prob, _ = match.groups()\n",
    "        return float(log_prob)\n",
    "    match_w1 = re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({})\\t*(\\-?[\\d]*[\\.]?[\\d]*)'.format(w1),text)\n",
    "    match_w2 = re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({})\\t*(\\-?[\\d]*[\\.]?[\\d]*)'.format(w2),text)\n",
    "    return float(match_w1.groups()[0]) + float(match_w2.groups()[2])  \n",
    "\n",
    "corpus_len = len(corpus_test)\n",
    "log_p = [get_log_prob(corpus_test[idx-1],corpus_test[idx]) for idx in range(1,corpus_len)]\n",
    "log_p.insert(0,float(re.search(r'([\\-]?[\\d]+[\\.]?[\\d]*)\\t({})\\t*(\\-?[\\d]*[\\.]?[\\d]*)'.format(corpus_test[0]),text).groups()[0]))\n",
    "print('El logaritmo de la perplejidad para el corpus de test es: {}'.format(sum(log_p)/corpus_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
