{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch:\n",
    "\n",
    "En Pytorch yo puedo crear objetos tipo `torch.Tensor` y usarlos como numpy arrays. En este caso, los tensores que se crean para este tipo de propósito son de tipo *leaf* (\"hoja\") y se refieren a que son \"hojas del grafo computacional del backpropagation\". Si sólo hago operaciones de tipo crear y calcular operaciones con esos tensores (es decir, no calculo gradientes), todos los tensores van a ser de tipo *leaf*, y va a ser lo mismo que usar numpy. \n",
    "\n",
    "Ahora, cada objeto de tipo `torch.Tensor` tiene un flag `requires_grad` y un objeto de tipo `torch.autograd.Function` llamado `grad_fn` que se encarga de guardar la información necesaria para hacer backpropagation. También contiene un objeto `grad` (también de tipo `torch.tensor`) que contiene el valor del gradiente para ese tensor. Si yo quiero armar un grafo $f(x)$ tengo que definir un tensor *leaf* con `requires_grad=True` y crear otros tensores a partir de éste. Todos los tensores creados a partir de un tensor *leaf* con `requires_grad=True`, dejan de ser *leaf*.\n",
    "\n",
    "Sólo es posible cambiar el flag `requires_grad` en los tensores *leaf*. Si quiero que un tensor no *leaf* pase a ser *leaf*, tengo que usar `.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 2, 3, 1],\n",
      "        [1, 2, 2, 3, 1],\n",
      "        [2, 2, 2, 3, 1]])\n",
      "tensor([[ 0.2518, -0.0550,  0.1407, -0.2840],\n",
      "        [ 0.5023, -0.0691,  0.2553, -0.5804],\n",
      "        [ 0.1473,  0.0573,  0.0261, -0.1966]], grad_fn=<MmBackward>)\n",
      "[Parameter containing:\n",
      "tensor([[ 0.4023, -0.2521],\n",
      "        [-0.3493,  2.1044],\n",
      "        [-0.0991, -1.1768],\n",
      "        [-0.2976,  2.8471]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0387,  0.5440],\n",
      "        [-0.6702, -0.2437],\n",
      "        [ 0.4380,  0.3827],\n",
      "        [ 0.1817, -0.5709]], requires_grad=True)]\n",
      "tensor([[0, 2, 2, 3, 1],\n",
      "        [1, 2, 2, 3, 1],\n",
      "        [2, 2, 2, 3, 1]])\n",
      "tensor([[0, 2, 2, 3, 1],\n",
      "        [1, 2, 2, 3, 1],\n",
      "        [2, 2, 2, 3, 1]], device='cuda:1')\n",
      "tensor([[ 0.2518, -0.0550,  0.1407, -0.2840],\n",
      "        [ 0.5023, -0.0691,  0.2553, -0.5804],\n",
      "        [ 0.1473,  0.0573,  0.0261, -0.1966]], device='cuda:1',\n",
      "       grad_fn=<MmBackward>)\n",
      "[Parameter containing:\n",
      "tensor([[ 0.4023, -0.2521],\n",
      "        [-0.3493,  2.1044],\n",
      "        [-0.0991, -1.1768],\n",
      "        [-0.2976,  2.8471]], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0387,  0.5440],\n",
      "        [-0.6702, -0.2437],\n",
      "        [ 0.4380,  0.3827],\n",
      "        [ 0.1817, -0.5709]], device='cuda:1', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.emb = nn.Embedding(4,2)\n",
    "        self.linear = nn.Linear(2,4,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(self.emb(x).mean(dim=1))\n",
    "    \n",
    "\n",
    "model = MyModel()\n",
    "x = torch.tensor([[0,2,2,3,1],[1,2,2,3,1],[2,2,2,3,1]])\n",
    "print(x)\n",
    "print(model.forward(x))\n",
    "print(list(model.parameters()))\n",
    "model = model.to(device=torch.device('cuda:1'))\n",
    "print(x)\n",
    "x = x.to(device=torch.device('cuda:1'))\n",
    "print(x)\n",
    "print(model.forward(x))\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Definir el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "cifar10_train = dset.CIFAR10('CIFAR10/train/', train=True, download=True, transform=transform)\n",
    "cifar10_test = dset.CIFAR10('CIFAR10/test/', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: pasarle el dataset al entrenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelTrainer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 train_dataset,\n",
    "                 test_dataset,\n",
    "                 batch_size=64,\n",
    "                 val_size=.02):\n",
    "        \n",
    "        # Model:\n",
    "        self.model = model\n",
    "        \n",
    "        # Data:\n",
    "        tr, val, te = self.generate_data_batches(train_dataset, test_dataset,batch_size,val_size)\n",
    "        self.train_dataloader, self.val_dataloader, self.test_dataloader = tr, val, te\n",
    "        \n",
    "        # Data-types:\n",
    "        self.input_dtype = next(iter(self.train_dataloader))[0].dtype\n",
    "        self.target_dtype = next(iter(self.train_dataloader))[1].dtype\n",
    "        \n",
    "        self.first_time = True\n",
    "        self.batch_len = len(self.train_dataloader)\n",
    "        \n",
    "        print('Model trainer created:')\n",
    "        train_samples = int((1 - val_size) * len(train_dataset)) \n",
    "        val_samples = len(train_dataset) - train_samples\n",
    "        test_samples = len(test_dataset)\n",
    "        total_samples = train_samples + val_samples + test_samples\n",
    "        percent_val, percent_test = int((val_samples / total_samples) * 100), int((test_samples / total_samples) * 100)\n",
    "        print('Number of training samples: {} ({}%)'.format(train_samples, 100 - percent_val - percent_test))\n",
    "        print('Number of validation samples: {} ({}%)'.format(val_samples, percent_val))\n",
    "        print('Number of test samples: {} ({}%)'.format(test_samples, percent_test))\n",
    "        print('Number of train batches: {}'.format(self.batch_len))\n",
    "        print('Number of samples per batch: {}'.format(batch_size))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    def generate_data_batches(self,train_dataset, test_dataset, # Train y test datasets\n",
    "                              batch_size = 64, # Tamaño del batch\n",
    "                              val_size = .02): # Proporción de muestras utilizadas para validación \n",
    "    \n",
    "        \"\"\"\n",
    "            Función para iterar sobre los batches de muestras. \n",
    "            Devuelve los dataloaders de train / validation / test.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Separo las muestras aleatoriamente en Train y Validation:\n",
    "        NUM_TRAIN = int((1 - val_size) * len(train_dataset)) \n",
    "        samples_idx = torch.randperm(len(train_dataset))\n",
    "        train_samples_idx = samples_idx[:NUM_TRAIN]\n",
    "        val_samples_idx = samples_idx[NUM_TRAIN:]\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices) # sampler\n",
    "\n",
    "        # Dataloader para las muestras de entrenamiento:\n",
    "        train_dataloader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      sampler=my_sampler(train_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de validación:\n",
    "        val_dataloader = DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=my_sampler(val_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de testeo:\n",
    "        test_dataloader = DataLoader(test_dataset, \n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "        return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "    \n",
    "    def InitParameters(self,from_pretrained=None,use_gpu=None, **kwargs):\n",
    "       \n",
    "        if from_pretrained is not None:\n",
    "            pass\n",
    "\n",
    "        # Defino el dispositivo sobre el cual trabajar:\n",
    "        if use_gpu == 0:\n",
    "            self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        elif use_gpu == 1:\n",
    "            self.device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        elif use_gpu is None:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.model.init_parameters(kwargs)\n",
    "        self.model = self.model.to(device=self.device)\n",
    "\n",
    "        \n",
    "    def SGDTrain(self, epochs=1, learning_rate=1e-1, sample_loss_every=100, check_on_train=False):\n",
    "        \n",
    "        if self.first_time:\n",
    "            print('Starting training...')\n",
    "            n_iter = 0\n",
    "            self.performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "            self.first_time = False\n",
    "        else:\n",
    "            n_iter = self.performance_history['iter'][-1]\n",
    "            print('Resuming training...')\n",
    "        \n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        print('Optimization method: Stochastic Gradient Descent')\n",
    "        print('Learning Rate: {:.2g}'.format(learning_rate))\n",
    "        print('Number of epochs: {}'.format(epochs))\n",
    "        print('Running on device \"{}\"'.format(self.device))\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            for e in range(epochs):\n",
    "                for t, (x,y) in enumerate(self.train_dataloader):\n",
    "\n",
    "                    x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "                    y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                    optimizer.zero_grad() # Llevo a cero los gradientes de la red\n",
    "                    scores = self.model(x) # Calculo la salida de la red\n",
    "                    loss = self.model.loss(scores,y) # Calculo el valor de la loss\n",
    "                    loss.backward() # Calculo los gradientes\n",
    "                    optimizer.step() # Actualizo los parámetros\n",
    "\n",
    "                    if (e * self.batch_len + t) % sample_loss_every == 0:\n",
    "                        num_correct_val, num_samples_val = self.check_accuracy('validation')\n",
    "                        self.performance_history['iter'].append(e * self.batch_len + t + n_iter)\n",
    "                        self.performance_history['loss'].append(loss.item())\n",
    "                        self.performance_history['accuracy'].append(float(num_correct_val / num_samples_val))\n",
    "                        print('Epoch: {}, Batch number: {}'.format(e+1, t))\n",
    "                        print('Accuracy on validation dataset: {}/{} ({:.2f}%)'.format(num_correct_val, num_samples_val, 100 * float(num_correct_val) / num_samples_val))\n",
    "                        print()\n",
    "\n",
    "                        if check_on_train:\n",
    "                            num_correct_train, num_samples_train = self.check_accuracy('train')\n",
    "                            print('Accuracy on train dataset: {}/{} ({:.2f}%)'.format(num_correct_train, num_samples_train, 100 * float(num_correct_train) / num_samples_train))\n",
    "                            print()\n",
    "\n",
    "            print('Training finished')\n",
    "            print()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "\n",
    "            print('Exiting training...')\n",
    "            print()    \n",
    "\n",
    "    def check_accuracy(self, dataset='validation'):\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        if dataset == 'train':\n",
    "            loader = self.train_dataloader\n",
    "        elif dataset == 'validation':\n",
    "            loader = self.val_dataloader\n",
    "        elif dataset == 'test':\n",
    "            loader = self.test_dataloader\n",
    "        else:\n",
    "            raise AttributeError('Please specify on which dataset to perform de accuracy calculation')\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=self.input_dtype)  \n",
    "                y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                scores = self.model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "\n",
    "        self.model.train()\n",
    "        return num_correct, num_samples\n",
    "\n",
    "    def CheckResultsOnTest(self):\n",
    "        \n",
    "        total_corrects = 0\n",
    "        total_samples = 0\n",
    "        total_performance = 0.\n",
    "        \n",
    "        for (x,y) in enumerate(self.test_dataloader):\n",
    "            x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "            y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "            num_correct, num_samples = self.check_accuracy('test')\n",
    "            total_corrects += num_corrects\n",
    "            total_samples += num_samples\n",
    "            total_performance += float(num_correct / num_samples)\n",
    "        \n",
    "        print('Final accuracy on test set: {}/{} ({}%)'.format(total_corrects,total_samples,total_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x.view(-1,self.in_features))\n",
    "\n",
    "    def init_parameters(self,a):\n",
    "        pass\n",
    "    \n",
    "    def loss(self,output,target):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, target)\n",
    "        return loss\n",
    "\n",
    "in_features = 3 * 32 * 32\n",
    "out_features = 10\n",
    "net = Net(in_features, out_features)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trainer created:\n",
      "Number of training samples: 49000 (83%)\n",
      "Number of validation samples: 1000 (1%)\n",
      "Number of test samples: 10000 (16%)\n",
      "Number of train batches: 766\n",
      "Number of samples per batch: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(net, cifar10_train, cifar10_test, batch_size=64, val_size=.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0101,  0.0064, -0.0052,  ...,  0.0025,  0.0015, -0.0036],\n",
      "        [-0.0177, -0.0170, -0.0030,  ...,  0.0053,  0.0032,  0.0105],\n",
      "        [-0.0100,  0.0029, -0.0092,  ...,  0.0034, -0.0066, -0.0021],\n",
      "        ...,\n",
      "        [ 0.0160, -0.0176,  0.0162,  ..., -0.0079, -0.0049,  0.0011],\n",
      "        [-0.0122, -0.0148,  0.0021,  ...,  0.0109, -0.0167, -0.0099],\n",
      "        [ 0.0012, -0.0089,  0.0152,  ...,  0.0119, -0.0066,  0.0041]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0054, -0.0082, -0.0037,  0.0144, -0.0171, -0.0168, -0.0086, -0.0070,\n",
      "         0.0069, -0.0057], device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "trainer.InitParameters(from_pretrained=None, use_gpu=1, a=1)\n",
    "for param in trainer.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Optimization method: Stochastic Gradient Descent\n",
      "Learning Rate: 0.1\n",
      "Number of epochs: 1\n",
      "Running on device \"cuda:1\"\n",
      "\n",
      "Epoch: 1, Batch number: 0\n",
      "Accuracy on validation dataset: 159/1000 (15.90%)\n",
      "\n",
      "Epoch: 1, Batch number: 100\n",
      "Accuracy on validation dataset: 196/1000 (19.60%)\n",
      "\n",
      "Epoch: 1, Batch number: 200\n",
      "Accuracy on validation dataset: 280/1000 (28.00%)\n",
      "\n",
      "Epoch: 1, Batch number: 300\n",
      "Accuracy on validation dataset: 329/1000 (32.90%)\n",
      "\n",
      "Epoch: 1, Batch number: 400\n",
      "Accuracy on validation dataset: 312/1000 (31.20%)\n",
      "\n",
      "Epoch: 1, Batch number: 500\n",
      "Accuracy on validation dataset: 292/1000 (29.20%)\n",
      "\n",
      "Epoch: 1, Batch number: 600\n",
      "Accuracy on validation dataset: 270/1000 (27.00%)\n",
      "\n",
      "Epoch: 1, Batch number: 700\n",
      "Accuracy on validation dataset: 279/1000 (27.90%)\n",
      "\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.SGDTrain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
