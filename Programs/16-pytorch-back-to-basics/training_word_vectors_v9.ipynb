{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NLPUtils import *\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../15-Training-word-vectors/text8','r') as file:\n",
    "    corpus = file.read()\n",
    "    corpus = [corpus.split(' ')[:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chau', 'guadi', 'hola', 'ir', 'lauti', 'luna', 'me', 'quiero', 'soy']\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "corpus = [['hola', 'soy', 'lauti'],['hola', 'soy', 'guadi'], ['chau', 'soy', 'luna','me','quiero','ir']]\n",
    "\n",
    "corpus_words = sorted(list(set([item for sublist in corpus for item in sublist])))\n",
    "counts = np.array([[doc.count(word) for doc in corpus] for word in corpus_words]).sum(axis=1)\n",
    "        \n",
    "print(corpus_words)\n",
    "print(type(counts.tolist()[0]))\n",
    "\n",
    "voc = Vocabulary()#Vocabulary(corpus_words, counts.tolist())\n",
    "#for token in itertools.chain.from_iterable(corpus):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'a', 'abolished', 'abuse', 'act', 'advocate', 'against', 'aid', 'also', 'although', 'anarchism', 'anarchists', 'anarchy', 'and', 'anomie', 'anti', 'any', 'archons', 'are', 'as', 'association', 'authoritarian', 'autonomous', 'based', 'be', 'been', 'belief', 'believe', 'but', 'by', 'chaos', 'chief', 'class', 'coercive', 'culottes', 'defined', 'derived', 'describe', 'destroy', 'differing', 'diggers', 'does', 'early', 'easily', 'economic', 'elimination', 'english', 'first', 'free', 'french', 'from', 'governance', 'greek', 'harmonious', 'has', 'imply', 'in', 'including', 'individuals', 'institutions', 'interpretations', 'is', 'it', 'king', 'label', 'means', 'most', 'movements', 'mutual', 'nihilism', 'not', 'of', 'offer', 'or', 'organization', 'originated', 'particularly', 'pejorative', 'philosophy', 'place', 'political', 'positive', 'radicals', 'rather', 'refers', 'regarded', 'related', 'relations', 'revolution', 'ruler', 'rulers', 'sans', 'self', 'should', 'social', 'society', 'state', 'still', 'structures', 'taken', 'term', 'that', 'the', 'there', 'they', 'this', 'to', 'truly', 'unnecessary', 'up', 'upon', 'use', 'used', 'violent', 'visions', 'voluntary', 'way', 'what', 'while', 'whilst', 'without', 'word', 'working']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-403c6df2a0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer_200\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2vecTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcutoff_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutoff_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/disco.lautaro/home/lestien/Documents/BecaNLP/Programs/16-pytorch-back-to-basics/NLPUtils/Training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, cutoff_freq, lm, window_size, batch_size, embedding_dim)\u001b[0m\n\u001b[1;32m    228\u001b[0m                           for idx in range(len(self.dataloader.dataset.vocabulary))])\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CBOW'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOWModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SkipGram'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disco.lautaro/home/lestien/Documents/BecaNLP/Programs/16-pytorch-back-to-basics/NLPUtils/WordVectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embedding_dim)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCBOWModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "#corpus = [['hola', 'soy', 'lauti'],['hola', 'soy', 'guadi'], ['chau', 'soy', 'luna','me','quiero','ir']]\n",
    "\n",
    "# Hiperpar√°metros:\n",
    "method = 'CBOW'\n",
    "window_size = 8\n",
    "embedding_dim = 200\n",
    "cutoff_freq = 1\n",
    "batch_size = 256\n",
    "\n",
    "trainer_200 = Word2vecTrainer(corpus,cutoff_freq=cutoff_freq,lm=method,window_size=window_size,batch_size=batch_size,embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../15-Training-word-vectors/vectors-emb200-win8-lmCBOW_2020-01-24-15-08-26.bin is not a file!\n"
     ]
    }
   ],
   "source": [
    "pretrained = './../15-Training-word-vectors/vectors-emb200-win8-lmCBOW_2020-01-24-15-08-26.bin'\n",
    "requires_grad = True\n",
    "use_gpu = 1\n",
    "\n",
    "trainer_200.InitEmbeddings(from_pretrained=pretrained, requires_grad=requires_grad, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Optimization method: Stochastic Gradient Descent\n",
      "Learning Rate: 0.005\n",
      "Number of epochs: 1\n",
      "Running on device (cpu)\n",
      "\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "learning_rate = 5e-3\n",
    "sample_loss_every = 100\n",
    "\n",
    "trainer_200.SGDTrain(epochs=epochs, learning_rate=learning_rate, sample_loss_every=sample_loss_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
