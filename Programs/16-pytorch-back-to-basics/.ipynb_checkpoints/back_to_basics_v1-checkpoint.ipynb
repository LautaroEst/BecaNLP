{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch:\n",
    "\n",
    "En Pytorch yo puedo crear objetos tipo `torch.Tensor` y usarlos como numpy arrays. En este caso, los tensores que se crean para este tipo de propósito son de tipo *leaf* (\"hoja\") y se refieren a que son \"hojas del grafo computacional del backpropagation\". Si sólo hago operaciones de tipo crear y calcular operaciones con esos tensores (es decir, no calculo gradientes), todos los tensores van a ser de tipo *leaf*, y va a ser lo mismo que usar numpy. \n",
    "\n",
    "Ahora, cada objeto de tipo `torch.Tensor` tiene un flag `requires_grad` y un objeto de tipo `torch.autograd.Function` llamado `grad_fn` que se encarga de guardar la información necesaria para hacer backpropagation. También contiene un objeto `grad` (también de tipo `torch.tensor`) que contiene el valor del gradiente para ese tensor. Si yo quiero armar un grafo $f(x)$ tengo que definir un tensor *leaf* con `requires_grad=True` y crear otros tensores a partir de éste. Todos los tensores creados a partir de un tensor *leaf* con `requires_grad=True`, dejan de ser *leaf*.\n",
    "\n",
    "Sólo es posible cambiar el flag `requires_grad` en los tensores *leaf*. Si quiero que un tensor no *leaf* pase a ser *leaf*, tengo que usar `.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 2, 3, 1],\n",
      "        [1, 2, 2, 3, 1],\n",
      "        [2, 2, 2, 3, 1]])\n",
      "tensor([[ 0.2518, -0.0550,  0.1407, -0.2840],\n",
      "        [ 0.5023, -0.0691,  0.2553, -0.5804],\n",
      "        [ 0.1473,  0.0573,  0.0261, -0.1966]], grad_fn=<MmBackward>)\n",
      "[Parameter containing:\n",
      "tensor([[ 0.4023, -0.2521],\n",
      "        [-0.3493,  2.1044],\n",
      "        [-0.0991, -1.1768],\n",
      "        [-0.2976,  2.8471]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0387,  0.5440],\n",
      "        [-0.6702, -0.2437],\n",
      "        [ 0.4380,  0.3827],\n",
      "        [ 0.1817, -0.5709]], requires_grad=True)]\n",
      "tensor([[0, 2, 2, 3, 1],\n",
      "        [1, 2, 2, 3, 1],\n",
      "        [2, 2, 2, 3, 1]])\n",
      "tensor([[0, 2, 2, 3, 1],\n",
      "        [1, 2, 2, 3, 1],\n",
      "        [2, 2, 2, 3, 1]], device='cuda:1')\n",
      "tensor([[ 0.2518, -0.0550,  0.1407, -0.2840],\n",
      "        [ 0.5023, -0.0691,  0.2553, -0.5804],\n",
      "        [ 0.1473,  0.0573,  0.0261, -0.1966]], device='cuda:1',\n",
      "       grad_fn=<MmBackward>)\n",
      "[Parameter containing:\n",
      "tensor([[ 0.4023, -0.2521],\n",
      "        [-0.3493,  2.1044],\n",
      "        [-0.0991, -1.1768],\n",
      "        [-0.2976,  2.8471]], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0387,  0.5440],\n",
      "        [-0.6702, -0.2437],\n",
      "        [ 0.4380,  0.3827],\n",
      "        [ 0.1817, -0.5709]], device='cuda:1', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.emb = nn.Embedding(4,2)\n",
    "        self.linear = nn.Linear(2,4,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(self.emb(x).mean(dim=1))\n",
    "    \n",
    "\n",
    "model = MyModel()\n",
    "x = torch.tensor([[0,2,2,3,1],[1,2,2,3,1],[2,2,2,3,1]])\n",
    "print(x)\n",
    "print(model.forward(x))\n",
    "print(list(model.parameters()))\n",
    "model = model.to(device=torch.device('cuda:1'))\n",
    "print(x)\n",
    "x = x.to(device=torch.device('cuda:1'))\n",
    "print(x)\n",
    "print(model.forward(x))\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Definir el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "cifar10_train = dset.CIFAR10('CIFAR10/train/', train=True, download=True, transform=transform)\n",
    "cifar10_test = dset.CIFAR10('CIFAR10/test/', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: pasarle el dataset al entrenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelTrainer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 train_dataset,\n",
    "                 test_dataset,\n",
    "                 batch_size=64,\n",
    "                 val_size=.02):\n",
    "        \n",
    "        # Model:\n",
    "        self.model = model\n",
    "        \n",
    "        # Data:\n",
    "        tr, val, te = self.generate_data_batches(train_dataset, test_dataset,batch_size,val_size)\n",
    "        self.train_dataloader, self.val_dataloader, self.test_dataloader = tr, val, te\n",
    "        \n",
    "        # Data-types:\n",
    "        self.input_dtype = next(iter(self.train_dataloader))[0].dtype\n",
    "        self.target_dtype = next(iter(self.train_dataloader))[1].dtype\n",
    "        \n",
    "        self.first_time = True\n",
    "        self.batch_len = len(self.train_dataloader)\n",
    "        \n",
    "        print('Model trainer created:')\n",
    "        train_samples = int((1 - val_size) * len(train_dataset)) \n",
    "        val_samples = len(train_dataset) - train_samples\n",
    "        test_samples = len(test_dataset)\n",
    "        total_samples = train_samples + val_samples + test_samples\n",
    "        percent_val, percent_test = int((val_samples / total_samples) * 100), int((test_samples / total_samples) * 100)\n",
    "        print('Number of training samples: {} ({}%)'.format(train_samples, 100 - percent_val - percent_test))\n",
    "        print('Number of validation samples: {} ({}%)'.format(val_samples, percent_val))\n",
    "        print('Number of test samples: {} ({}%)'.format(test_samples, percent_test))\n",
    "        print('Number of train batches: {}'.format(self.batch_len))\n",
    "        print('Number of samples per batch: {}'.format(batch_size))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    def generate_data_batches(self,train_dataset, test_dataset, # Train y test datasets\n",
    "                              batch_size = 64, # Tamaño del batch\n",
    "                              val_size = .02): # Proporción de muestras utilizadas para validación \n",
    "    \n",
    "        \"\"\"\n",
    "            Función para iterar sobre los batches de muestras. \n",
    "            Devuelve los dataloaders de train / validation / test.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Separo las muestras aleatoriamente en Train y Validation:\n",
    "        NUM_TRAIN = int((1 - val_size) * len(train_dataset)) \n",
    "        samples_idx = torch.randperm(len(train_dataset))\n",
    "        train_samples_idx = samples_idx[:NUM_TRAIN]\n",
    "        val_samples_idx = samples_idx[NUM_TRAIN:]\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices) # sampler\n",
    "\n",
    "        # Dataloader para las muestras de entrenamiento:\n",
    "        train_dataloader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      sampler=my_sampler(train_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de validación:\n",
    "        val_dataloader = DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=my_sampler(val_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de testeo:\n",
    "        test_dataloader = DataLoader(test_dataset, \n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "        return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "    \n",
    "    def InitParameters(self,from_pretrained=None,use_gpu=None, **kwargs):\n",
    "       \n",
    "        if from_pretrained is not None:\n",
    "            pass\n",
    "\n",
    "        # Defino el dispositivo sobre el cual trabajar:\n",
    "        if use_gpu == 0:\n",
    "            self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        elif use_gpu == 1:\n",
    "            self.device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        elif use_gpu is None:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.model.init_parameters(kwargs)\n",
    "        self.model = self.model.to(device=self.device)\n",
    "\n",
    "        \n",
    "    def SGDTrain(self, epochs=1, learning_rate=1e-1, sample_loss_every=100, check_on_train=False):\n",
    "        \n",
    "        if self.first_time:\n",
    "            print('Starting training...')\n",
    "            n_iter = 0\n",
    "            self.performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "            self.first_time = False\n",
    "        else:\n",
    "            n_iter = self.performance_history['iter'][-1]\n",
    "            print('Resuming training...')\n",
    "        \n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        print('Optimization method: Stochastic Gradient Descent')\n",
    "        print('Learning Rate: {:.2g}'.format(learning_rate))\n",
    "        print('Number of epochs: {}'.format(epochs))\n",
    "        print('Running on device \"{}\"'.format(self.device))\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            for e in range(epochs):\n",
    "                for t, (x,y) in enumerate(self.train_dataloader):\n",
    "\n",
    "                    x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "                    y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                    optimizer.zero_grad() # Llevo a cero los gradientes de la red\n",
    "                    scores = self.model(x) # Calculo la salida de la red\n",
    "                    loss = self.model.loss(scores,y) # Calculo el valor de la loss\n",
    "                    loss.backward() # Calculo los gradientes\n",
    "                    optimizer.step() # Actualizo los parámetros\n",
    "\n",
    "                    if (e * self.batch_len + t) % sample_loss_every == 0:\n",
    "                        num_correct_val, num_samples_val = check_accuracy('validation')\n",
    "                        performance_history['iter'].append(e * self.batch_len + t + n_iter)\n",
    "                        performance_history['loss'].append(loss.item())\n",
    "                        performance_history['accuracy'].append(float(num_correct_val / num_samples_val))\n",
    "                        print('Epoch: {}, Batch number: {}'.format(e+1, t))\n",
    "                        print('Accuracy on validation dataset: {}/{} ({:.2f}%)'.format(num_correct_val, num_samples_val, 100 * float(num_correct_val) / num_samples_val))\n",
    "                        print()\n",
    "\n",
    "                        if check_on_train:\n",
    "                            num_correct_train, num_samples_train = check_accuracy('train')\n",
    "                            print('Accuracy on train dataset: {}/{} ({:.2f}%)'.format(num_correct_train, num_samples_train, 100 * float(num_correct_train) / num_samples_train))\n",
    "                            print()\n",
    "\n",
    "            print('Training finished')\n",
    "            print()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "\n",
    "            print('Exiting training...')\n",
    "            print()    \n",
    "\n",
    "    def check_accuracy(self, dataset='validation'):\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        if dataset == 'train':\n",
    "            loader = self.train_dataloader\n",
    "        elif dataset == 'validation':\n",
    "            loader = self.val_dataloader\n",
    "        elif dataset == 'test':\n",
    "            loader = self.test_dataloader\n",
    "        else:\n",
    "            raise AttributeError('Please specify on which dataset to perform de accuracy calculation')\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=self.input_dtype)  \n",
    "                y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                scores = self.model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "\n",
    "        self.model.train()\n",
    "        return num_correct, num_samples\n",
    "\n",
    "    def CheckResultsOnTest(self):\n",
    "        \n",
    "        total_corrects = 0\n",
    "        total_samples = 0\n",
    "        total_performance = 0.\n",
    "        \n",
    "        for (x,y) in enumerate(self.test_dataloader):\n",
    "            x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "            y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "            num_correct, num_samples = check_accuracy('test')\n",
    "            total_corrects += num_corrects\n",
    "            total_samples += num_samples\n",
    "            total_performance += float(num_correct / num_samples)\n",
    "        \n",
    "        print('Final accuracy on test set: {}/{} ({}%)'.format(total_corrects,total_samples,total_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def init_parameters(self,a):\n",
    "        pass\n",
    "    \n",
    "    def loss(self,output,target):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(output, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trainer created:\n",
      "Number of training samples: 49000 (83%)\n",
      "Number of validation samples: 1000 (1%)\n",
      "Number of test samples: 10000 (16%)\n",
      "Number of train batches: 766\n",
      "Number of samples per batch: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(net, cifar10_train, cifar10_test, batch_size=64, val_size=.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1310, -0.1424, -0.1720],\n",
      "          [ 0.1197,  0.1184, -0.2506],\n",
      "          [ 0.2374, -0.0751, -0.0696]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0798,  0.0304,  0.1062],\n",
      "          [-0.1129, -0.2112,  0.0842],\n",
      "          [-0.1649,  0.0471,  0.2805]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1573, -0.2870,  0.0680],\n",
      "          [ 0.0907, -0.1963,  0.3085],\n",
      "          [-0.0539, -0.1999, -0.0290]]],\n",
      "\n",
      "\n",
      "        [[[-0.1169, -0.2866, -0.1622],\n",
      "          [ 0.0678, -0.1545, -0.2050],\n",
      "          [ 0.2296,  0.0229,  0.3149]]],\n",
      "\n",
      "\n",
      "        [[[-0.1902, -0.2176, -0.2299],\n",
      "          [ 0.3053,  0.1966, -0.1620],\n",
      "          [ 0.0115,  0.1770, -0.1851]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2123,  0.1677, -0.0322],\n",
      "          [ 0.0689,  0.0589, -0.1155],\n",
      "          [ 0.1154,  0.1046, -0.2012]]]], device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3007,  0.2125, -0.2648,  0.2078, -0.0942, -0.1546], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0338,  0.0019, -0.1326],\n",
      "          [ 0.0267,  0.1319,  0.0918],\n",
      "          [ 0.0978,  0.0941, -0.0599]],\n",
      "\n",
      "         [[-0.0771, -0.0318, -0.0696],\n",
      "          [ 0.0532, -0.0048, -0.0801],\n",
      "          [-0.0860,  0.1213,  0.0792]],\n",
      "\n",
      "         [[-0.0200,  0.1024, -0.0488],\n",
      "          [ 0.1290,  0.0986, -0.0381],\n",
      "          [-0.0957, -0.0127,  0.1329]],\n",
      "\n",
      "         [[ 0.0663,  0.0754,  0.1184],\n",
      "          [-0.1026,  0.1112, -0.1232],\n",
      "          [-0.0321,  0.0496, -0.0220]],\n",
      "\n",
      "         [[ 0.1085, -0.0381,  0.1090],\n",
      "          [-0.0659,  0.0819,  0.0518],\n",
      "          [ 0.0876,  0.0102, -0.0157]],\n",
      "\n",
      "         [[ 0.0050, -0.0847, -0.1258],\n",
      "          [ 0.0320, -0.0760,  0.0412],\n",
      "          [-0.0128, -0.0319, -0.0678]]],\n",
      "\n",
      "\n",
      "        [[[-0.0003,  0.1083,  0.0386],\n",
      "          [-0.0776,  0.0609,  0.0291],\n",
      "          [-0.0516, -0.0502, -0.0794]],\n",
      "\n",
      "         [[ 0.0391,  0.0834, -0.1258],\n",
      "          [ 0.1184, -0.0339,  0.1093],\n",
      "          [ 0.0591, -0.0180,  0.0820]],\n",
      "\n",
      "         [[ 0.0219,  0.0673,  0.0161],\n",
      "          [-0.0266, -0.1247, -0.0020],\n",
      "          [-0.1047,  0.0954, -0.1050]],\n",
      "\n",
      "         [[ 0.0920,  0.0593,  0.0404],\n",
      "          [ 0.0580, -0.0537, -0.1145],\n",
      "          [ 0.0797,  0.0178, -0.0587]],\n",
      "\n",
      "         [[ 0.0423,  0.1114,  0.0966],\n",
      "          [ 0.0254, -0.1140,  0.1282],\n",
      "          [ 0.0198, -0.0566, -0.0338]],\n",
      "\n",
      "         [[ 0.0240,  0.0841,  0.1036],\n",
      "          [ 0.0384,  0.0644, -0.0480],\n",
      "          [ 0.1043,  0.0155,  0.1070]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0636,  0.1207,  0.0707],\n",
      "          [ 0.1276, -0.0322, -0.0430],\n",
      "          [-0.1097, -0.1353, -0.0495]],\n",
      "\n",
      "         [[ 0.0138,  0.0421, -0.0987],\n",
      "          [-0.1158, -0.0669,  0.1287],\n",
      "          [ 0.0990, -0.0594,  0.0643]],\n",
      "\n",
      "         [[ 0.0489, -0.0479,  0.0775],\n",
      "          [ 0.0118, -0.1143,  0.0453],\n",
      "          [-0.0923, -0.0602,  0.1264]],\n",
      "\n",
      "         [[-0.0370, -0.0739, -0.1281],\n",
      "          [-0.0469, -0.0371, -0.0245],\n",
      "          [-0.0444,  0.0768,  0.0950]],\n",
      "\n",
      "         [[ 0.0038,  0.0978, -0.0411],\n",
      "          [-0.0917,  0.0176, -0.0973],\n",
      "          [ 0.1042, -0.0466,  0.0471]],\n",
      "\n",
      "         [[ 0.0088, -0.0790,  0.0344],\n",
      "          [ 0.0217,  0.0242, -0.0288],\n",
      "          [ 0.0132,  0.0643,  0.0521]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0612,  0.1068, -0.1226],\n",
      "          [-0.0144,  0.1194, -0.1327],\n",
      "          [-0.1001,  0.0725,  0.0177]],\n",
      "\n",
      "         [[-0.0020,  0.0613,  0.1215],\n",
      "          [-0.0136,  0.0059, -0.1260],\n",
      "          [-0.0947, -0.0560, -0.0286]],\n",
      "\n",
      "         [[ 0.0858,  0.1116, -0.1316],\n",
      "          [-0.0814, -0.1272, -0.0149],\n",
      "          [-0.0745, -0.0549, -0.0344]],\n",
      "\n",
      "         [[ 0.0736, -0.1267,  0.0539],\n",
      "          [ 0.0603,  0.1087,  0.1069],\n",
      "          [ 0.0999, -0.0788, -0.1191]],\n",
      "\n",
      "         [[-0.0244,  0.1118,  0.0241],\n",
      "          [ 0.0643, -0.0115,  0.1187],\n",
      "          [-0.0626, -0.0578,  0.0235]],\n",
      "\n",
      "         [[-0.0117, -0.0858, -0.0988],\n",
      "          [-0.0231, -0.1184, -0.0452],\n",
      "          [-0.0752,  0.0738,  0.1092]]],\n",
      "\n",
      "\n",
      "        [[[-0.0212, -0.0530,  0.1125],\n",
      "          [-0.0913,  0.0978, -0.0827],\n",
      "          [ 0.0538, -0.0103,  0.0401]],\n",
      "\n",
      "         [[-0.0119, -0.1294,  0.0010],\n",
      "          [ 0.0661,  0.0704, -0.1214],\n",
      "          [ 0.0591,  0.0028, -0.0066]],\n",
      "\n",
      "         [[-0.0539, -0.0869,  0.0109],\n",
      "          [-0.0453,  0.0961, -0.1326],\n",
      "          [-0.0163,  0.0214, -0.0125]],\n",
      "\n",
      "         [[ 0.0885,  0.0684, -0.0067],\n",
      "          [ 0.0499,  0.0975,  0.0908],\n",
      "          [-0.0665, -0.0157,  0.0841]],\n",
      "\n",
      "         [[-0.1144, -0.0447,  0.0404],\n",
      "          [-0.0165, -0.0232, -0.1130],\n",
      "          [-0.1142, -0.0886,  0.1010]],\n",
      "\n",
      "         [[-0.0679, -0.0467, -0.1319],\n",
      "          [-0.1130,  0.1291,  0.0413],\n",
      "          [-0.0889, -0.0471, -0.1205]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0069,  0.0736,  0.0966],\n",
      "          [ 0.0752,  0.0983, -0.0628],\n",
      "          [ 0.0699, -0.0138,  0.0157]],\n",
      "\n",
      "         [[-0.1073,  0.1320,  0.0638],\n",
      "          [-0.0763,  0.1208,  0.0385],\n",
      "          [-0.1311,  0.0416,  0.0818]],\n",
      "\n",
      "         [[ 0.0897, -0.0180, -0.0834],\n",
      "          [ 0.0917, -0.0802,  0.0489],\n",
      "          [ 0.0459, -0.1041,  0.0305]],\n",
      "\n",
      "         [[-0.0785,  0.0441, -0.0085],\n",
      "          [-0.0358, -0.0586,  0.0515],\n",
      "          [-0.0482, -0.0105, -0.0599]],\n",
      "\n",
      "         [[-0.0838,  0.1243, -0.0756],\n",
      "          [-0.1302,  0.1263,  0.0319],\n",
      "          [-0.0283,  0.1137, -0.0980]],\n",
      "\n",
      "         [[-0.1043, -0.0365, -0.1072],\n",
      "          [ 0.0565, -0.0633, -0.1301],\n",
      "          [ 0.0614,  0.0567,  0.0806]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0556,  0.0295, -0.0170],\n",
      "          [ 0.0132,  0.1343, -0.0174],\n",
      "          [ 0.0640,  0.0419, -0.1041]],\n",
      "\n",
      "         [[-0.0933,  0.0777,  0.0414],\n",
      "          [ 0.1263,  0.0933,  0.0725],\n",
      "          [-0.0425,  0.1142,  0.1082]],\n",
      "\n",
      "         [[ 0.0466, -0.0378,  0.0090],\n",
      "          [ 0.0076,  0.0168, -0.0822],\n",
      "          [ 0.0179, -0.0177, -0.1134]],\n",
      "\n",
      "         [[ 0.0255,  0.0315, -0.1092],\n",
      "          [ 0.0635, -0.1087,  0.0819],\n",
      "          [-0.1002, -0.0341,  0.1344]],\n",
      "\n",
      "         [[-0.0711,  0.0510, -0.1177],\n",
      "          [ 0.0013,  0.0640, -0.0239],\n",
      "          [-0.0608, -0.0671, -0.1287]],\n",
      "\n",
      "         [[-0.0970, -0.1246,  0.0527],\n",
      "          [-0.0521, -0.0334,  0.1089],\n",
      "          [ 0.1141, -0.0395,  0.0433]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0806, -0.0469, -0.0673],\n",
      "          [-0.0065,  0.1232, -0.0577],\n",
      "          [ 0.0493, -0.0972, -0.0368]],\n",
      "\n",
      "         [[-0.0617,  0.1093,  0.0471],\n",
      "          [-0.0389,  0.0587,  0.0656],\n",
      "          [-0.0608,  0.0151,  0.0335]],\n",
      "\n",
      "         [[ 0.0628,  0.0638,  0.0120],\n",
      "          [-0.1052, -0.0457,  0.1237],\n",
      "          [ 0.0102, -0.0844, -0.0124]],\n",
      "\n",
      "         [[ 0.0732, -0.1347,  0.0788],\n",
      "          [-0.1352, -0.0405,  0.0087],\n",
      "          [ 0.0063, -0.0750, -0.0130]],\n",
      "\n",
      "         [[-0.0386,  0.0831,  0.0038],\n",
      "          [ 0.1036,  0.0971, -0.0489],\n",
      "          [ 0.1178, -0.0386,  0.0566]],\n",
      "\n",
      "         [[-0.1303, -0.0467, -0.1250],\n",
      "          [-0.1057,  0.0121,  0.1313],\n",
      "          [ 0.0284, -0.1230, -0.0206]]],\n",
      "\n",
      "\n",
      "        [[[-0.0088, -0.1027,  0.0992],\n",
      "          [-0.0167, -0.1350,  0.0931],\n",
      "          [ 0.0590,  0.0114, -0.0913]],\n",
      "\n",
      "         [[ 0.0463, -0.1094,  0.0154],\n",
      "          [ 0.0551,  0.0394,  0.0195],\n",
      "          [ 0.0797, -0.1142,  0.0842]],\n",
      "\n",
      "         [[ 0.0557,  0.0151, -0.1291],\n",
      "          [-0.0272, -0.1300, -0.1036],\n",
      "          [-0.0799,  0.1029,  0.0845]],\n",
      "\n",
      "         [[-0.0463,  0.1275,  0.1217],\n",
      "          [-0.0505, -0.0378,  0.1264],\n",
      "          [ 0.0756,  0.0069,  0.0870]],\n",
      "\n",
      "         [[-0.1102, -0.0568, -0.1093],\n",
      "          [ 0.1284, -0.0428, -0.0436],\n",
      "          [-0.0880,  0.1225, -0.1202]],\n",
      "\n",
      "         [[ 0.1295,  0.0893, -0.0630],\n",
      "          [-0.0744, -0.0717, -0.0098],\n",
      "          [ 0.0814,  0.1054,  0.0215]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0656, -0.0440,  0.0680],\n",
      "          [ 0.0213, -0.0410,  0.0797],\n",
      "          [ 0.0557, -0.0914,  0.1302]],\n",
      "\n",
      "         [[ 0.0677, -0.0070,  0.0995],\n",
      "          [ 0.0183, -0.0260, -0.0612],\n",
      "          [ 0.1318,  0.1123, -0.0716]],\n",
      "\n",
      "         [[ 0.0914, -0.0435, -0.0822],\n",
      "          [ 0.0785, -0.0576,  0.0758],\n",
      "          [ 0.0469,  0.0148,  0.0861]],\n",
      "\n",
      "         [[ 0.0696,  0.1359, -0.0168],\n",
      "          [ 0.0988, -0.1017,  0.0079],\n",
      "          [ 0.1086,  0.0212, -0.0726]],\n",
      "\n",
      "         [[ 0.1210,  0.0761, -0.0200],\n",
      "          [ 0.0082,  0.0103, -0.0172],\n",
      "          [ 0.0223, -0.0614,  0.1325]],\n",
      "\n",
      "         [[-0.0195,  0.0967,  0.0239],\n",
      "          [ 0.1189,  0.0353,  0.0952],\n",
      "          [-0.0649,  0.0735, -0.0563]]],\n",
      "\n",
      "\n",
      "        [[[-0.1320,  0.1350,  0.0056],\n",
      "          [ 0.0239,  0.0643,  0.1292],\n",
      "          [ 0.1222, -0.1229, -0.0831]],\n",
      "\n",
      "         [[ 0.1030,  0.0465,  0.0600],\n",
      "          [-0.0644, -0.0503,  0.1023],\n",
      "          [-0.0090, -0.1354, -0.0173]],\n",
      "\n",
      "         [[ 0.0109, -0.0796,  0.1206],\n",
      "          [-0.0981,  0.1079,  0.0448],\n",
      "          [-0.0749,  0.0319,  0.0522]],\n",
      "\n",
      "         [[ 0.0613,  0.0340,  0.0838],\n",
      "          [ 0.0753,  0.0274, -0.0130],\n",
      "          [ 0.1141,  0.1300, -0.0058]],\n",
      "\n",
      "         [[ 0.0425,  0.0646, -0.0366],\n",
      "          [ 0.0431,  0.1298, -0.0069],\n",
      "          [-0.1012,  0.0374,  0.0705]],\n",
      "\n",
      "         [[-0.1119, -0.0432, -0.0650],\n",
      "          [-0.1037,  0.0547, -0.0722],\n",
      "          [-0.0145,  0.1047,  0.1197]]],\n",
      "\n",
      "\n",
      "        [[[-0.0184, -0.0974,  0.0981],\n",
      "          [ 0.0683,  0.0419,  0.0661],\n",
      "          [-0.0230,  0.0931,  0.0164]],\n",
      "\n",
      "         [[-0.1017, -0.0371,  0.0289],\n",
      "          [ 0.0274, -0.0569, -0.1007],\n",
      "          [ 0.1356, -0.0128,  0.0677]],\n",
      "\n",
      "         [[-0.0263, -0.0274,  0.0366],\n",
      "          [ 0.0918,  0.0236, -0.1183],\n",
      "          [ 0.0954,  0.0302, -0.1315]],\n",
      "\n",
      "         [[ 0.0135,  0.0510, -0.0774],\n",
      "          [ 0.0789, -0.0494, -0.0181],\n",
      "          [ 0.0066,  0.1040, -0.0597]],\n",
      "\n",
      "         [[ 0.0926,  0.0285, -0.0807],\n",
      "          [-0.1122,  0.0952, -0.0791],\n",
      "          [ 0.1281,  0.1028, -0.0431]],\n",
      "\n",
      "         [[ 0.0272,  0.0534, -0.0101],\n",
      "          [ 0.0280, -0.0894,  0.0368],\n",
      "          [ 0.0126, -0.0293, -0.1165]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0884,  0.0032,  0.0940],\n",
      "          [-0.0028,  0.0335, -0.0907],\n",
      "          [-0.0457, -0.0678, -0.0279]],\n",
      "\n",
      "         [[ 0.0718,  0.0796, -0.0507],\n",
      "          [ 0.0511, -0.0545,  0.0441],\n",
      "          [ 0.0178,  0.1126,  0.1118]],\n",
      "\n",
      "         [[ 0.0720,  0.0062, -0.0803],\n",
      "          [-0.0245,  0.0162, -0.1004],\n",
      "          [-0.1133,  0.0261,  0.1182]],\n",
      "\n",
      "         [[ 0.1138, -0.0535, -0.1352],\n",
      "          [-0.1349,  0.1111,  0.1070],\n",
      "          [ 0.0545, -0.1204,  0.0011]],\n",
      "\n",
      "         [[-0.0182,  0.0046, -0.1311],\n",
      "          [-0.0250, -0.1020, -0.0487],\n",
      "          [ 0.0481,  0.0277, -0.0539]],\n",
      "\n",
      "         [[ 0.1314,  0.1087,  0.0416],\n",
      "          [-0.0458,  0.0452, -0.1133],\n",
      "          [-0.0907,  0.0796, -0.0197]]],\n",
      "\n",
      "\n",
      "        [[[-0.0990,  0.1187,  0.1025],\n",
      "          [-0.0674,  0.0811, -0.0828],\n",
      "          [ 0.0704,  0.1076,  0.0412]],\n",
      "\n",
      "         [[-0.0242, -0.0348,  0.0283],\n",
      "          [-0.0879,  0.0357, -0.0901],\n",
      "          [ 0.1273,  0.1181, -0.1051]],\n",
      "\n",
      "         [[-0.1144,  0.0181,  0.1042],\n",
      "          [ 0.1081,  0.0890,  0.0077],\n",
      "          [-0.0890,  0.0540,  0.0941]],\n",
      "\n",
      "         [[ 0.0497, -0.0459,  0.0832],\n",
      "          [-0.0833,  0.1052,  0.0985],\n",
      "          [-0.0446,  0.1173, -0.0708]],\n",
      "\n",
      "         [[ 0.0984, -0.0299,  0.0728],\n",
      "          [-0.0078, -0.0202,  0.1311],\n",
      "          [-0.0406,  0.0265, -0.0592]],\n",
      "\n",
      "         [[ 0.0232,  0.0365, -0.1141],\n",
      "          [-0.1170, -0.0341, -0.0113],\n",
      "          [-0.0290, -0.1251,  0.0376]]],\n",
      "\n",
      "\n",
      "        [[[-0.0100,  0.0534, -0.0896],\n",
      "          [-0.1350, -0.0235,  0.0021],\n",
      "          [-0.0744, -0.0733,  0.1020]],\n",
      "\n",
      "         [[-0.1272, -0.0596, -0.1257],\n",
      "          [-0.1108, -0.0005,  0.0981],\n",
      "          [-0.0678, -0.0471,  0.0760]],\n",
      "\n",
      "         [[ 0.0835,  0.0722, -0.0812],\n",
      "          [ 0.0486, -0.1262,  0.1252],\n",
      "          [ 0.0331,  0.0126,  0.0474]],\n",
      "\n",
      "         [[ 0.1141, -0.1137, -0.1198],\n",
      "          [ 0.0156, -0.0571, -0.0326],\n",
      "          [-0.0704, -0.1151, -0.1158]],\n",
      "\n",
      "         [[ 0.0528,  0.0965, -0.1356],\n",
      "          [-0.1180,  0.0332,  0.0341],\n",
      "          [-0.0810, -0.0695, -0.0361]],\n",
      "\n",
      "         [[ 0.0424,  0.0942,  0.0995],\n",
      "          [ 0.1114,  0.0602,  0.0240],\n",
      "          [ 0.0087,  0.0063,  0.1081]]],\n",
      "\n",
      "\n",
      "        [[[-0.0649,  0.0181, -0.0211],\n",
      "          [-0.1116,  0.0486, -0.1200],\n",
      "          [ 0.0530,  0.0429, -0.0623]],\n",
      "\n",
      "         [[-0.0643,  0.1111,  0.0148],\n",
      "          [-0.0832,  0.0062,  0.0782],\n",
      "          [-0.0645, -0.0236,  0.0060]],\n",
      "\n",
      "         [[ 0.0271, -0.0249,  0.0544],\n",
      "          [-0.1297,  0.0827, -0.1153],\n",
      "          [ 0.0412,  0.0756,  0.0437]],\n",
      "\n",
      "         [[ 0.0392,  0.0142,  0.1287],\n",
      "          [-0.0773, -0.0013,  0.0134],\n",
      "          [ 0.0346,  0.1321,  0.0028]],\n",
      "\n",
      "         [[ 0.0433, -0.0893, -0.0180],\n",
      "          [ 0.0996, -0.0198,  0.0983],\n",
      "          [-0.0409, -0.0868, -0.0524]],\n",
      "\n",
      "         [[ 0.0934,  0.1207,  0.0779],\n",
      "          [ 0.1303, -0.1181,  0.1128],\n",
      "          [-0.0979, -0.0202, -0.1191]]]], device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0363, -0.0216, -0.0265, -0.1129,  0.0971, -0.0307, -0.1033,  0.0539,\n",
      "         0.1055, -0.1254,  0.1322, -0.0964,  0.0510, -0.0186, -0.1081,  0.1216],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0025, -0.0059, -0.0296,  ..., -0.0156, -0.0207, -0.0325],\n",
      "        [ 0.0344,  0.0410, -0.0397,  ..., -0.0324, -0.0240, -0.0169],\n",
      "        [-0.0150, -0.0126,  0.0383,  ..., -0.0364, -0.0259, -0.0148],\n",
      "        ...,\n",
      "        [-0.0101, -0.0184,  0.0178,  ..., -0.0346, -0.0007,  0.0231],\n",
      "        [ 0.0352,  0.0375, -0.0175,  ..., -0.0194, -0.0094, -0.0040],\n",
      "        [-0.0211,  0.0286,  0.0006,  ..., -0.0186, -0.0029,  0.0080]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.1899e-03,  2.6380e-02, -1.3474e-03,  2.3744e-02,  1.6357e-03,\n",
      "         1.4433e-02,  3.9135e-02,  1.8354e-02,  6.7503e-03,  1.3277e-02,\n",
      "        -3.1284e-02,  2.7802e-02, -5.3283e-05,  3.7997e-02, -4.0521e-02,\n",
      "        -1.3839e-02, -2.6110e-02,  3.2283e-02,  1.5308e-02, -2.6817e-02,\n",
      "        -3.6182e-02,  2.8206e-02,  2.9115e-02, -3.5480e-02, -2.6218e-02,\n",
      "        -9.0562e-03,  6.1429e-03, -3.1880e-02, -2.7718e-02, -2.9868e-02,\n",
      "        -2.8822e-02, -3.3734e-02, -2.3627e-02, -1.4611e-02, -1.8625e-02,\n",
      "        -1.7803e-02,  3.0810e-02, -5.3169e-03, -3.1380e-02, -2.3463e-02,\n",
      "        -3.2587e-02,  2.9317e-02,  2.6664e-02,  4.0077e-02,  2.5635e-02,\n",
      "        -9.0472e-03,  2.0872e-02, -3.8295e-02, -4.0674e-02, -7.1153e-03,\n",
      "        -8.6357e-04, -1.3942e-02,  2.2195e-02,  3.4217e-02,  2.4086e-02,\n",
      "        -3.3297e-02,  8.8679e-03,  4.0687e-02,  2.7910e-02, -2.4289e-03,\n",
      "         1.2769e-02,  2.2598e-02,  3.6733e-02,  3.5704e-02, -2.6134e-02,\n",
      "         1.1402e-02,  1.5884e-02, -3.9574e-02,  1.3049e-02,  1.8963e-02,\n",
      "         2.7160e-02, -3.8777e-02,  1.1911e-02, -2.4118e-02, -4.0435e-02,\n",
      "        -2.0501e-03,  2.2240e-02, -3.9792e-02, -3.9249e-02, -3.4787e-03,\n",
      "        -3.5673e-02,  4.8097e-04,  2.0213e-02, -4.1050e-02, -1.2662e-02,\n",
      "         1.8803e-02,  1.1996e-02, -1.6672e-02, -1.6142e-02,  4.4144e-03,\n",
      "         4.7885e-03, -1.1908e-04, -2.1668e-02, -2.4277e-02, -2.7162e-02,\n",
      "        -1.5730e-02,  2.5715e-02, -1.0576e-02, -2.5383e-02,  3.1652e-02,\n",
      "        -3.0797e-02,  1.5281e-02,  1.9993e-02,  3.0098e-02,  2.3674e-02,\n",
      "         1.7431e-02,  2.5625e-02,  1.2422e-02, -3.0262e-02, -1.2279e-02,\n",
      "         1.9284e-03, -5.5759e-03, -1.0505e-02, -4.5649e-03,  4.0221e-02,\n",
      "         2.3918e-02,  1.1226e-02, -2.0208e-03, -2.4261e-02, -1.0159e-02],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0110, -0.0658, -0.0859,  ..., -0.0280,  0.0108,  0.0210],\n",
      "        [-0.0838,  0.0904,  0.0563,  ..., -0.0017,  0.0673, -0.0071],\n",
      "        [-0.0306,  0.0880,  0.0065,  ...,  0.0778, -0.0440,  0.0841],\n",
      "        ...,\n",
      "        [ 0.0580,  0.0535,  0.0585,  ..., -0.0741,  0.0270, -0.0041],\n",
      "        [ 0.0063,  0.0134,  0.0203,  ...,  0.0615,  0.0419, -0.0035],\n",
      "        [-0.0307,  0.0242,  0.0907,  ..., -0.0156, -0.0631, -0.0070]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0029,  0.0013, -0.0730,  0.0641,  0.0008,  0.0871,  0.0437,  0.0592,\n",
      "        -0.0858, -0.0467,  0.0899, -0.0560, -0.0391,  0.0755,  0.0879, -0.0460,\n",
      "         0.0104, -0.0348, -0.0218,  0.0890,  0.0651,  0.0758, -0.0236, -0.0288,\n",
      "        -0.0413,  0.0611,  0.0307,  0.0501,  0.0753, -0.0354, -0.0336, -0.0209,\n",
      "         0.0783,  0.0645,  0.0886, -0.0514, -0.0074,  0.0813,  0.0715,  0.0298,\n",
      "         0.0106,  0.0765, -0.0801, -0.0549, -0.0077, -0.0609, -0.0158,  0.0204,\n",
      "        -0.0768,  0.0259,  0.0734, -0.0516,  0.0308,  0.0010, -0.0185, -0.0830,\n",
      "        -0.0135,  0.0868, -0.0756,  0.0491,  0.0108,  0.0047,  0.0751,  0.0384,\n",
      "        -0.0157,  0.0077, -0.0497,  0.0012,  0.0029,  0.0477, -0.0560, -0.0438,\n",
      "         0.0168,  0.0352, -0.0847, -0.0438, -0.0042,  0.0706,  0.0344, -0.0732,\n",
      "         0.0565, -0.0208,  0.0184, -0.0690], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1035,  0.0986,  0.0526, -0.0027, -0.0854, -0.0669,  0.0524, -0.0499,\n",
      "          0.0949, -0.0382, -0.0108, -0.0618, -0.0645,  0.0858, -0.0280, -0.0794,\n",
      "          0.0393,  0.0173, -0.0032,  0.0962,  0.0401, -0.0294,  0.0494,  0.0742,\n",
      "          0.0046, -0.0248,  0.0835,  0.1008, -0.0968, -0.0282, -0.1040,  0.1049,\n",
      "          0.0859,  0.0217, -0.0212, -0.0136, -0.0642, -0.0885,  0.0394,  0.0645,\n",
      "          0.0875,  0.0293, -0.0395, -0.1065, -0.0751, -0.0203, -0.0342,  0.0608,\n",
      "         -0.0726,  0.0293,  0.0677,  0.0398,  0.0704,  0.0117, -0.1016,  0.1007,\n",
      "          0.0991, -0.0615,  0.0737, -0.0848, -0.0003,  0.0975, -0.0637,  0.0647,\n",
      "          0.0597, -0.0886,  0.1025,  0.0696,  0.0362, -0.0192,  0.0036,  0.1052,\n",
      "          0.0128, -0.0027,  0.0859,  0.0486,  0.0409,  0.0409,  0.0556,  0.0601,\n",
      "         -0.0618,  0.0362, -0.1060, -0.0809],\n",
      "        [ 0.0101, -0.0642, -0.0296,  0.0052, -0.0449,  0.0259, -0.0237, -0.0285,\n",
      "          0.0207,  0.0452, -0.0412,  0.0310, -0.0966, -0.0871,  0.0958, -0.0574,\n",
      "          0.0738,  0.0836,  0.0457,  0.0703, -0.0187, -0.0964,  0.0699, -0.0008,\n",
      "         -0.0046,  0.0167, -0.1086, -0.0404,  0.0760,  0.0933,  0.0586, -0.0679,\n",
      "          0.0979, -0.0244, -0.0234, -0.0547, -0.1013,  0.0470,  0.0852, -0.0605,\n",
      "          0.0363,  0.0607,  0.0396,  0.0098,  0.0616, -0.0666,  0.0217,  0.0593,\n",
      "          0.0011, -0.0279,  0.0062,  0.0676,  0.0394, -0.0786, -0.0836, -0.0640,\n",
      "         -0.1013,  0.0360, -0.0390, -0.0215, -0.0744, -0.0755,  0.0604,  0.0248,\n",
      "         -0.1005,  0.0237,  0.0992, -0.0983,  0.0146, -0.0685, -0.0928, -0.0664,\n",
      "         -0.0570,  0.0484, -0.0616,  0.0155, -0.1017, -0.1034, -0.0296, -0.0473,\n",
      "          0.0297,  0.0488, -0.0093,  0.0831],\n",
      "        [-0.0593, -0.0810,  0.0920,  0.0389, -0.0990,  0.0575,  0.0500,  0.0341,\n",
      "          0.0453,  0.0095, -0.0518,  0.0745,  0.0960,  0.0999,  0.0837, -0.0495,\n",
      "          0.0566, -0.0931, -0.0446, -0.0033,  0.0354,  0.0174,  0.0107, -0.0861,\n",
      "         -0.0711, -0.0566, -0.0358, -0.0046, -0.0988,  0.0505,  0.0264,  0.0391,\n",
      "          0.0002,  0.0437,  0.0092,  0.1075,  0.0927,  0.0322, -0.0175, -0.0587,\n",
      "          0.0203, -0.0189,  0.0262, -0.0773,  0.0062, -0.0374,  0.0802, -0.0253,\n",
      "         -0.0663,  0.0384,  0.0047, -0.0979, -0.0675, -0.0802,  0.0358, -0.0890,\n",
      "          0.0222,  0.0632,  0.0470, -0.1070, -0.0473,  0.1077,  0.0373,  0.0022,\n",
      "         -0.1044,  0.0666, -0.0632, -0.0680, -0.0512, -0.1022, -0.0914,  0.0036,\n",
      "          0.0581,  0.0303,  0.0638, -0.1002, -0.0158, -0.0355,  0.0392, -0.0814,\n",
      "          0.0227, -0.0848,  0.0220,  0.0621],\n",
      "        [ 0.0504, -0.0167,  0.0550,  0.0694,  0.0604,  0.1086, -0.0373,  0.0510,\n",
      "          0.0255, -0.0569,  0.0073, -0.0086, -0.0288, -0.0365,  0.1030,  0.1021,\n",
      "          0.0911,  0.0424, -0.0546,  0.0811,  0.1016, -0.0092, -0.0328, -0.0429,\n",
      "         -0.0233,  0.0038,  0.0781,  0.0060, -0.0631,  0.0584, -0.0206,  0.0082,\n",
      "         -0.0433, -0.0116, -0.0190,  0.1068, -0.0933,  0.0718,  0.0635, -0.0532,\n",
      "          0.0971,  0.0558, -0.0577, -0.0048, -0.0950, -0.0495,  0.0949,  0.0083,\n",
      "         -0.0908,  0.0523, -0.0396, -0.0327, -0.0765,  0.0508,  0.0038, -0.0023,\n",
      "          0.0509, -0.0070,  0.0049,  0.0453,  0.0670,  0.0930,  0.0855, -0.0008,\n",
      "          0.0784,  0.0861, -0.1067, -0.0207,  0.0999, -0.1065,  0.0660,  0.0733,\n",
      "          0.1075,  0.0843,  0.0236, -0.0080,  0.0726, -0.0825,  0.0107,  0.0379,\n",
      "         -0.0130,  0.0141,  0.0171,  0.0754],\n",
      "        [ 0.0155, -0.1026, -0.0044, -0.0784, -0.0598,  0.1075,  0.0214,  0.0315,\n",
      "          0.0608, -0.0707, -0.0020, -0.0387, -0.0540, -0.0867, -0.0422,  0.0238,\n",
      "          0.0780,  0.1004, -0.0689,  0.0971,  0.0559,  0.0258,  0.0689, -0.0597,\n",
      "          0.0838,  0.0402, -0.0669, -0.0866, -0.0502,  0.0690, -0.0811,  0.0277,\n",
      "         -0.0364,  0.0470,  0.0718, -0.0233, -0.0725,  0.0523, -0.0147,  0.0315,\n",
      "         -0.0636,  0.0588, -0.0080, -0.0221, -0.0993,  0.0829,  0.0118,  0.0318,\n",
      "          0.1002,  0.0325, -0.0252, -0.0528, -0.0391, -0.0520,  0.0856, -0.0616,\n",
      "          0.0221,  0.0902,  0.0400, -0.0268, -0.0600,  0.0959, -0.0305,  0.0800,\n",
      "          0.0260,  0.0837, -0.0808,  0.0804, -0.0924,  0.0300, -0.0962,  0.0862,\n",
      "         -0.0904, -0.0396, -0.0305, -0.0737, -0.0801, -0.0016, -0.0760,  0.0050,\n",
      "         -0.0885, -0.0469, -0.0769, -0.0176],\n",
      "        [ 0.0434,  0.0436, -0.0447, -0.0586, -0.1034,  0.0773,  0.0739, -0.0903,\n",
      "          0.0425, -0.0605,  0.0430, -0.1073, -0.0942,  0.0237, -0.0795, -0.0913,\n",
      "          0.0628,  0.0512,  0.0167, -0.0746, -0.0368, -0.0729,  0.0923,  0.0560,\n",
      "          0.0567,  0.0682,  0.0995, -0.0764,  0.0135,  0.0262, -0.0302, -0.0773,\n",
      "          0.0909, -0.0248, -0.0083, -0.0090,  0.0415, -0.0152,  0.1022,  0.0101,\n",
      "          0.0334, -0.0936, -0.0924, -0.0618,  0.1034,  0.0385, -0.0748, -0.0559,\n",
      "          0.0869,  0.0635, -0.0365, -0.0720, -0.0211, -0.0734, -0.0431,  0.0231,\n",
      "         -0.0312, -0.0042, -0.0859,  0.1032,  0.1018,  0.0529,  0.0776, -0.0866,\n",
      "         -0.0111,  0.0716,  0.0506,  0.0281, -0.0546, -0.0021,  0.0091, -0.1044,\n",
      "         -0.0868,  0.0577,  0.1031,  0.1010,  0.0963, -0.0500,  0.1052, -0.0782,\n",
      "         -0.0890,  0.0247, -0.0972, -0.0884],\n",
      "        [ 0.0196, -0.0217, -0.0438,  0.0816, -0.0273,  0.0758,  0.0531,  0.0885,\n",
      "         -0.0671, -0.0708, -0.0626, -0.0190,  0.0980,  0.0774,  0.0557, -0.0688,\n",
      "          0.0758, -0.0351, -0.0392,  0.0100, -0.0700, -0.0957, -0.0981,  0.0386,\n",
      "          0.0209, -0.0231, -0.0151,  0.0196,  0.0294,  0.0546, -0.0815, -0.0985,\n",
      "         -0.0744, -0.0695, -0.0012, -0.0521,  0.0549, -0.0362, -0.0772, -0.0520,\n",
      "         -0.0241,  0.0206,  0.0919,  0.1033, -0.1025, -0.1014,  0.0278, -0.0314,\n",
      "         -0.1059, -0.0602, -0.0893,  0.0990,  0.0959, -0.0915, -0.0261, -0.0350,\n",
      "          0.0071,  0.0431,  0.0572,  0.0877, -0.0482,  0.0029, -0.0386,  0.0258,\n",
      "         -0.0854, -0.0300,  0.1066, -0.0072,  0.0125,  0.0222, -0.0763, -0.1058,\n",
      "          0.0857, -0.0173,  0.0889, -0.0733,  0.0229, -0.0168,  0.0258, -0.0682,\n",
      "         -0.1055, -0.0623, -0.0266,  0.0754],\n",
      "        [-0.0851,  0.0263,  0.0138,  0.0552, -0.0627, -0.0701,  0.0962, -0.0643,\n",
      "         -0.0996,  0.0646, -0.0185,  0.0847,  0.0250,  0.0988,  0.0478, -0.0685,\n",
      "          0.0230, -0.1004,  0.0077,  0.0219, -0.0992,  0.1037, -0.0618,  0.0923,\n",
      "         -0.0389,  0.0358, -0.0174, -0.0480, -0.0604,  0.0088,  0.1022,  0.0838,\n",
      "          0.0649,  0.0816,  0.1087,  0.0219, -0.0222, -0.0831, -0.0669,  0.0135,\n",
      "          0.0719, -0.0041, -0.0204, -0.0999, -0.0844, -0.0939,  0.0954, -0.0852,\n",
      "          0.1037, -0.0013,  0.0425, -0.0242, -0.0682, -0.0842,  0.0090,  0.0498,\n",
      "         -0.0137, -0.0834,  0.0354,  0.0758, -0.0596,  0.0273, -0.0016,  0.0948,\n",
      "          0.0079, -0.0318,  0.0781,  0.0375,  0.0428,  0.0453,  0.0645, -0.0389,\n",
      "         -0.0176,  0.0262, -0.0713, -0.0043, -0.0718, -0.0474, -0.0415, -0.0015,\n",
      "          0.0691, -0.0452,  0.0185,  0.0148],\n",
      "        [-0.0656, -0.0658, -0.0861,  0.1025,  0.0794, -0.0145, -0.0551,  0.0592,\n",
      "          0.0152, -0.0393, -0.0351, -0.0628,  0.1048,  0.0412,  0.0737, -0.0666,\n",
      "          0.0750,  0.0493,  0.0341,  0.0625,  0.0777,  0.0978, -0.0644,  0.0208,\n",
      "         -0.0906,  0.1075, -0.1043,  0.0884, -0.0930, -0.0815, -0.0601, -0.0313,\n",
      "         -0.0413, -0.0523, -0.0690,  0.0092, -0.0991, -0.1056, -0.0306, -0.0009,\n",
      "         -0.1004, -0.0063, -0.0783, -0.0479, -0.0767,  0.0914,  0.0676, -0.1072,\n",
      "         -0.0991,  0.0730, -0.0038, -0.0249,  0.0720, -0.0537, -0.0341,  0.0100,\n",
      "          0.0842,  0.0201,  0.0699, -0.0664, -0.0348,  0.0896,  0.0357, -0.0886,\n",
      "          0.0312,  0.0489,  0.0160,  0.0275,  0.0861,  0.1090,  0.0036, -0.0369,\n",
      "          0.0488, -0.0391, -0.0117, -0.0508, -0.1094,  0.0305, -0.0954, -0.0585,\n",
      "          0.1062, -0.1037,  0.0303,  0.0685],\n",
      "        [-0.0284, -0.0145, -0.0616,  0.0195,  0.0237,  0.1001,  0.0755,  0.0044,\n",
      "          0.0947, -0.0592,  0.0701, -0.0760, -0.0019,  0.0996, -0.0285, -0.1021,\n",
      "          0.0455, -0.0686, -0.0796, -0.1034,  0.0955,  0.0859, -0.0048,  0.0749,\n",
      "          0.0702,  0.0809,  0.0443, -0.0555, -0.1095,  0.0930, -0.0344, -0.0469,\n",
      "         -0.0006,  0.0847,  0.0746,  0.0405,  0.0570, -0.0612,  0.0434,  0.0149,\n",
      "         -0.0422, -0.0451, -0.0612, -0.1040,  0.0454,  0.0386, -0.0302,  0.0428,\n",
      "          0.0980, -0.0501, -0.0690, -0.0212, -0.0612, -0.0144,  0.0824,  0.1034,\n",
      "         -0.0129,  0.0723, -0.0762, -0.0521,  0.0834, -0.0736,  0.0713, -0.0699,\n",
      "         -0.0217,  0.0741, -0.0241,  0.0205, -0.0415,  0.0506,  0.0707,  0.0888,\n",
      "          0.0657,  0.0384, -0.1054,  0.0890,  0.0374,  0.0956,  0.0568, -0.0557,\n",
      "         -0.0273, -0.0004,  0.0248, -0.0471]], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0586, -0.0489, -0.0077,  0.1096,  0.0111, -0.0139, -0.0592,  0.0979,\n",
      "         0.0710,  0.0789], device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "trainer.InitParameters(from_pretrained=None, use_gpu=1, a=1)\n",
    "for param in trainer.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1310, -0.1424, -0.1720],\n",
      "          [ 0.1197,  0.1184, -0.2506],\n",
      "          [ 0.2374, -0.0751, -0.0696]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0798,  0.0304,  0.1062],\n",
      "          [-0.1129, -0.2112,  0.0842],\n",
      "          [-0.1649,  0.0471,  0.2805]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1573, -0.2870,  0.0680],\n",
      "          [ 0.0907, -0.1963,  0.3085],\n",
      "          [-0.0539, -0.1999, -0.0290]]],\n",
      "\n",
      "\n",
      "        [[[-0.1169, -0.2866, -0.1622],\n",
      "          [ 0.0678, -0.1545, -0.2050],\n",
      "          [ 0.2296,  0.0229,  0.3149]]],\n",
      "\n",
      "\n",
      "        [[[-0.1902, -0.2176, -0.2299],\n",
      "          [ 0.3053,  0.1966, -0.1620],\n",
      "          [ 0.0115,  0.1770, -0.1851]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2123,  0.1677, -0.0322],\n",
      "          [ 0.0689,  0.0589, -0.1155],\n",
      "          [ 0.1154,  0.1046, -0.2012]]]], device='cuda:1', requires_grad=True)\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]], device='cuda:1')\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1309, -0.1429, -0.1732],\n",
      "          [ 0.1180,  0.1204, -0.2538],\n",
      "          [ 0.2382, -0.0738, -0.0703]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0803,  0.0302,  0.1072],\n",
      "          [-0.1124, -0.2124,  0.0861],\n",
      "          [-0.1662,  0.0449,  0.2790]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1588, -0.2851,  0.0694],\n",
      "          [ 0.0897, -0.1964,  0.3074],\n",
      "          [-0.0547, -0.1994, -0.0298]]],\n",
      "\n",
      "\n",
      "        [[[-0.1156, -0.2869, -0.1608],\n",
      "          [ 0.0664, -0.1562, -0.2032],\n",
      "          [ 0.2311,  0.0254,  0.3146]]],\n",
      "\n",
      "\n",
      "        [[[-0.1921, -0.2156, -0.2292],\n",
      "          [ 0.3052,  0.1951, -0.1605],\n",
      "          [ 0.0102,  0.1760, -0.1856]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2084,  0.1653, -0.0320],\n",
      "          [ 0.0710,  0.0572, -0.1149],\n",
      "          [ 0.1133,  0.1059, -0.1999]]]], device='cuda:1', requires_grad=True)\n",
      "tensor([[[[-0.0013,  0.0050,  0.0120],\n",
      "          [ 0.0176, -0.0198,  0.0314],\n",
      "          [-0.0082, -0.0124,  0.0071]]],\n",
      "\n",
      "\n",
      "        [[[-0.0051,  0.0020, -0.0099],\n",
      "          [-0.0053,  0.0113, -0.0184],\n",
      "          [ 0.0134,  0.0219,  0.0152]]],\n",
      "\n",
      "\n",
      "        [[[-0.0153, -0.0199, -0.0145],\n",
      "          [ 0.0095,  0.0008,  0.0112],\n",
      "          [ 0.0087, -0.0049,  0.0078]]],\n",
      "\n",
      "\n",
      "        [[[-0.0136,  0.0028, -0.0136],\n",
      "          [ 0.0136,  0.0164, -0.0178],\n",
      "          [-0.0156, -0.0246,  0.0026]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0196, -0.0196, -0.0073],\n",
      "          [ 0.0011,  0.0158, -0.0159],\n",
      "          [ 0.0128,  0.0096,  0.0044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0390,  0.0242, -0.0017],\n",
      "          [-0.0209,  0.0177, -0.0064],\n",
      "          [ 0.0214, -0.0127, -0.0131]]]], device='cuda:1')\n",
      "tensor(1.3067, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "<MseLossBackward object at 0x7f4c14174748>\n",
      "<AddmmBackward object at 0x7f4c14174da0>\n",
      "<AccumulateGrad object at 0x7f4c14174da0>\n"
     ]
    }
   ],
   "source": [
    "trainer.SGDTrain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
