{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NLPUtils import *\n",
    "import re\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento:\n",
    "\n",
    "with open('../14-Latino-40/promptsl40.train','rb') as file:\n",
    "    train_lines = file.readlines()\n",
    "    corpus = list(itertools.chain([' '.join(re.findall(r'\\w+',line.decode('iso-8859-1'))[1:]) for line in train_lines]))\n",
    "\n",
    "window_size = 2           # Tamaño de la ventana del contexto.\n",
    "cutoff_freq = 0           # Palabras con una frecuencia menor o igual a cutoff_freq son excluídas del vocabulario.\n",
    "batch_size = 512          # Tamaño del batch.\n",
    "\n",
    "embedding_dim = 200       # Dimensión del espacio de los word vectors.\n",
    "device = 'cuda:1'         # Dispositivo sobre el cual se entrena. \n",
    "state_dict = None         # Parámetros pre-entrenados.\n",
    "paralelize = False        # Flag para decirle al programa que use las 2 gpus\n",
    "\n",
    "trainer = SkipGramTrainer(corpus, cutoff_freq=cutoff_freq,window_size=window_size,batch_size=batch_size)\n",
    "trainer.InitModel(state_dict=state_dict, device=device, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.005\n",
      "Number of epochs: 150\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 5479.048828125\n",
      "Epoch: 1, Batch number: 10, Loss: 5453.08935546875\n",
      "Epoch: 1, Batch number: 20, Loss: 5486.080078125\n",
      "Epoch: 1, Batch number: 30, Loss: 5467.57958984375\n",
      "Epoch: 1, Batch number: 40, Loss: 5562.5\n",
      "Epoch: 1, Batch number: 50, Loss: 5492.37353515625\n",
      "Epoch: 1, Batch number: 60, Loss: 5400.1083984375\n",
      "Epoch: 1, Batch number: 70, Loss: 5489.0927734375\n",
      "Epoch: 1, Batch number: 80, Loss: 5470.99169921875\n",
      "Epoch: 1, Batch number: 90, Loss: 5555.48974609375\n",
      "Epoch: 1, Batch number: 100, Loss: 5477.42724609375\n",
      "Epoch: 1, Batch number: 110, Loss: 5474.50537109375\n",
      "Epoch: 1, Batch number: 120, Loss: 5436.896484375\n",
      "Epoch: 1, Batch number: 130, Loss: 5451.74853515625\n",
      "Epoch: 1, Batch number: 140, Loss: 5462.14501953125\n",
      "Epoch: 1, Batch number: 150, Loss: 5503.76416015625\n",
      "Epoch: 1, Batch number: 160, Loss: 5537.57470703125\n",
      "Epoch: 1, Batch number: 170, Loss: 5514.62939453125\n",
      "Epoch: 1, Batch number: 180, Loss: 5525.5224609375\n",
      "Epoch: 1, Batch number: 190, Loss: 5447.921875\n",
      "Epoch: 1, Batch number: 200, Loss: 5397.373046875\n",
      "Epoch: 1, Batch number: 210, Loss: 5428.43505859375\n",
      "Epoch: 1, Batch number: 220, Loss: 5568.455078125\n",
      "Epoch: 1, Batch number: 230, Loss: 5516.34228515625\n",
      "Epoch: 1, Batch number: 240, Loss: 5512.55859375\n",
      "Epoch: 1, Batch number: 250, Loss: 5487.0341796875\n",
      "Epoch: 1, Batch number: 260, Loss: 5475.92431640625\n",
      "Epoch: 1, Batch number: 270, Loss: 5507.595703125\n",
      "Epoch: 1, Batch number: 280, Loss: 5461.28466796875\n",
      "Epoch: 1, Batch number: 290, Loss: 5517.6015625\n",
      "Epoch: 1, Batch number: 300, Loss: 5445.93017578125\n",
      "Epoch: 1, Batch number: 310, Loss: 5505.48388671875\n",
      "Epoch: 1, Batch number: 320, Loss: 5510.14990234375\n",
      "Epoch: 1, Batch number: 330, Loss: 5533.49755859375\n",
      "Epoch: 1, Batch number: 340, Loss: 5429.37109375\n",
      "Epoch: 1, Batch number: 350, Loss: 5448.3857421875\n",
      "Epoch: 2, Batch number: 2, Loss: 5487.109375\n",
      "Epoch: 2, Batch number: 12, Loss: 5510.8056640625\n",
      "Epoch: 2, Batch number: 22, Loss: 5439.04296875\n",
      "Epoch: 2, Batch number: 32, Loss: 5357.25390625\n",
      "Epoch: 2, Batch number: 42, Loss: 5449.8134765625\n",
      "Epoch: 2, Batch number: 52, Loss: 5441.212890625\n",
      "Epoch: 2, Batch number: 62, Loss: 5495.74951171875\n",
      "Epoch: 2, Batch number: 72, Loss: 5516.8828125\n",
      "Epoch: 2, Batch number: 82, Loss: 5489.12744140625\n",
      "Epoch: 2, Batch number: 92, Loss: 5539.89453125\n",
      "Epoch: 2, Batch number: 102, Loss: 5503.365234375\n",
      "Epoch: 2, Batch number: 112, Loss: 5507.41064453125\n",
      "Epoch: 2, Batch number: 122, Loss: 5476.79052734375\n",
      "Epoch: 2, Batch number: 132, Loss: 5474.5166015625\n",
      "Epoch: 2, Batch number: 142, Loss: 5505.88623046875\n",
      "Epoch: 2, Batch number: 152, Loss: 5436.404296875\n",
      "Epoch: 2, Batch number: 162, Loss: 5501.84130859375\n",
      "Epoch: 2, Batch number: 172, Loss: 5463.927734375\n",
      "Epoch: 2, Batch number: 182, Loss: 5475.70654296875\n",
      "Epoch: 2, Batch number: 192, Loss: 5410.23095703125\n",
      "Epoch: 2, Batch number: 202, Loss: 5515.86669921875\n",
      "Epoch: 2, Batch number: 212, Loss: 5532.1201171875\n",
      "Epoch: 2, Batch number: 222, Loss: 5426.33837890625\n",
      "Epoch: 2, Batch number: 232, Loss: 5386.2529296875\n",
      "Epoch: 2, Batch number: 242, Loss: 5469.03662109375\n",
      "Epoch: 2, Batch number: 252, Loss: 5516.453125\n",
      "Epoch: 2, Batch number: 262, Loss: 5515.9501953125\n",
      "Epoch: 2, Batch number: 272, Loss: 5480.56103515625\n",
      "Epoch: 2, Batch number: 282, Loss: 5522.6103515625\n",
      "Epoch: 2, Batch number: 292, Loss: 5520.26220703125\n",
      "Epoch: 2, Batch number: 302, Loss: 5499.63525390625\n",
      "Epoch: 2, Batch number: 312, Loss: 5432.4189453125\n",
      "Epoch: 2, Batch number: 322, Loss: 5471.3994140625\n",
      "Epoch: 2, Batch number: 332, Loss: 5469.53515625\n",
      "Epoch: 2, Batch number: 342, Loss: 5405.10595703125\n",
      "Epoch: 2, Batch number: 352, Loss: 5488.74169921875\n",
      "Epoch: 3, Batch number: 4, Loss: 5486.37451171875\n",
      "Epoch: 3, Batch number: 14, Loss: 5457.607421875\n",
      "Epoch: 3, Batch number: 24, Loss: 5484.837890625\n",
      "Epoch: 3, Batch number: 34, Loss: 5567.1259765625\n",
      "Epoch: 3, Batch number: 44, Loss: 5590.23876953125\n",
      "Epoch: 3, Batch number: 54, Loss: 5501.43994140625\n",
      "Epoch: 3, Batch number: 64, Loss: 5472.31298828125\n",
      "Epoch: 3, Batch number: 74, Loss: 5478.93896484375\n",
      "Epoch: 3, Batch number: 84, Loss: 5554.8994140625\n",
      "Epoch: 3, Batch number: 94, Loss: 5484.8876953125\n",
      "Epoch: 3, Batch number: 104, Loss: 5582.86376953125\n",
      "Epoch: 3, Batch number: 114, Loss: 5315.81689453125\n",
      "Epoch: 3, Batch number: 124, Loss: 5566.8330078125\n",
      "Epoch: 3, Batch number: 134, Loss: 5486.93603515625\n",
      "Epoch: 3, Batch number: 144, Loss: 5450.7236328125\n",
      "Epoch: 3, Batch number: 154, Loss: 5437.5419921875\n",
      "Epoch: 3, Batch number: 164, Loss: 5462.50927734375\n",
      "Epoch: 3, Batch number: 174, Loss: 5378.90087890625\n",
      "Epoch: 3, Batch number: 184, Loss: 5566.20947265625\n",
      "Epoch: 3, Batch number: 194, Loss: 5546.97021484375\n",
      "Epoch: 3, Batch number: 204, Loss: 5406.10693359375\n",
      "Epoch: 3, Batch number: 214, Loss: 5524.18017578125\n",
      "Epoch: 3, Batch number: 224, Loss: 5486.3125\n",
      "Epoch: 3, Batch number: 234, Loss: 5540.626953125\n",
      "Epoch: 3, Batch number: 244, Loss: 5536.04541015625\n",
      "Epoch: 3, Batch number: 254, Loss: 5468.51953125\n",
      "Epoch: 3, Batch number: 264, Loss: 5514.8896484375\n",
      "Epoch: 3, Batch number: 274, Loss: 5398.55810546875\n",
      "Epoch: 3, Batch number: 284, Loss: 5446.79736328125\n",
      "Epoch: 3, Batch number: 294, Loss: 5502.2939453125\n",
      "Epoch: 3, Batch number: 304, Loss: 5515.13818359375\n",
      "Epoch: 3, Batch number: 314, Loss: 5513.40234375\n",
      "Epoch: 3, Batch number: 324, Loss: 5473.40283203125\n",
      "Epoch: 3, Batch number: 334, Loss: 5558.05615234375\n",
      "Epoch: 3, Batch number: 344, Loss: 5498.1044921875\n",
      "Epoch: 3, Batch number: 354, Loss: 5400.51220703125\n",
      "Epoch: 4, Batch number: 6, Loss: 5535.87353515625\n",
      "Epoch: 4, Batch number: 16, Loss: 5567.3818359375\n",
      "Epoch: 4, Batch number: 26, Loss: 5528.4501953125\n",
      "Epoch: 4, Batch number: 36, Loss: 5467.3330078125\n",
      "Epoch: 4, Batch number: 46, Loss: 5441.5634765625\n",
      "Epoch: 4, Batch number: 56, Loss: 5431.24951171875\n",
      "Epoch: 4, Batch number: 66, Loss: 5485.99169921875\n",
      "Epoch: 4, Batch number: 76, Loss: 5452.43896484375\n",
      "Epoch: 4, Batch number: 86, Loss: 5573.783203125\n",
      "Epoch: 4, Batch number: 96, Loss: 5500.71240234375\n",
      "Epoch: 4, Batch number: 106, Loss: 5430.07861328125\n",
      "Epoch: 4, Batch number: 116, Loss: 5452.3818359375\n",
      "Epoch: 4, Batch number: 126, Loss: 5425.921875\n",
      "Epoch: 4, Batch number: 136, Loss: 5518.5791015625\n",
      "Epoch: 4, Batch number: 146, Loss: 5485.7978515625\n",
      "Epoch: 4, Batch number: 156, Loss: 5526.02587890625\n",
      "Epoch: 4, Batch number: 166, Loss: 5475.87109375\n",
      "Epoch: 4, Batch number: 176, Loss: 5503.15380859375\n",
      "Epoch: 4, Batch number: 186, Loss: 5450.6005859375\n",
      "Epoch: 4, Batch number: 196, Loss: 5500.55810546875\n",
      "Epoch: 4, Batch number: 206, Loss: 5413.21484375\n",
      "Epoch: 4, Batch number: 216, Loss: 5494.6494140625\n",
      "Epoch: 4, Batch number: 226, Loss: 5479.79150390625\n",
      "Epoch: 4, Batch number: 236, Loss: 5513.30517578125\n",
      "Epoch: 4, Batch number: 246, Loss: 5473.73681640625\n",
      "Epoch: 4, Batch number: 256, Loss: 5482.79150390625\n",
      "Epoch: 4, Batch number: 266, Loss: 5457.82958984375\n",
      "Epoch: 4, Batch number: 276, Loss: 5432.19921875\n",
      "Epoch: 4, Batch number: 286, Loss: 5506.470703125\n",
      "Epoch: 4, Batch number: 296, Loss: 5511.66748046875\n",
      "Epoch: 4, Batch number: 306, Loss: 5451.68212890625\n",
      "Epoch: 4, Batch number: 316, Loss: 5442.80859375\n",
      "Epoch: 4, Batch number: 326, Loss: 5456.56640625\n",
      "Epoch: 4, Batch number: 336, Loss: 5524.03076171875\n",
      "Epoch: 4, Batch number: 346, Loss: 5485.3037109375\n",
      "Epoch: 4, Batch number: 356, Loss: 5490.14990234375\n",
      "Epoch: 5, Batch number: 8, Loss: 5364.888671875\n",
      "Epoch: 5, Batch number: 18, Loss: 5522.7861328125\n",
      "Epoch: 5, Batch number: 28, Loss: 5452.12841796875\n",
      "Epoch: 5, Batch number: 38, Loss: 5472.6474609375\n",
      "Epoch: 5, Batch number: 48, Loss: 5446.2861328125\n",
      "Epoch: 5, Batch number: 58, Loss: 5500.86181640625\n",
      "Epoch: 5, Batch number: 68, Loss: 5419.6103515625\n",
      "Epoch: 5, Batch number: 78, Loss: 5386.39404296875\n",
      "Epoch: 5, Batch number: 88, Loss: 5434.26904296875\n",
      "Epoch: 5, Batch number: 98, Loss: 5458.0380859375\n",
      "Epoch: 5, Batch number: 108, Loss: 5511.42822265625\n",
      "Epoch: 5, Batch number: 118, Loss: 5578.68310546875\n",
      "Epoch: 5, Batch number: 128, Loss: 5534.283203125\n",
      "Epoch: 5, Batch number: 138, Loss: 5534.2529296875\n",
      "Epoch: 5, Batch number: 148, Loss: 5494.81494140625\n",
      "Epoch: 5, Batch number: 158, Loss: 5502.65966796875\n",
      "Epoch: 5, Batch number: 168, Loss: 5535.6123046875\n",
      "Epoch: 5, Batch number: 178, Loss: 5457.54736328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Batch number: 188, Loss: 5474.38134765625\n",
      "Epoch: 5, Batch number: 198, Loss: 5517.1025390625\n",
      "Epoch: 5, Batch number: 208, Loss: 5478.375\n",
      "Epoch: 5, Batch number: 218, Loss: 5528.892578125\n",
      "Epoch: 5, Batch number: 228, Loss: 5481.8017578125\n",
      "Epoch: 5, Batch number: 238, Loss: 5426.97802734375\n",
      "Epoch: 5, Batch number: 248, Loss: 5506.64697265625\n",
      "Epoch: 5, Batch number: 258, Loss: 5453.58837890625\n",
      "Epoch: 5, Batch number: 268, Loss: 5487.0908203125\n",
      "Epoch: 5, Batch number: 278, Loss: 5462.5283203125\n",
      "Epoch: 5, Batch number: 288, Loss: 5478.11865234375\n",
      "Epoch: 5, Batch number: 298, Loss: 5498.7021484375\n",
      "Epoch: 5, Batch number: 308, Loss: 5472.541015625\n",
      "Epoch: 5, Batch number: 318, Loss: 5448.64990234375\n",
      "Epoch: 5, Batch number: 328, Loss: 5400.56787109375\n",
      "Epoch: 5, Batch number: 338, Loss: 5470.7958984375\n",
      "Epoch: 5, Batch number: 348, Loss: 5498.5517578125\n",
      "Epoch: 6, Batch number: 0, Loss: 5406.748046875\n",
      "Epoch: 6, Batch number: 10, Loss: 5474.18896484375\n",
      "Epoch: 6, Batch number: 20, Loss: 5440.69921875\n",
      "Epoch: 6, Batch number: 30, Loss: 5443.1259765625\n",
      "Epoch: 6, Batch number: 40, Loss: 5490.955078125\n",
      "Epoch: 6, Batch number: 50, Loss: 5474.3525390625\n",
      "Epoch: 6, Batch number: 60, Loss: 5438.20263671875\n",
      "Epoch: 6, Batch number: 70, Loss: 5425.55615234375\n",
      "Epoch: 6, Batch number: 80, Loss: 5447.06689453125\n",
      "Epoch: 6, Batch number: 90, Loss: 5559.916015625\n",
      "Epoch: 6, Batch number: 100, Loss: 5415.36474609375\n",
      "Epoch: 6, Batch number: 110, Loss: 5542.810546875\n",
      "Epoch: 6, Batch number: 120, Loss: 5490.85107421875\n",
      "Epoch: 6, Batch number: 130, Loss: 5505.45361328125\n",
      "Epoch: 6, Batch number: 140, Loss: 5477.88916015625\n",
      "Epoch: 6, Batch number: 150, Loss: 5456.91162109375\n",
      "Epoch: 6, Batch number: 160, Loss: 5515.6865234375\n",
      "Epoch: 6, Batch number: 170, Loss: 5436.18994140625\n",
      "Epoch: 6, Batch number: 180, Loss: 5415.26171875\n",
      "Epoch: 6, Batch number: 190, Loss: 5434.44140625\n",
      "Epoch: 6, Batch number: 200, Loss: 5478.4453125\n",
      "Epoch: 6, Batch number: 210, Loss: 5483.8544921875\n",
      "Epoch: 6, Batch number: 220, Loss: 5461.994140625\n",
      "Epoch: 6, Batch number: 230, Loss: 5419.916015625\n",
      "Epoch: 6, Batch number: 240, Loss: 5558.1728515625\n",
      "Epoch: 6, Batch number: 250, Loss: 5391.74658203125\n",
      "Epoch: 6, Batch number: 260, Loss: 5477.0634765625\n",
      "Epoch: 6, Batch number: 270, Loss: 5515.482421875\n",
      "Epoch: 6, Batch number: 280, Loss: 5457.68994140625\n",
      "Epoch: 6, Batch number: 290, Loss: 5638.11962890625\n",
      "Epoch: 6, Batch number: 300, Loss: 5432.69580078125\n",
      "Epoch: 6, Batch number: 310, Loss: 5397.4638671875\n",
      "Epoch: 6, Batch number: 320, Loss: 5504.025390625\n",
      "Epoch: 6, Batch number: 330, Loss: 5513.84228515625\n",
      "Epoch: 6, Batch number: 340, Loss: 5494.1220703125\n",
      "Epoch: 6, Batch number: 350, Loss: 5430.03759765625\n",
      "Epoch: 7, Batch number: 2, Loss: 5522.8623046875\n",
      "Epoch: 7, Batch number: 12, Loss: 5499.1728515625\n",
      "Epoch: 7, Batch number: 22, Loss: 5453.2998046875\n",
      "Epoch: 7, Batch number: 32, Loss: 5511.5068359375\n",
      "Epoch: 7, Batch number: 42, Loss: 5491.9306640625\n",
      "Epoch: 7, Batch number: 52, Loss: 5547.41455078125\n",
      "Epoch: 7, Batch number: 62, Loss: 5494.80224609375\n",
      "Epoch: 7, Batch number: 72, Loss: 5442.43408203125\n",
      "Epoch: 7, Batch number: 82, Loss: 5421.056640625\n",
      "Epoch: 7, Batch number: 92, Loss: 5487.4296875\n",
      "Epoch: 7, Batch number: 102, Loss: 5515.43603515625\n",
      "Epoch: 7, Batch number: 112, Loss: 5473.361328125\n",
      "Epoch: 7, Batch number: 122, Loss: 5514.44921875\n",
      "Epoch: 7, Batch number: 132, Loss: 5457.86572265625\n",
      "Epoch: 7, Batch number: 142, Loss: 5548.48046875\n",
      "Epoch: 7, Batch number: 152, Loss: 5420.45751953125\n",
      "Epoch: 7, Batch number: 162, Loss: 5424.81494140625\n",
      "Epoch: 7, Batch number: 172, Loss: 5444.650390625\n",
      "Epoch: 7, Batch number: 182, Loss: 5515.5517578125\n",
      "Epoch: 7, Batch number: 192, Loss: 5499.86181640625\n",
      "Epoch: 7, Batch number: 202, Loss: 5425.3388671875\n",
      "Epoch: 7, Batch number: 212, Loss: 5447.4443359375\n",
      "Epoch: 7, Batch number: 222, Loss: 5376.765625\n",
      "Epoch: 7, Batch number: 232, Loss: 5513.021484375\n",
      "Epoch: 7, Batch number: 242, Loss: 5545.09228515625\n",
      "Epoch: 7, Batch number: 252, Loss: 5565.28955078125\n",
      "Epoch: 7, Batch number: 262, Loss: 5388.61083984375\n",
      "Epoch: 7, Batch number: 272, Loss: 5545.4609375\n",
      "Epoch: 7, Batch number: 282, Loss: 5502.72802734375\n",
      "Epoch: 7, Batch number: 292, Loss: 5372.66796875\n",
      "Epoch: 7, Batch number: 302, Loss: 5440.39404296875\n",
      "Epoch: 7, Batch number: 312, Loss: 5550.22021484375\n",
      "Epoch: 7, Batch number: 322, Loss: 5482.5830078125\n",
      "Epoch: 7, Batch number: 332, Loss: 5452.7998046875\n",
      "Epoch: 7, Batch number: 342, Loss: 5427.9765625\n",
      "Epoch: 7, Batch number: 352, Loss: 5596.41796875\n",
      "Epoch: 8, Batch number: 4, Loss: 5444.48486328125\n",
      "Epoch: 8, Batch number: 14, Loss: 5600.98388671875\n",
      "Epoch: 8, Batch number: 24, Loss: 5489.05419921875\n",
      "Epoch: 8, Batch number: 34, Loss: 5525.6064453125\n",
      "Epoch: 8, Batch number: 44, Loss: 5535.8203125\n",
      "Epoch: 8, Batch number: 54, Loss: 5480.3076171875\n",
      "Epoch: 8, Batch number: 64, Loss: 5494.9638671875\n",
      "Epoch: 8, Batch number: 74, Loss: 5543.9658203125\n",
      "Epoch: 8, Batch number: 84, Loss: 5469.95751953125\n",
      "Epoch: 8, Batch number: 94, Loss: 5454.13427734375\n",
      "Epoch: 8, Batch number: 104, Loss: 5396.78369140625\n",
      "Epoch: 8, Batch number: 114, Loss: 5345.81640625\n",
      "Epoch: 8, Batch number: 124, Loss: 5504.2216796875\n",
      "Epoch: 8, Batch number: 134, Loss: 5444.84521484375\n",
      "Epoch: 8, Batch number: 144, Loss: 5516.302734375\n",
      "Epoch: 8, Batch number: 154, Loss: 5465.52392578125\n",
      "Epoch: 8, Batch number: 164, Loss: 5423.5078125\n",
      "Epoch: 8, Batch number: 174, Loss: 5489.7626953125\n",
      "Epoch: 8, Batch number: 184, Loss: 5563.7060546875\n",
      "Epoch: 8, Batch number: 194, Loss: 5480.59130859375\n",
      "Epoch: 8, Batch number: 204, Loss: 5439.20068359375\n",
      "Epoch: 8, Batch number: 214, Loss: 5455.556640625\n",
      "Epoch: 8, Batch number: 224, Loss: 5412.84814453125\n",
      "Epoch: 8, Batch number: 234, Loss: 5394.291015625\n",
      "Epoch: 8, Batch number: 244, Loss: 5431.8564453125\n",
      "Epoch: 8, Batch number: 254, Loss: 5502.4736328125\n",
      "Epoch: 8, Batch number: 264, Loss: 5478.169921875\n",
      "Epoch: 8, Batch number: 274, Loss: 5523.015625\n",
      "Epoch: 8, Batch number: 284, Loss: 5495.0791015625\n",
      "Epoch: 8, Batch number: 294, Loss: 5390.9619140625\n",
      "Epoch: 8, Batch number: 304, Loss: 5448.88330078125\n",
      "Epoch: 8, Batch number: 314, Loss: 5443.79296875\n",
      "Epoch: 8, Batch number: 324, Loss: 5466.63427734375\n",
      "Epoch: 8, Batch number: 334, Loss: 5474.91650390625\n",
      "Epoch: 8, Batch number: 344, Loss: 5542.1982421875\n",
      "Epoch: 8, Batch number: 354, Loss: 5489.791015625\n",
      "Epoch: 9, Batch number: 6, Loss: 5387.93896484375\n",
      "Epoch: 9, Batch number: 16, Loss: 5550.65966796875\n",
      "Epoch: 9, Batch number: 26, Loss: 5403.87255859375\n",
      "Epoch: 9, Batch number: 36, Loss: 5515.63037109375\n",
      "Epoch: 9, Batch number: 46, Loss: 5493.50390625\n",
      "Epoch: 9, Batch number: 56, Loss: 5537.73779296875\n",
      "Epoch: 9, Batch number: 66, Loss: 5527.36083984375\n",
      "Epoch: 9, Batch number: 76, Loss: 5499.97412109375\n",
      "Epoch: 9, Batch number: 86, Loss: 5459.05126953125\n",
      "Epoch: 9, Batch number: 96, Loss: 5441.31787109375\n",
      "Epoch: 9, Batch number: 106, Loss: 5504.58447265625\n",
      "Epoch: 9, Batch number: 116, Loss: 5499.13330078125\n",
      "Epoch: 9, Batch number: 126, Loss: 5397.35791015625\n",
      "Epoch: 9, Batch number: 136, Loss: 5485.42529296875\n",
      "Epoch: 9, Batch number: 146, Loss: 5409.7216796875\n",
      "Epoch: 9, Batch number: 156, Loss: 5504.05322265625\n",
      "Epoch: 9, Batch number: 166, Loss: 5590.6611328125\n",
      "Epoch: 9, Batch number: 176, Loss: 5475.078125\n",
      "Epoch: 9, Batch number: 186, Loss: 5435.916015625\n",
      "Epoch: 9, Batch number: 196, Loss: 5484.1005859375\n",
      "Epoch: 9, Batch number: 206, Loss: 5489.81982421875\n",
      "Epoch: 9, Batch number: 216, Loss: 5504.34912109375\n",
      "Epoch: 9, Batch number: 226, Loss: 5472.8671875\n",
      "Epoch: 9, Batch number: 236, Loss: 5497.93115234375\n",
      "Epoch: 9, Batch number: 246, Loss: 5503.580078125\n",
      "Epoch: 9, Batch number: 256, Loss: 5365.93798828125\n",
      "Epoch: 9, Batch number: 266, Loss: 5377.79736328125\n",
      "Epoch: 9, Batch number: 276, Loss: 5556.95361328125\n",
      "Epoch: 9, Batch number: 286, Loss: 5504.4560546875\n",
      "Epoch: 9, Batch number: 296, Loss: 5497.1962890625\n",
      "Epoch: 9, Batch number: 306, Loss: 5491.380859375\n",
      "Epoch: 9, Batch number: 316, Loss: 5399.037109375\n",
      "Epoch: 9, Batch number: 326, Loss: 5451.474609375\n",
      "Epoch: 9, Batch number: 336, Loss: 5569.7353515625\n",
      "Epoch: 9, Batch number: 346, Loss: 5469.50439453125\n",
      "Epoch: 9, Batch number: 356, Loss: 5507.26953125\n",
      "Epoch: 10, Batch number: 8, Loss: 5456.720703125\n",
      "Epoch: 10, Batch number: 18, Loss: 5482.65478515625\n",
      "Epoch: 10, Batch number: 28, Loss: 5506.50390625\n",
      "Epoch: 10, Batch number: 38, Loss: 5487.02392578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Batch number: 48, Loss: 5447.2470703125\n",
      "Epoch: 10, Batch number: 58, Loss: 5548.76025390625\n",
      "Epoch: 10, Batch number: 68, Loss: 5498.87158203125\n",
      "Epoch: 10, Batch number: 78, Loss: 5546.6044921875\n",
      "Epoch: 10, Batch number: 88, Loss: 5420.09814453125\n",
      "Epoch: 10, Batch number: 98, Loss: 5419.2890625\n",
      "Epoch: 10, Batch number: 108, Loss: 5508.44189453125\n",
      "Epoch: 10, Batch number: 118, Loss: 5417.25634765625\n",
      "Epoch: 10, Batch number: 128, Loss: 5453.39111328125\n",
      "Epoch: 10, Batch number: 138, Loss: 5477.7666015625\n",
      "Epoch: 10, Batch number: 148, Loss: 5474.76025390625\n",
      "Epoch: 10, Batch number: 158, Loss: 5480.20751953125\n",
      "Epoch: 10, Batch number: 168, Loss: 5420.2080078125\n",
      "Epoch: 10, Batch number: 178, Loss: 5497.556640625\n",
      "Epoch: 10, Batch number: 188, Loss: 5453.93798828125\n",
      "Epoch: 10, Batch number: 198, Loss: 5442.42724609375\n",
      "Epoch: 10, Batch number: 208, Loss: 5533.88671875\n",
      "Epoch: 10, Batch number: 218, Loss: 5523.193359375\n",
      "Epoch: 10, Batch number: 228, Loss: 5460.52734375\n",
      "Epoch: 10, Batch number: 238, Loss: 5479.45556640625\n",
      "Epoch: 10, Batch number: 248, Loss: 5461.96435546875\n",
      "Epoch: 10, Batch number: 258, Loss: 5570.31640625\n",
      "Epoch: 10, Batch number: 268, Loss: 5508.048828125\n",
      "Epoch: 10, Batch number: 278, Loss: 5503.10302734375\n",
      "Epoch: 10, Batch number: 288, Loss: 5503.51708984375\n",
      "Epoch: 10, Batch number: 298, Loss: 5416.3564453125\n",
      "Epoch: 10, Batch number: 308, Loss: 5414.39404296875\n",
      "Epoch: 10, Batch number: 318, Loss: 5442.5126953125\n",
      "Epoch: 10, Batch number: 328, Loss: 5457.14794921875\n",
      "Epoch: 10, Batch number: 338, Loss: 5411.47509765625\n",
      "Epoch: 10, Batch number: 348, Loss: 5488.15234375\n",
      "Epoch: 11, Batch number: 0, Loss: 5444.10400390625\n",
      "Epoch: 11, Batch number: 10, Loss: 5499.16357421875\n",
      "Epoch: 11, Batch number: 20, Loss: 5427.52880859375\n",
      "Epoch: 11, Batch number: 30, Loss: 5464.8994140625\n",
      "Epoch: 11, Batch number: 40, Loss: 5487.02734375\n",
      "Epoch: 11, Batch number: 50, Loss: 5426.72900390625\n",
      "Epoch: 11, Batch number: 60, Loss: 5466.2607421875\n",
      "Epoch: 11, Batch number: 70, Loss: 5459.9013671875\n",
      "Epoch: 11, Batch number: 80, Loss: 5501.49658203125\n",
      "Epoch: 11, Batch number: 90, Loss: 5359.34814453125\n",
      "Epoch: 11, Batch number: 100, Loss: 5530.12890625\n",
      "Epoch: 11, Batch number: 110, Loss: 5486.416015625\n",
      "Epoch: 11, Batch number: 120, Loss: 5464.51025390625\n",
      "Epoch: 11, Batch number: 130, Loss: 5466.92822265625\n",
      "Epoch: 11, Batch number: 140, Loss: 5424.3955078125\n",
      "Epoch: 11, Batch number: 150, Loss: 5404.8974609375\n",
      "Epoch: 11, Batch number: 160, Loss: 5447.22705078125\n",
      "Epoch: 11, Batch number: 170, Loss: 5485.48583984375\n",
      "Epoch: 11, Batch number: 180, Loss: 5354.33740234375\n",
      "Epoch: 11, Batch number: 190, Loss: 5406.95947265625\n",
      "Epoch: 11, Batch number: 200, Loss: 5427.3876953125\n",
      "Epoch: 11, Batch number: 210, Loss: 5301.734375\n",
      "Epoch: 11, Batch number: 220, Loss: 5486.97216796875\n",
      "Epoch: 11, Batch number: 230, Loss: 5458.15576171875\n",
      "Epoch: 11, Batch number: 240, Loss: 5534.99267578125\n",
      "Epoch: 11, Batch number: 250, Loss: 5439.001953125\n",
      "Epoch: 11, Batch number: 260, Loss: 5489.43701171875\n",
      "Epoch: 11, Batch number: 270, Loss: 5493.1298828125\n",
      "Epoch: 11, Batch number: 280, Loss: 5367.68603515625\n",
      "Epoch: 11, Batch number: 290, Loss: 5511.51611328125\n",
      "Epoch: 11, Batch number: 300, Loss: 5474.18603515625\n",
      "Epoch: 11, Batch number: 310, Loss: 5508.7333984375\n",
      "Epoch: 11, Batch number: 320, Loss: 5480.93212890625\n",
      "Epoch: 11, Batch number: 330, Loss: 5573.201171875\n",
      "Epoch: 11, Batch number: 340, Loss: 5468.6220703125\n",
      "Epoch: 11, Batch number: 350, Loss: 5478.05712890625\n",
      "Exiting training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 150              # Cantidad de epochs\n",
    "learning_rate = 5e-3      # Tasa de aprendizaje\n",
    "sample_loss_every = 10    # Calcular la loss cada este número\n",
    "algorithm = 'Adam'        # Algoritmo de optimización\n",
    "\n",
    "trainer.Train(algorithm=algorithm, epochs=epochs, sample_loss_every=sample_loss_every, lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
