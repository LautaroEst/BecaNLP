{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "\n",
    "# WordVectors.py\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_dict={}, frequencies_dict={}):\n",
    "        \n",
    "        self._idx_to_tk = tokens_dict\n",
    "        self._tk_to_idx = {tk: idx for idx, tk in tokens_dict.items()}\n",
    "        self._idx_to_freq = frequencies_dict\n",
    "        self.max_idx = len(self)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, cutoff_freq=0):\n",
    "        corpus_words = sorted(list(set([item for sublist in corpus for item in sublist])))\n",
    "        freqs_dict = {word: 0 for word in corpus_words}\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                freqs_dict[token] += 1\n",
    "        freqs = np.array(list(freqs_dict.values()))\n",
    "        mask = freqs > cutoff_freq\n",
    "        corpus_words = {idx: tk for idx, tk in enumerate(itertools.compress(corpus_words,mask))}\n",
    "        freqs = {idx: freq for idx, freq in enumerate(freqs[mask])}\n",
    "        return cls(corpus_words, freqs)\n",
    "\n",
    "    def index_to_token(self, index):\n",
    "        return self._idx_to_tk[index]\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        return self._tk_to_idx[token]\n",
    "        \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            freq = 0 if tk_or_idx not in self._tk_to_idx else self._idx_to_freq[self._tk_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._idx_to_tk)\n",
    "    \n",
    "    def __getitem__(self,tk_or_idx):\n",
    "        if isinstance(tk_or_idx, int):\n",
    "            return self.index_to_token(tk_or_idx)\n",
    "        if isinstance(tk_or_idx, str):\n",
    "            return self.token_to_index(tk_or_idx)\n",
    "        raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current >= self.max_idx:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            token = self._idx_to_tk[self.current]\n",
    "            self.current += 1\n",
    "            return token\n",
    "\n",
    "    def __contains__(self,key):\n",
    "        return key in self._tk_to_idx\n",
    "    \n",
    "    \n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    unk_token = '<UNK>'\n",
    "    \n",
    "    def samples_generator(self, doc):\n",
    "        for t, token in enumerate(doc):\n",
    "            if token in self.vocabulary:\n",
    "                len_doc = len(doc)\n",
    "                cond1 = max(-1,t-self.window_size) == -1\n",
    "                cond2 = min(t+self.window_size, len_doc) == len_doc\n",
    "                if cond1 and cond2:\n",
    "                    context = itertools.chain(doc[:t],doc[t+1:])\n",
    "                if cond1 and not cond2:\n",
    "                    context = itertools.chain(doc[:t],doc[t+1:t+self.window_size+1])\n",
    "                if cond2 and not cond1:\n",
    "                    context = itertools.chain(doc[t-self.window_size:t],doc[t+1:])\n",
    "                if not cond1 and not cond2:\n",
    "                    context = itertools.chain(doc[t-self.window_size:t],doc[t+1:t+self.window_size+1])\n",
    "\n",
    "                context_list = [self.vocabulary.token_to_index(tk) for tk in context if tk in self.vocabulary]\n",
    "                if len(context_list) != 0:\n",
    "                    yield (self.vocabulary.token_to_index(token), context_list)\n",
    "    \n",
    "\n",
    "    def __init__(self, corpus, window_size=2, cutoff_freq=0):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.vocabulary = Vocabulary.from_corpus(corpus,cutoff_freq=cutoff_freq)\n",
    "    \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        word_indeces = []\n",
    "        word_contexts = []\n",
    "        for doc in corpus:\n",
    "            gen = self.samples_generator(doc)\n",
    "            for word_index, word_context in gen:\n",
    "                word_indeces.append(word_index)\n",
    "                padd_num = 2 * window_size - len(word_context)\n",
    "                if padd_num > 0:\n",
    "                    word_contexts.append(word_context + [self.padding_idx for i in range(padd_num)])\n",
    "                else:\n",
    "                    word_contexts.append(word_context)\n",
    "        \n",
    "        self.word_indeces = torch.tensor(word_indeces,dtype=torch.long)\n",
    "        self.context_indeces = torch.tensor(word_contexts,dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.word_indeces[idx], self.context_indeces[idx,:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_indeces)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Training.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "#from .WordVectors import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ModelTrainer(object):\n",
    "    \"\"\"\n",
    "        Clase madre de todos los trainers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_dataset,\n",
    "                 test_dataset,\n",
    "                 batch_size=64,\n",
    "                 val_size=.02):\n",
    "        \n",
    "        # Data:\n",
    "        tr, val, te = self.generate_data_batches(train_dataset, test_dataset,batch_size,val_size)\n",
    "        self.train_dataloader, self.val_dataloader, self.test_dataloader = tr, val, te\n",
    "        \n",
    "        # Data-types:\n",
    "        self.input_dtype = next(iter(self.train_dataloader))[0].dtype\n",
    "        self.target_dtype = next(iter(self.train_dataloader))[1].dtype\n",
    "        \n",
    "        self.first_time = True\n",
    "        self.batch_len = len(self.train_dataloader)\n",
    "        \n",
    "        print('Model trainer created:')\n",
    "        train_samples = int((1 - val_size) * len(train_dataset)) \n",
    "        val_samples = len(train_dataset) - train_samples\n",
    "        test_samples = len(test_dataset)\n",
    "        total_samples = train_samples + val_samples + test_samples\n",
    "        percent_val, percent_test = int((val_samples / total_samples) * 100), int((test_samples / total_samples) * 100)\n",
    "        print('Number of training samples: {} ({}%)'.format(train_samples, 100 - percent_val - percent_test))\n",
    "        print('Number of validation samples: {} ({}%)'.format(val_samples, percent_val))\n",
    "        print('Number of test samples: {} ({}%)'.format(test_samples, percent_test))\n",
    "        print('Number of train batches: {}'.format(self.batch_len))\n",
    "        print('Number of samples per batch: {}'.format(batch_size))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    def generate_data_batches(self,train_dataset, test_dataset, # Train y test datasets\n",
    "                              batch_size = 64, # Tamaño del batch\n",
    "                              val_size = .02): # Proporción de muestras utilizadas para validación \n",
    "    \n",
    "        \"\"\"\n",
    "            Función para iterar sobre los batches de muestras. \n",
    "            Devuelve los dataloaders de train / validation / test.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Separo las muestras aleatoriamente en Train y Validation:\n",
    "        NUM_TRAIN = int((1 - val_size) * len(train_dataset)) \n",
    "        samples_idx = torch.randperm(len(train_dataset))\n",
    "        train_samples_idx = samples_idx[:NUM_TRAIN]\n",
    "        val_samples_idx = samples_idx[NUM_TRAIN:]\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices) # sampler\n",
    "\n",
    "        # Dataloader para las muestras de entrenamiento:\n",
    "        train_dataloader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      sampler=my_sampler(train_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de validación:\n",
    "        val_dataloader = DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=my_sampler(val_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de testeo:\n",
    "        test_dataloader = DataLoader(test_dataset, \n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "        return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "    \n",
    "    def InitModel(self, model, state_dict=None, device='cpu'):\n",
    "        \n",
    "        # Defino el dispositivo sobre el cual trabajar:\n",
    "        if device is None:\n",
    "            self.device = torch.device('cpu')\n",
    "            print('No se seleccionó ningún dispositivo de entrenamiento. Se asigna la cpu')\n",
    "        elif device == 'cpu':\n",
    "            self.device = torch.device('cpu')\n",
    "            print('Dispositivo seleccionado: cpu')\n",
    "        elif device == 'cuda:0' or device == 'cuda:1':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(device)\n",
    "                print('Dispositivo seleccionado: {}'.format(device))\n",
    "            else:\n",
    "                self.device = torch.device('cpu')\n",
    "                print('No se dispone de GPUs. Se asigna como dispositivo de entrenamiento la cpu')\n",
    "        else:\n",
    "            raise TypeError('No se seleccionó un dispositivo válido')\n",
    "            \n",
    "        # Defino el modelo:\n",
    "        self.model = model\n",
    "        \n",
    "        # Inicializo con los parámetros de state_dict si hubiera:\n",
    "        if state_dict is not None:\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        \n",
    "        # Copio el modelo al dispositivo:\n",
    "        self.model = self.model.to(device=self.device)\n",
    "\n",
    "    def SaveModel(self,file):\n",
    "        \n",
    "        try:\n",
    "            torch.save(self.model.state_dict(),file)\n",
    "            print('Embeddings saved to file {}'.format(file))\n",
    "        except:\n",
    "            print('Embeddings could not be saved to file')\n",
    "        \n",
    "        \n",
    "    def Train(self, algorithm='SGD', epochs=1, sample_loss_every=100, check_on_train=False, **kwargs):\n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            optimizer = optim.SGD(self.model.parameters(), **kwargs)\n",
    "        elif algorithm == 'Adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(), **kwargs)\n",
    "        self.model.train()\n",
    "        \n",
    "        if self.first_time:\n",
    "            print('Starting training...')\n",
    "            self.loss_history = {'iter': [], 'loss': []}\n",
    "            n_iter = 0\n",
    "            self.first_time = False\n",
    "        else:\n",
    "            n_iter = self.loss_history['iter'][-1]\n",
    "            print('Resuming training...')\n",
    "        \n",
    "        print('Optimization method: {}'.format(algorithm))\n",
    "        print('Learning Rate: {:.2g}'.format(kwargs['lr']))\n",
    "        print('Number of epochs: {}'.format(epochs))\n",
    "        print('Running on device ({})'.format(self.device))\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            for e in range(epochs):\n",
    "                for t, (x,y) in enumerate(self.train_dataloader):\n",
    "\n",
    "                    x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "                    y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                    optimizer.zero_grad() # Llevo a cero los gradientes de la red\n",
    "                    scores = self.model(x) # Calculo la salida de la red\n",
    "                    loss = self.model.loss(scores,y) # Calculo el valor de la loss\n",
    "                    loss.backward() # Calculo los gradientes\n",
    "                    optimizer.step() # Actualizo los parámetros\n",
    "\n",
    "                    if (e * self.batch_len + t) % sample_loss_every == 0:\n",
    "                        num_correct_val, num_samples_val = self.check_accuracy('validation')\n",
    "                        self.performance_history['iter'].append(e * self.batch_len + t + n_iter)\n",
    "                        self.performance_history['loss'].append(loss.item())\n",
    "                        self.performance_history['accuracy'].append(float(num_correct_val / num_samples_val))\n",
    "                        print('Epoch: {}, Batch number: {}'.format(e+1, t))\n",
    "                        print('Accuracy on validation dataset: {}/{} ({:.2f}%)'.format(num_correct_val, num_samples_val, 100 * float(num_correct_val) / num_samples_val))\n",
    "                        print()\n",
    "\n",
    "                        if check_on_train:\n",
    "                            num_correct_train, num_samples_train = self.check_accuracy('train')\n",
    "                            print('Accuracy on train dataset: {}/{} ({:.2f}%)'.format(num_correct_train, num_samples_train, 100 * float(num_correct_train) / num_samples_train))\n",
    "                            print()\n",
    "\n",
    "            print('Training finished')\n",
    "            print()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "\n",
    "            print('Exiting training...')\n",
    "            print()    \n",
    "\n",
    "    def check_accuracy(self, dataset='validation'):\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        if dataset == 'train':\n",
    "            loader = self.train_dataloader\n",
    "        elif dataset == 'validation':\n",
    "            loader = self.val_dataloader\n",
    "        elif dataset == 'test':\n",
    "            loader = self.test_dataloader\n",
    "        else:\n",
    "            raise AttributeError('Please specify on which dataset to perform de accuracy calculation')\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=self.input_dtype)  \n",
    "                y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                scores = self.model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "\n",
    "        self.model.train()\n",
    "        return num_correct, num_samples\n",
    "\n",
    "    def CheckResultsOnTest(self):\n",
    "        \n",
    "        total_corrects = 0\n",
    "        total_samples = 0\n",
    "        total_performance = 0.\n",
    "        \n",
    "        for (x,y) in enumerate(self.test_dataloader):\n",
    "            x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "            y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "            num_correct, num_samples = self.check_accuracy('test')\n",
    "            total_corrects += num_corrects\n",
    "            total_samples += num_samples\n",
    "            total_performance += float(num_correct / num_samples)\n",
    "        \n",
    "        print('Final accuracy on test set: {}/{} ({}%)'.format(total_corrects,total_samples,total_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vecTrainer(ModelTrainer):\n",
    "    \n",
    "    \"\"\"\n",
    "        Clase para entrenar word embeddings. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_corpus,           # Corpus de entrenamiento (debe ser una lista de listas de strings).\n",
    "                 test_corpus=None,       # Corpus para testear los resultados. Puede no darse ninguno.\n",
    "                 cutoff_freq=0,          # Descartar palabras cuya frecuencia sea menor a este valor.\n",
    "                 window_size=2,          # Tamaño de la ventana.\n",
    "                 batch_size=64):         # Tamaño del batch.\n",
    "        \n",
    "        #iter_trough_token = itertools.chain(corpus)\n",
    "        \n",
    "        # Obtengo los batches de muestras:\n",
    "        word2vec_samples = Word2VecSamples(train_corpus, window_size=window_size, cutoff_freq=cutoff_freq)\n",
    "        samples_idx = torch.randperm(len(word2vec_samples))\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "        self.dataloader = DataLoader(word2vec_samples, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "        self.batch_len = len(self.dataloader)\n",
    "        \n",
    "        \n",
    "corpus = [['Esto', 'es', 'un', 'corpus', 'de', 'prueba'], ['Esto', 'también'], ['corpus', 'de', 'prueba']]\n",
    "trainer = Word2vecTrainer(corpus,cutoff_freq=0,window_size=2,batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
