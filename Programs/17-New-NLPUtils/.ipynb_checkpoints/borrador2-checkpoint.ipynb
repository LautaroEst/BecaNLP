{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase madre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \n",
    "    def generate_data_batches(self,train_dataset, test_dataset, # Train y test datasets\n",
    "                              batch_size = 64, # Tamaño del batch\n",
    "                              val_size = .02): # Proporción de muestras utilizadas para validación \n",
    "\n",
    "        \"\"\"\n",
    "            Función para iterar sobre los batches de muestras. \n",
    "            Devuelve los dataloaders de train / validation / test.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Sampler\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices) \n",
    "        \n",
    "        if val_size != 0:\n",
    "            # Separo las muestras aleatoriamente en Train y Validation:\n",
    "            NUM_TRAIN = int((1 - val_size) * len(train_dataset)) \n",
    "            samples_idx = torch.randperm(len(train_dataset))\n",
    "            train_samples_idx = samples_idx[:NUM_TRAIN]\n",
    "            val_samples_idx = samples_idx[NUM_TRAIN:]\n",
    "            # Dataloader para las muestras de validación:\n",
    "            val_dataloader = DataLoader(train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        sampler=my_sampler(val_samples_idx))\n",
    "        else:\n",
    "            val_dataloader = None\n",
    "            \n",
    "        if test_dataset is not None:\n",
    "            # Dataloader para las muestras de testeo:\n",
    "            test_dataloader = DataLoader(test_dataset, \n",
    "                                         batch_size=batch_size)\n",
    "        else:\n",
    "            test_dataloader = None\n",
    "            \n",
    "        # Dataloader para las muestras de entrenamiento:\n",
    "        train_dataloader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      sampler=my_sampler(train_samples_idx))\n",
    "\n",
    "        return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_dataloader,\n",
    "                 val_dataloader,\n",
    "                 test_dataloader):\n",
    "        \n",
    "        # Dataloaders:\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        \n",
    "        # Data-types:\n",
    "        self.input_dtype = next(iter(self.train_dataloader))[0].dtype\n",
    "        self.target_dtype = next(iter(self.train_dataloader))[1].dtype\n",
    "        \n",
    "        \n",
    "    def InitModel(self, model, state_dict=None, device='cpu'):\n",
    "        \n",
    "        # Defino el dispositivo sobre el cual trabajar:\n",
    "        if device is None:\n",
    "            self.device = torch.device('cpu')\n",
    "            print('No se seleccionó ningún dispositivo de entrenamiento. Se asigna la cpu')\n",
    "        elif device == 'cpu':\n",
    "            self.device = torch.device('cpu')\n",
    "            print('Dispositivo seleccionado: cpu')\n",
    "        elif device == 'cuda:0' or device == 'cuda:1':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(device)\n",
    "                print('Dispositivo seleccionado: {}'.format(device))\n",
    "            else:\n",
    "                self.device = torch.device('cpu')\n",
    "                print('No se dispone de GPUs. Se asigna como dispositivo de entrenamiento la cpu')\n",
    "        else:\n",
    "            raise TypeError('No se seleccionó un dispositivo válido')\n",
    "            \n",
    "        # Defino el modelo:\n",
    "        self.model = model\n",
    "        \n",
    "        # Inicializo con los parámetros de state_dict si hubiera:\n",
    "        if state_dict is not None:\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        \n",
    "        # Copio el modelo al dispositivo:\n",
    "        self.model = self.model.to(device=self.device)\n",
    "    \n",
    "    def SaveModel(self,file):\n",
    "        \n",
    "        try:\n",
    "            torch.save(self.model.state_dict(),file)\n",
    "            print('Embeddings saved to file {}'.format(file))\n",
    "        except:\n",
    "            print('Embeddings could not be saved to file')\n",
    "    \n",
    "    def Train(self, epochs=1, sample_loss_every=100, algorithm='SGD', **kwargs):\n",
    "        \n",
    "        # Defino el algoritmo de optimización:\n",
    "        if algorithm == 'SGD':\n",
    "            optimizer = optim.SGD(self.model.parameters(), **kwargs)\n",
    "        elif algorithm == 'Adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(), **kwargs)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Identifico si es la primera vez que entreno o no:\n",
    "        try:\n",
    "            n_iter = self.performance_history['iter'][-1]\n",
    "            print('Resuming training...')\n",
    "        except (IndexError, AttributeError):\n",
    "            print('Starting training...')\n",
    "            self.performance_history = {'iter': [], 'loss': []}\n",
    "            n_iter = 0\n",
    "        \n",
    "        # Varios:\n",
    "        print('Optimization method: {}'.format(algorithm))\n",
    "        print('Learning Rate: {:.2g}'.format(kwargs['lr']))\n",
    "        print('Number of epochs: {}'.format(epochs))\n",
    "        print('Running on device ({})'.format(self.device))\n",
    "        print()\n",
    "        \n",
    "        # Comienzo a entrenar:\n",
    "        batch_len = len(self.train_dataloader)\n",
    "        try:\n",
    "    \n",
    "            for e in range(epochs):\n",
    "                for t, (x,y) in enumerate(self.train_dataloader):\n",
    "\n",
    "                    x = x.to(device=self.device, dtype=self.input_dtype)\n",
    "                    y = y.to(device=self.device, dtype=self.target_dtype)\n",
    "\n",
    "                    optimizer.zero_grad() # Llevo a cero los gradientes de la red\n",
    "                    scores = self.model(x) # Calculo la salida de la red\n",
    "                    loss = self.Loss(scores,y) # Calculo el valor de la loss\n",
    "                    loss.backward() # Calculo los gradientes\n",
    "                    optimizer.step() # Actualizo los parámetros\n",
    "                    \n",
    "                    if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                        l = loss.item()\n",
    "                        print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,l))\n",
    "                        self.performance_history['iter'].append(e * batch_len + t + n_iter)\n",
    "                        self.performance_history['loss'].append(l)\n",
    "                        self.EvalPerformance()\n",
    "                    \n",
    "            print('Training finished')\n",
    "            print()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "\n",
    "            print('Exiting training...')\n",
    "            print()   \n",
    "            \n",
    "    def Loss(self,scores,target):\n",
    "        pass\n",
    "    \n",
    "    def EvalPerformance(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n",
      "Starting training...\n",
      "Optimization method: SGD\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 1\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.0\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_data_batches(train_dataset, test_dataset, # Train y test datasets\n",
    "                          batch_size = 64, # Tamaño del batch\n",
    "                          val_size = .02): # Proporción de muestras utilizadas para validación \n",
    "\n",
    "    \"\"\"\n",
    "        Función para iterar sobre los batches de muestras. \n",
    "        Devuelve los dataloaders de train / validation / test.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Separo las muestras aleatoriamente en Train y Validation:\n",
    "    NUM_TRAIN = int((1 - val_size) * len(train_dataset)) \n",
    "    samples_idx = torch.randperm(len(train_dataset))\n",
    "    train_samples_idx = samples_idx[:NUM_TRAIN]\n",
    "    val_samples_idx = samples_idx[NUM_TRAIN:]\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices) # sampler\n",
    "\n",
    "    # Dataloader para las muestras de entrenamiento:\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size=batch_size, \n",
    "                                  sampler=my_sampler(train_samples_idx))\n",
    "\n",
    "    # Dataloader para las muestras de validación:\n",
    "    val_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=batch_size, \n",
    "                                sampler=my_sampler(val_samples_idx))\n",
    "\n",
    "    # Dataloader para las muestras de testeo:\n",
    "    test_dataloader = DataLoader(test_dataset, \n",
    "                                 batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,j):\n",
    "        \n",
    "        self.x_samples = torch.tensor([[i] for i in range(j,1000+j)],dtype=torch.float)\n",
    "        self.y_samples = torch.tensor([0 for i in range(j,1000+j)],dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_samples[idx], self.y_samples[idx]\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "def Loss(scores,target):\n",
    "    lf = nn.CrossEntropyLoss(reduction='sum')\n",
    "    return lf(scores,target)\n",
    "    \n",
    "train_dataset = ToyDataset(10)\n",
    "test_dataset = ToyDataset(30)\n",
    "model = Model()\n",
    "train_dataloader, val_dataloader, test_dataloader = generate_data_batches(train_dataset, test_dataset, batch_size = 64, val_size = .02)\n",
    "trainer = Trainer(train_dataloader, val_dataloader, test_dataloader)\n",
    "trainer.Loss = Loss\n",
    "trainer.InitModel(model)\n",
    "trainer.Train(epochs=1, algorithm='SGD', lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n",
      "Starting training...\n",
      "Optimization method: SGD\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 1\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.0\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TrainValTestTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, train_dataset, test_dataset, batch_size, val_size):\n",
    "        \n",
    "        tr, val, te = self.generate_data_batches(train_dataset, test_dataset,batch_size,val_size)\n",
    "        super().__init__(tr, val, te)\n",
    "    \n",
    "    def generate_data_batches(self, train_dataset, test_dataset, # Train y test datasets\n",
    "                              batch_size = 64, # Tamaño del batch\n",
    "                              val_size = .02): # Proporción de muestras utilizadas para validación \n",
    "\n",
    "        \"\"\"\n",
    "            Función para iterar sobre los batches de muestras. \n",
    "            Devuelve los dataloaders de train / validation / test.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Separo las muestras aleatoriamente en Train y Validation:\n",
    "        NUM_TRAIN = int((1 - val_size) * len(train_dataset)) \n",
    "        samples_idx = torch.randperm(len(train_dataset))\n",
    "        train_samples_idx = samples_idx[:NUM_TRAIN]\n",
    "        val_samples_idx = samples_idx[NUM_TRAIN:]\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices) # sampler\n",
    "\n",
    "        # Dataloader para las muestras de entrenamiento:\n",
    "        train_dataloader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      sampler=my_sampler(train_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de validación:\n",
    "        val_dataloader = DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=my_sampler(val_samples_idx))\n",
    "\n",
    "        # Dataloader para las muestras de testeo:\n",
    "        test_dataloader = DataLoader(test_dataset, \n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "        return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "    def Loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(reduction='sum')\n",
    "        return lf(scores,target)\n",
    "    \n",
    "    \n",
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,j):\n",
    "        \n",
    "        self.x_samples = torch.tensor([[i] for i in range(j,1000+j)],dtype=torch.float)\n",
    "        self.y_samples = torch.tensor([0 for i in range(j,1000+j)],dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_samples[idx], self.y_samples[idx]\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "train_dataset = ToyDataset(10)\n",
    "test_dataset = ToyDataset(30)\n",
    "model = Model()\n",
    "trainer = TrainValTestTrainer(train_dataset, test_dataset, batch_size = 64, val_size = .02)\n",
    "trainer.InitModel(model)\n",
    "trainer.Train(epochs=1, algorithm='SGD', lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n",
      "Starting training...\n",
      "Optimization method: SGD\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 10\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 18.593631744384766\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    unk_token = '<UNK>'\n",
    "    \n",
    "    def samples_generator(self, doc):\n",
    "        for t, token in enumerate(doc):\n",
    "            if token in self.vocabulary:\n",
    "                len_doc = len(doc)\n",
    "                cond1 = max(-1,t-self.window_size) == -1\n",
    "                cond2 = min(t+self.window_size, len_doc) == len_doc\n",
    "                if cond1 and cond2:\n",
    "                    context = itertools.chain(doc[:t],doc[t+1:])\n",
    "                if cond1 and not cond2:\n",
    "                    context = itertools.chain(doc[:t],doc[t+1:t+self.window_size+1])\n",
    "                if cond2 and not cond1:\n",
    "                    context = itertools.chain(doc[t-self.window_size:t],doc[t+1:])\n",
    "                if not cond1 and not cond2:\n",
    "                    context = itertools.chain(doc[t-self.window_size:t],doc[t+1:t+self.window_size+1])\n",
    "\n",
    "                context_list = [self.vocabulary.token_to_index(tk) for tk in context if tk in self.vocabulary]\n",
    "                if len(context_list) != 0:\n",
    "                    yield (self.vocabulary.token_to_index(token), context_list)\n",
    "    \n",
    "\n",
    "    def __init__(self, corpus, window_size=2, cutoff_freq=0):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.vocabulary = Vocabulary.from_corpus(corpus,cutoff_freq=cutoff_freq)\n",
    "    \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        word_indeces = []\n",
    "        word_contexts = []\n",
    "        for doc in corpus:\n",
    "            gen = self.samples_generator(doc)\n",
    "            for word_index, word_context in gen:\n",
    "                word_indeces.append(word_index)\n",
    "                padd_num = 2 * window_size - len(word_context)\n",
    "                if padd_num > 0:\n",
    "                    word_contexts.append(word_context + [self.padding_idx for i in range(padd_num)])\n",
    "                else:\n",
    "                    word_contexts.append(word_context)\n",
    "        \n",
    "        self.word_indeces = torch.tensor(word_indeces,dtype=torch.long)\n",
    "        self.context_indeces = torch.tensor(word_contexts,dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.context_indeces[idx,:], self.word_indeces[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_indeces)\n",
    "    \n",
    "\n",
    "class Word2VecTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 corpus,                 # Corpus de entrenamiento (debe ser una lista de listas de strings).\n",
    "                 cutoff_freq=1,          # Descartar palabras cuya frecuencia sea menor o igual a este valor.\n",
    "                 window_size=2,          # Tamaño de la ventana.\n",
    "                 batch_size=64):         # Tamaño del batch.\n",
    "        \n",
    "        self.cutoff_freq = cutoff_freq\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Obtengo los batches de muestras:\n",
    "        dataset = Word2VecSamples(corpus, window_size=window_size, cutoff_freq=cutoff_freq)\n",
    "        samples_idx = torch.randperm(len(dataset))\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "        \n",
    "        self.vocab_size = len(dataset.vocabulary)\n",
    "        \n",
    "        super().__init__(dataloader, None, None)\n",
    "        \n",
    "    def Loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(reduction='sum')\n",
    "        return lf(scores,target)\n",
    "        \n",
    "import numpy as np        \n",
    "import itertools\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_dict={}, frequencies_dict={}):\n",
    "        \n",
    "        self._idx_to_tk = tokens_dict\n",
    "        self._tk_to_idx = {tk: idx for idx, tk in tokens_dict.items()}\n",
    "        self._idx_to_freq = frequencies_dict\n",
    "        self.max_idx = len(self)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, cutoff_freq=0):\n",
    "        corpus_words = sorted(list(set([item for sublist in corpus for item in sublist])))\n",
    "        freqs_dict = {word: 0 for word in corpus_words}\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                freqs_dict[token] += 1\n",
    "        freqs = np.array(list(freqs_dict.values()))\n",
    "        mask = freqs > cutoff_freq\n",
    "        corpus_words = {idx: tk for idx, tk in enumerate(itertools.compress(corpus_words,mask))}\n",
    "        freqs = {idx: freq for idx, freq in enumerate(freqs[mask])}\n",
    "        return cls(corpus_words, freqs)\n",
    "\n",
    "    def index_to_token(self, index):\n",
    "        return self._idx_to_tk[index]\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        return self._tk_to_idx[token]\n",
    "        \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            freq = 0 if tk_or_idx not in self._tk_to_idx else self._idx_to_freq[self._tk_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._idx_to_tk)\n",
    "    \n",
    "    def __getitem__(self,tk_or_idx):\n",
    "        if isinstance(tk_or_idx, int):\n",
    "            return self.index_to_token(tk_or_idx)\n",
    "        if isinstance(tk_or_idx, str):\n",
    "            return self.token_to_index(tk_or_idx)\n",
    "        raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current >= self.max_idx:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            token = self._idx_to_tk[self.current]\n",
    "            self.current += 1\n",
    "            return token\n",
    "\n",
    "    def __contains__(self,key):\n",
    "        return key in self._tk_to_idx   \n",
    "        \n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)    \n",
    "\n",
    "        \n",
    "corpus = [['w1', 'w2', 'w2', 'w3'], ['w3', 'w2', 'w1', 'w1', 'w1', 'w2', 'w1', 'w1'], ['w1', 'w1', 'w1']]\n",
    "trainer = Word2VecTrainer(corpus, 0, 2, 64)\n",
    "model = CBOWModel(len(trainer.train_dataloader.dataset.vocabulary),10)\n",
    "trainer.InitModel(model)\n",
    "trainer.Train(epochs=10, algorithm='SGD', lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n",
      "Starting training...\n",
      "Optimization method: SGD\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 10\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 18.626895904541016\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Word2VecTrainer(Trainer):\n",
    "    \n",
    "    model_name = 'CBOW'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 corpus,                 # Corpus de entrenamiento (debe ser una lista de listas de strings).\n",
    "                 cutoff_freq=1,          # Descartar palabras cuya frecuencia sea menor o igual a este valor.\n",
    "                 window_size=2,          # Tamaño de la ventana.\n",
    "                 batch_size=64):         # Tamaño del batch.\n",
    "        \n",
    "        self.cutoff_freq = cutoff_freq\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Obtengo los batches de muestras:\n",
    "        dataset = Word2VecSamples(corpus, window_size=window_size, cutoff_freq=cutoff_freq)\n",
    "        samples_idx = torch.randperm(len(dataset))\n",
    "        my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "        \n",
    "        self.vocab_size = len(dataset.vocabulary)\n",
    "        \n",
    "        super().__init__(dataloader, None, None)\n",
    "        \n",
    "    class CBOWModel(nn.Module):\n",
    "    \n",
    "        def __init__(self, vocab_size, embedding_dim):\n",
    "            super().__init__()\n",
    "            self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "            self.out = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "        def forward(self,x):\n",
    "            embedding = self.emb(x).mean(dim=1)\n",
    "            return self.out(embedding)\n",
    "        \n",
    "    class SkipGramModel(nn.Module):\n",
    "    \n",
    "        def __init__(self, vocab_size, embedding_dim):\n",
    "            super(SkipGramModel,self).__init__()\n",
    "            self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "            self.out = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "            self.vocab_size = vocab_size\n",
    "\n",
    "        def forward(self,x):\n",
    "            return self.out(self.emb(x))\n",
    "\n",
    "\n",
    "    def InitModel(self, model, embedding_dim, state_dict=None, device='cpu'):\n",
    "        \n",
    "        self.model_name = model\n",
    "        if model == 'CBOW':\n",
    "            model = self.CBOWModel(len(self.train_dataloader.dataset.vocabulary),embedding_dim)\n",
    "            self.Loss = self.CBOWLoss\n",
    "        elif model == 'SkipGram':\n",
    "            model = self.SkipGramModel(len(self.train_dataloader.dataset.vocabulary),embedding_dim)\n",
    "            self.Loss = self.SkipGramLoss\n",
    "        \n",
    "        super().InitModel(model, state_dict, device)\n",
    "    \n",
    "    \n",
    "    def CBOWLoss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(reduction='sum')\n",
    "        return lf(scores,target)\n",
    "    \n",
    "    def SkipGramLoss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size,reduction='sum')\n",
    "        scores = scores.view(-1,self.vocab_size,1).repeat(1,1,target.size(1))\n",
    "        return lf(scores,target)\n",
    "    \n",
    "    def EvalPerformance(self):\n",
    "        \n",
    "    \n",
    "    \n",
    "corpus = [['w1', 'w2', 'w2', 'w3'], ['w3', 'w2', 'w1', 'w1', 'w1', 'w2', 'w1', 'w1'], ['w1', 'w1', 'w1']]\n",
    "trainer = Word2VecTrainer(corpus, 0, 2, 64)\n",
    "trainer.InitModel('CBOW',10)\n",
    "trainer.Train(epochs=10, algorithm='SGD', lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### UTILS ####################\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_dict={}, frequencies_dict={}):\n",
    "        \n",
    "        self._idx_to_tk = tokens_dict\n",
    "        self._tk_to_idx = {tk: idx for idx, tk in tokens_dict.items()}\n",
    "        self._idx_to_freq = frequencies_dict\n",
    "        self.max_idx = len(self)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, cutoff_freq=0):\n",
    "        corpus_words = sorted(list(set([item for sublist in corpus for item in sublist])))\n",
    "        freqs_dict = {word: 0 for word in corpus_words}\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                freqs_dict[token] += 1\n",
    "        freqs = np.array(list(freqs_dict.values()))\n",
    "        mask = freqs > cutoff_freq\n",
    "        corpus_words = {idx: tk for idx, tk in enumerate(itertools.compress(corpus_words,mask))}\n",
    "        freqs = {idx: freq for idx, freq in enumerate(freqs[mask])}\n",
    "        return cls(corpus_words, freqs)\n",
    "\n",
    "    def index_to_token(self, index):\n",
    "        return self._idx_to_tk[index]\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        return self._tk_to_idx[token]\n",
    "        \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            freq = 0 if tk_or_idx not in self._tk_to_idx else self._idx_to_freq[self._tk_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._idx_to_tk)\n",
    "    \n",
    "    def __getitem__(self,tk_or_idx):\n",
    "        if isinstance(tk_or_idx, int):\n",
    "            return self.index_to_token(tk_or_idx)\n",
    "        if isinstance(tk_or_idx, str):\n",
    "            return self.token_to_index(tk_or_idx)\n",
    "        raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current >= self.max_idx:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            token = self._idx_to_tk[self.current]\n",
    "            self.current += 1\n",
    "            return token\n",
    "\n",
    "    def __contains__(self,key):\n",
    "        return key in self._tk_to_idx\n",
    "\n",
    "    \n",
    "def samples_generator(doc, vocab, window_size):\n",
    "    for t, token in enumerate(doc):\n",
    "        if token in vocab:\n",
    "            len_doc = len(doc)\n",
    "            cond1 = max(-1,t-window_size) == -1\n",
    "            cond2 = min(t+window_size, len_doc) == len_doc\n",
    "            if cond1 and cond2:\n",
    "                context = itertools.chain(doc[:t],doc[t+1:])\n",
    "            if cond1 and not cond2:\n",
    "                context = itertools.chain(doc[:t],doc[t+1:t+window_size+1])\n",
    "            if cond2 and not cond1:\n",
    "                context = itertools.chain(doc[t-window_size:t],doc[t+1:])\n",
    "            if not cond1 and not cond2:\n",
    "                context = itertools.chain(doc[t-window_size:t],doc[t+1:t+window_size+1])\n",
    "\n",
    "            context_list = [vocab.token_to_index(tk) for tk in context if tk in vocab]\n",
    "            if len(context_list) != 0:\n",
    "                yield vocab.token_to_index(token), context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Samples(Dataset):\n",
    "    \n",
    "    unk_token = '<UNK>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2, cutoff_freq=0):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.vocabulary = Vocabulary.from_corpus(corpus,cutoff_freq=cutoff_freq)\n",
    "    \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        word_indeces = []\n",
    "        word_contexts = []\n",
    "        for doc in corpus:\n",
    "            gen = samples_generator(doc, vocab, window_size)\n",
    "            for word_index, word_context in gen:\n",
    "                word_indeces.append(word_index)\n",
    "                padd_num = 2 * window_size - len(word_context)\n",
    "                if padd_num > 0:\n",
    "                    word_contexts.append(word_context + [self.padding_idx for i in range(padd_num)])\n",
    "                else:\n",
    "                    word_contexts.append(word_context)\n",
    "        \n",
    "        self.word_indeces = torch.tensor(word_indeces,dtype=torch.long)\n",
    "        self.context_indeces = torch.tensor(word_contexts,dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.context_indeces[idx,:], self.word_indeces[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_indeces)\n",
    "    \n",
    "    \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "\n",
    "\n",
    "    \n",
    "class CBOWTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, samples, model):\n",
    "        \n",
    "        self.model_cls = model\n",
    "        self.samples_cls = samples\n",
    "        \n",
    "        super().__init__(samples, None, None, )\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------- Desde archivos --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from NLPUtils import *\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo seleccionado: cpu\n",
      "Starting training...\n",
      "Optimization method: SGD\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 10\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15.119260787963867\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = [['w1', 'w2', 'w2', 'w3'], ['w3', 'w2', 'w1', 'w1', 'w1', 'w2', 'w1', 'w1'], ['w1', 'w1', 'w1']]\n",
    "trainer = Word2VecTrainer(corpus, 0, 2, 64)\n",
    "trainer.InitModel('CBOW',10)\n",
    "trainer.Train(epochs=10, algorithm='SGD', lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-5b7654691fcd>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-5b7654691fcd>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    train_dataset = ToyDataset(10)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,j):\n",
    "        \n",
    "        self.x_samples = torch.tensor([[i] for i in range(j,1000+j)],dtype=torch.float)\n",
    "        self.y_samples = torch.tensor([0 for i in range(j,1000+j)],dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_samples[idx], self.y_samples[idx]\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "train_dataset = ToyDataset(10)\n",
    "test_dataset = ToyDataset(30)\n",
    "model = Model()\n",
    "trainer = Trainer(train_dataset, test_dataset, batch_size = 64, val_size = .02)\n",
    "trainer.InitModel(model)\n",
    "trainer.Train(epochs=1, algorithm='SGD', lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
