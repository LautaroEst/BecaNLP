{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing en Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Implementación de Skip-Gram Word2Vec\n",
    "\n",
    "El objetivo es calcular la probabilidad de $P(\\mathbf{O}|\\mathbf{C})$ (probabilidad de las palabras de alrededor dada la palabra centra).\n",
    "\n",
    "Podemos aproximar esta probabilidad con un modelo de Softmax:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{O}=o|\\mathbf{C}=c) = \\frac{\\exp(\\Theta x_c)}{\\sum_{w\\in Vocab}\\exp(\\theta_w^T x_c)}\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\Theta = \n",
    "\\begin{bmatrix}\n",
    "-\\;\\theta_1^T \\;- \\\\\n",
    "-\\; \\theta_2^T \\;- \\\\\n",
    "\\vdots \\\\\n",
    "-\\; \\theta_{|V|}^T \\; -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "y $x_c$ es un *one-hot* vector en la palabra $c$ del vocabulario $V$.\n",
    "Si se define\n",
    "\n",
    "$$\n",
    "\\Theta = U V\n",
    "$$\n",
    "\n",
    "con \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U = \n",
    "\\begin{bmatrix}\n",
    "- \\; u_1^T \\; - \\\\\n",
    "- \\; u_2^T \\; - \\\\\n",
    "\\vdots \\\\\n",
    "- \\; u_{|V|}^T \\; -\n",
    "\\end{bmatrix} & &\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "v_1 & v_2 & \\cdots & v_{|V|} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix} & & \n",
    "u_i , v_i \\in \\mathbb{R}^{n} \\; i= 1, \\ldots, |V|\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "puede verse que la expresión de la probabilidad condicional anterior queda\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathbf{O}=o|\\mathbf{C}=c) &= \\frac{\\exp(\\Theta c)}{\\sum_{w\\in V}\\exp(\\theta_w^T c)} \\\\[.5em]\n",
    "&= \\frac{\\exp(u_o^T v_c)}{\\sum_{w\\in V}\\exp(u_w^T v_c)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $v_c$ es la columna de $V$ correspondiente a la plabra $c$ del vocabulario y $u_o^T$ es la fila $o$ de $U$ correspondiente a la palabra $o$ del vocabulario.\n",
    "\n",
    "El siguiente paso es encontrar los parámetros de la distribución anterior, lo cual se hará minimizando la función de costo *Negative LogLikelihood* por gradiente estocástico. Para eso, se puede utilizar las relaciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial U} = \\frac{\\partial J}{\\partial \\Theta} V^T & \\hspace{3em} &\n",
    "\\frac{\\partial J}{\\partial V} = U^T \\frac{\\partial J}{\\partial \\Theta} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "con $J(U,V) = NLL(P(o|c))$, o hacer el cálculo a mano a partir de la expresión completa:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(U,V) &= - \\log \\left( \\prod_{i=1}^N \\prod_{j=1}^{|V|} P(o_i = j | x_i)^{\\mathbb{1}_{\\{y_i = j\\} }} \\right) \\\\\n",
    "&= \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cd14135fcf59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m train_dataset = Stanford(path='../04-Prueba-standarization/StanfordDataset/datasets/stanfordSentimentTreebank',\n\u001b[0;32m---> 82\u001b[0;31m                          samples='Train')\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cd14135fcf59>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, samples, context_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cd14135fcf59>\u001b[0m in \u001b[0;36mgetSamples\u001b[0;34m(self, corpus, sentences)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mno_sentence\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfirst_context_word_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                           \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_context_word_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                           \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlast_context_word_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                           \u001b[0;34m[\u001b[0m\u001b[0mno_sentence\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_context_word_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cd14135fcf59>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mlast_context_word_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mno_sentence\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfirst_context_word_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                           \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_context_word_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                           \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlast_context_word_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'no_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "class Stanford(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, samples='Train', context_size=2):\n",
    "        \n",
    "        super(Stanford, self).__init__()\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        \n",
    "        sentences = []\n",
    "        with open(path + \"/datasetSentences.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split()[1:]\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower() for w in splitted]]\n",
    "        \n",
    "        split = [[] for i in range(3)]\n",
    "        with open(path + \"/datasetSplit.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split(\",\")\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "        \n",
    "        if samples == 'Train':\n",
    "            self.samples = self.getSamples(split[0], sentences)\n",
    "        elif samples == 'Validation':\n",
    "            self.samples = self.getSamples(split[1], sentences)\n",
    "        elif samples == 'Test':\n",
    "            self.samples = self.getSamples(split[2], sentences)\n",
    "        else:\n",
    "            print('Error: especificar si las muestras son de train, validation o test')\n",
    "            self.samples = None\n",
    "        \n",
    "        # Get the tokens:\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "        for sentence in sentences:\n",
    "            for w in sentence:\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "        tokens[\"UNK\"] = idx\n",
    "        revtokens += [\"UNK\"]\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "\n",
    "        self.word_to_index = tokens\n",
    "        self.revtokens = revtokens\n",
    "        self.index_to_word = {idx: word for word, idx in zip(tokens.keys(),tokens.values())}\n",
    "        self.vocabulary = list(tokens.keys())\n",
    "        self.tokens_freqs = tokenfreq\n",
    "        \n",
    "        \n",
    "    def getSamples(self, corpus, sentences):\n",
    "        samples = []\n",
    "        no_sentence = \n",
    "        for sentence_idx in corpus:\n",
    "            sentence = sentences[sentence_idx]\n",
    "            for i, word in enumerate(sentence):\n",
    "                first_context_word_index = max(0,i-self.context_size)\n",
    "                last_context_word_index = min(i+self.context_size+1, len(sentence))\n",
    "                \n",
    "                context = [no_sentence for j in range(i-self.context_size,first_context_word_index)] + \\\n",
    "                          sentence[first_context_word_index:i] + \\\n",
    "                          sentence[i+1:last_context_word_index] + \\\n",
    "                          [no_sentence for j in range(last_context_word_index,i+self.context_size+1)]\n",
    "                \n",
    "                samples.append((word, context))\n",
    "    \n",
    "train_dataset = Stanford(path='../04-Prueba-standarization/StanfordDataset/datasets/stanfordSentimentTreebank',\n",
    "                         samples='Train')\n",
    "\n",
    "train_dataset.samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'rock',\n",
       "  'is',\n",
       "  'destined',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  '21st',\n",
       "  'century',\n",
       "  \"'s\",\n",
       "  'new',\n",
       "  '``',\n",
       "  'conan',\n",
       "  \"''\",\n",
       "  'and',\n",
       "  'that',\n",
       "  'he',\n",
       "  \"'s\",\n",
       "  'going',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'splash',\n",
       "  'even',\n",
       "  'greater',\n",
       "  'than',\n",
       "  'arnold',\n",
       "  'schwarzenegger',\n",
       "  ',',\n",
       "  'jean-claud',\n",
       "  'van',\n",
       "  'damme',\n",
       "  'or',\n",
       "  'steven',\n",
       "  'segal',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'gorgeously',\n",
       "  'elaborate',\n",
       "  'continuation',\n",
       "  'of',\n",
       "  '``',\n",
       "  'the',\n",
       "  'lord',\n",
       "  'of',\n",
       "  'the',\n",
       "  'rings',\n",
       "  \"''\",\n",
       "  'trilogy',\n",
       "  'is',\n",
       "  'so',\n",
       "  'huge',\n",
       "  'that',\n",
       "  'a',\n",
       "  'column',\n",
       "  'of',\n",
       "  'words',\n",
       "  'can',\n",
       "  'not',\n",
       "  'adequately',\n",
       "  'describe',\n",
       "  'co-writer\\\\/director',\n",
       "  'peter',\n",
       "  'jackson',\n",
       "  \"'s\",\n",
       "  'expanded',\n",
       "  'vision',\n",
       "  'of',\n",
       "  'j.r.r.',\n",
       "  'tolkien',\n",
       "  \"'s\",\n",
       "  'middle-earth',\n",
       "  '.'],\n",
       " ['effective', 'but', 'too-tepid', 'biopic'],\n",
       " ['if',\n",
       "  'you',\n",
       "  'sometimes',\n",
       "  'like',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'the',\n",
       "  'movies',\n",
       "  'to',\n",
       "  'have',\n",
       "  'fun',\n",
       "  ',',\n",
       "  'wasabi',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'place',\n",
       "  'to',\n",
       "  'start',\n",
       "  '.'],\n",
       " ['emerges',\n",
       "  'as',\n",
       "  'something',\n",
       "  'rare',\n",
       "  ',',\n",
       "  'an',\n",
       "  'issue',\n",
       "  'movie',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'so',\n",
       "  'honest',\n",
       "  'and',\n",
       "  'keenly',\n",
       "  'observed',\n",
       "  'that',\n",
       "  'it',\n",
       "  'does',\n",
       "  \"n't\",\n",
       "  'feel',\n",
       "  'like',\n",
       "  'one',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'film',\n",
       "  'provides',\n",
       "  'some',\n",
       "  'great',\n",
       "  'insight',\n",
       "  'into',\n",
       "  'the',\n",
       "  'neurotic',\n",
       "  'mindset',\n",
       "  'of',\n",
       "  'all',\n",
       "  'comics',\n",
       "  '--',\n",
       "  'even',\n",
       "  'those',\n",
       "  'who',\n",
       "  'have',\n",
       "  'reached',\n",
       "  'the',\n",
       "  'absolute',\n",
       "  'top',\n",
       "  'of',\n",
       "  'the',\n",
       "  'game',\n",
       "  '.'],\n",
       " ['offers',\n",
       "  'that',\n",
       "  'rare',\n",
       "  'combination',\n",
       "  'of',\n",
       "  'entertainment',\n",
       "  'and',\n",
       "  'education',\n",
       "  '.'],\n",
       " ['perhaps',\n",
       "  'no',\n",
       "  'picture',\n",
       "  'ever',\n",
       "  'made',\n",
       "  'has',\n",
       "  'more',\n",
       "  'literally',\n",
       "  'showed',\n",
       "  'that',\n",
       "  'the',\n",
       "  'road',\n",
       "  'to',\n",
       "  'hell',\n",
       "  'is',\n",
       "  'paved',\n",
       "  'with',\n",
       "  'good',\n",
       "  'intentions',\n",
       "  '.'],\n",
       " ['steers',\n",
       "  'turns',\n",
       "  'in',\n",
       "  'a',\n",
       "  'snappy',\n",
       "  'screenplay',\n",
       "  'that',\n",
       "  'curls',\n",
       "  'at',\n",
       "  'the',\n",
       "  'edges',\n",
       "  ';',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'so',\n",
       "  'clever',\n",
       "  'you',\n",
       "  'want',\n",
       "  'to',\n",
       "  'hate',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['but', 'he', 'somehow', 'pulls', 'it', 'off', '.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyStanfordSentiment(object):\n",
    "    \n",
    "    def __init__(self, path=None, tablesize = 1000000):\n",
    "        \n",
    "        if not path:\n",
    "            path = \"../04-Prueba-standarization/StanfordDataset/datasets/stanfordSentimentTreebank\"\n",
    "\n",
    "        self.path = path\n",
    "        self.tablesize = tablesize\n",
    "        \n",
    "        # Get the sentences:\n",
    "        sentences = []\n",
    "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split()[1:]\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower() for w in splitted]]\n",
    "        self._sentences = sentences\n",
    "        self._sentlengths = [len(s) for s in sentences]\n",
    "        self._numSentences = len(sentences)\n",
    "        \n",
    "        \n",
    "        # Get the tokens:\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        wordcount = 0\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "        for sentence in sentences:\n",
    "            for w in sentence:\n",
    "                wordcount += 1\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "        tokens[\"UNK\"] = idx\n",
    "        revtokens += [\"UNK\"]\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "        wordcount += 1\n",
    "\n",
    "        self._tokens = tokens\n",
    "        self._tokenfreq = tokenfreq\n",
    "        self._wordcount = wordcount\n",
    "        self._revtokens = revtokens\n",
    "        \n",
    "        # Split Dataset:\n",
    "        split = [[] for i in range(3)]\n",
    "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split(\",\")\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "\n",
    "        self._split = split\n",
    "        \n",
    "        # Reject Probs:\n",
    "        threshold = 1e-5 * self._wordcount\n",
    "        nTokens = len(tokens)\n",
    "        rejectProb = torch.zeros((nTokens,))\n",
    "        for i in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            freq = 1.0 * self._tokenfreq[w]\n",
    "            # Reweigh\n",
    "            rejectProb[i] = max(torch.tensor(0.), 1 - torch.sqrt(torch.tensor(threshold / freq)))\n",
    "\n",
    "        self._rejectProb = rejectProb\n",
    "        \n",
    "dataset_obj = MyStanfordSentiment()\n",
    "dataset_obj._sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordSentiment:\n",
    "    def __init__(self, path=None, tablesize = 1000000):\n",
    "        if not path:\n",
    "            path = \"../04-Prueba-standarization/StanfordDataset/datasets/stanfordSentimentTreebank\"\n",
    "\n",
    "        self.path = path\n",
    "        self.tablesize = tablesize\n",
    "\n",
    "    def tokens(self):\n",
    "        if hasattr(self, \"_tokens\") and self._tokens:\n",
    "            return self._tokens\n",
    "\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        wordcount = 0\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "\n",
    "        for sentence in self.sentences():\n",
    "            for w in sentence:\n",
    "                wordcount += 1\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "\n",
    "        tokens[\"UNK\"] = idx\n",
    "        revtokens += [\"UNK\"]\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "        wordcount += 1\n",
    "\n",
    "        self._tokens = tokens\n",
    "        self._tokenfreq = tokenfreq\n",
    "        self._wordcount = wordcount\n",
    "        self._revtokens = revtokens\n",
    "        return self._tokens\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"_sentences\") and self._sentences:\n",
    "            return self._sentences\n",
    "\n",
    "        sentences = []\n",
    "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split()[1:]\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower() for w in splitted]]\n",
    "\n",
    "        self._sentences = sentences\n",
    "        self._sentlengths = np.array([len(s) for s in sentences])\n",
    "        self._cumsentlen = np.cumsum(self._sentlengths)\n",
    "\n",
    "        return self._sentences\n",
    "\n",
    "    def numSentences(self):\n",
    "        if hasattr(self, \"_numSentences\") and self._numSentences:\n",
    "            return self._numSentences\n",
    "        else:\n",
    "            self._numSentences = len(self.sentences())\n",
    "            return self._numSentences\n",
    "\n",
    "    def allSentences(self):\n",
    "        if hasattr(self, \"_allsentences\") and self._allsentences:\n",
    "            return self._allsentences\n",
    "\n",
    "        sentences = self.sentences()\n",
    "        rejectProb = self.rejectProb()\n",
    "        tokens = self.tokens()\n",
    "        allsentences = [[w for w in s\n",
    "            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n",
    "            for s in sentences * 30]\n",
    "\n",
    "        allsentences = [s for s in allsentences if len(s) > 1]\n",
    "\n",
    "        self._allsentences = allsentences\n",
    "\n",
    "        return self._allsentences\n",
    "\n",
    "    def getRandomContext(self, C=5):\n",
    "        allsent = self.allSentences()\n",
    "        sentID = random.randint(0, len(allsent) - 1)\n",
    "        sent = allsent[sentID]\n",
    "        wordID = random.randint(0, len(sent) - 1)\n",
    "\n",
    "        context = sent[max(0, wordID - C):wordID]\n",
    "        if wordID+1 < len(sent):\n",
    "            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n",
    "\n",
    "        centerword = sent[wordID]\n",
    "        context = [w for w in context if w != centerword]\n",
    "\n",
    "        if len(context) > 0:\n",
    "            return centerword, context\n",
    "        else:\n",
    "            return self.getRandomContext(C)\n",
    "\n",
    "    def sent_labels(self):\n",
    "        if hasattr(self, \"_sent_labels\") and self._sent_labels:\n",
    "            return self._sent_labels\n",
    "\n",
    "        dictionary = dict()\n",
    "        phrases = 0\n",
    "        with open(self.path + \"/dictionary.txt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "                phrases += 1\n",
    "\n",
    "        labels = [0.0] * phrases\n",
    "        with open(self.path + \"/sentiment_labels.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "        sent_labels = [0.0] * self.numSentences()\n",
    "        sentences = self.sentences()\n",
    "        for i in range(self.numSentences()):\n",
    "            sentence = sentences[i]\n",
    "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-', ')')\n",
    "            sent_labels[i] = labels[dictionary[full_sent]]\n",
    "\n",
    "        self._sent_labels = sent_labels\n",
    "        return self._sent_labels\n",
    "\n",
    "    def dataset_split(self):\n",
    "        if hasattr(self, \"_split\") and self._split:\n",
    "            return self._split\n",
    "\n",
    "        split = [[] for i in range(3)]\n",
    "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split(\",\")\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "\n",
    "        self._split = split\n",
    "        return self._split\n",
    "\n",
    "    def getRandomTrainSentence(self):\n",
    "        split = self.dataset_split()\n",
    "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
    "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
    "\n",
    "    def categorify(self, label):\n",
    "        if label <= 0.2:\n",
    "            return 0\n",
    "        elif label <= 0.4:\n",
    "            return 1\n",
    "        elif label <= 0.6:\n",
    "            return 2\n",
    "        elif label <= 0.8:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    def getDevSentences(self):\n",
    "        return self.getSplitSentences(2)\n",
    "\n",
    "    def getTestSentences(self):\n",
    "        return self.getSplitSentences(1)\n",
    "\n",
    "    def getTrainSentences(self):\n",
    "        return self.getSplitSentences(0)\n",
    "\n",
    "    def getSplitSentences(self, split=0):\n",
    "        ds_split = self.dataset_split()\n",
    "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
    "\n",
    "    def sampleTable(self):\n",
    "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
    "            return self._sampleTable\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        samplingFreq = np.zeros((nTokens,))\n",
    "        self.allSentences()\n",
    "        i = 0\n",
    "        for w in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            if w in self._tokenfreq:\n",
    "                freq = 1.0 * self._tokenfreq[w]\n",
    "                # Reweigh\n",
    "                freq = freq ** 0.75\n",
    "            else:\n",
    "                freq = 0.0\n",
    "            samplingFreq[i] = freq\n",
    "            i += 1\n",
    "\n",
    "        samplingFreq /= np.sum(samplingFreq)\n",
    "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n",
    "\n",
    "        self._sampleTable = [0] * self.tablesize\n",
    "\n",
    "        j = 0\n",
    "        for i in range(self.tablesize):\n",
    "            while i > samplingFreq[j]:\n",
    "                j += 1\n",
    "            self._sampleTable[i] = j\n",
    "\n",
    "        return self._sampleTable\n",
    "\n",
    "    def rejectProb(self):\n",
    "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
    "            return self._rejectProb\n",
    "\n",
    "        threshold = 1e-5 * self._wordcount\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        rejectProb = np.zeros((nTokens,))\n",
    "        for i in range(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            freq = 1.0 * self._tokenfreq[w]\n",
    "            # Reweigh\n",
    "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
    "\n",
    "        self._rejectProb = rejectProb\n",
    "        return self._rejectProb\n",
    "\n",
    "    def sampleTokenIdx(self):\n",
    "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
