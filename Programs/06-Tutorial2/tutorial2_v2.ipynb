{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing en Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Implementación de Skip-Gram Word2Vec\n",
    "\n",
    "El objetivo es calcular la probabilidad de $P(\\mathbf{O}|\\mathbf{C})$ (probabilidad de las palabras de alrededor dada la palabra centra).\n",
    "\n",
    "Podemos aproximar esta probabilidad con un modelo de Softmax:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{O}=o|\\mathbf{C}=c) = \\frac{\\exp(\\Theta x_c)}{\\sum_{w\\in Vocab}\\exp(\\theta_w^T x_c)}\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\Theta = \n",
    "\\begin{bmatrix}\n",
    "-\\;\\theta_1^T \\;- \\\\\n",
    "-\\; \\theta_2^T \\;- \\\\\n",
    "\\vdots \\\\\n",
    "-\\; \\theta_{|V|}^T \\; -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "y $x_c$ es un *one-hot* vector en la palabra $c$ del vocabulario $V$.\n",
    "Si se define\n",
    "\n",
    "$$\n",
    "\\Theta = U V\n",
    "$$\n",
    "\n",
    "con \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U = \n",
    "\\begin{bmatrix}\n",
    "- \\; u_1^T \\; - \\\\\n",
    "- \\; u_2^T \\; - \\\\\n",
    "\\vdots \\\\\n",
    "- \\; u_{|V|}^T \\; -\n",
    "\\end{bmatrix} & &\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "v_1 & v_2 & \\cdots & v_{|V|} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix} & & \n",
    "u_i , v_i \\in \\mathbb{R}^{n} \\; i= 1, \\ldots, |V|\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "puede verse que la expresión de la probabilidad condicional anterior queda\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathbf{O}=o|\\mathbf{C}=c) &= \\frac{\\exp(\\Theta c)}{\\sum_{w\\in V}\\exp(\\theta_w^T c)} \\\\[.5em]\n",
    "&= \\frac{\\exp(u_o^T v_c)}{\\sum_{w\\in V}\\exp(u_w^T v_c)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $v_c$ es la columna de $V$ correspondiente a la plabra $c$ del vocabulario y $u_o^T$ es la fila $o$ de $U$ correspondiente a la palabra $o$ del vocabulario.\n",
    "\n",
    "El siguiente paso es encontrar los parámetros de la distribución anterior, lo cual se hará minimizando la función de costo *Negative LogLikelihood* por gradiente estocástico. Para eso, se puede utilizar las relaciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial U} = \\frac{\\partial J}{\\partial \\Theta} V^T & \\hspace{3em} &\n",
    "\\frac{\\partial J}{\\partial V} = U^T \\frac{\\partial J}{\\partial \\Theta} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "con $J(U,V) = NLL(P(o|c))$, o hacer el cálculo a mano a partir de la expresión completa:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(U,V) &= - \\log \\left( \\prod_{i=1}^N \\prod_{j=1}^{|V|} P(o_i = j | x_i)^{\\mathbb{1}_{\\{y_i = j\\} }} \\right) \\\\\n",
    "&= \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('../AG_NEWS'):\n",
    "    os.mkdir('../AG_NEWS')\n",
    "\n",
    "\n",
    "class AgNewsClassification(torch.utils.data.Dataset):\n",
    "    \n",
    "    unk_token = 'UNK_TOKEN'\n",
    "    pad_token = 'PAD_TOKEN'\n",
    "    \n",
    "    def __init__(self, root_dir='./AG_NEWS', n_grams=2, train=True):\n",
    "        \n",
    "        super(AgNewsClassification, self).__init__()\n",
    "        train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "            root=root_dir, ngrams=n_grams, vocab=None)\n",
    "        \n",
    "        if train:\n",
    "            self.samples = train_dataset._data\n",
    "            self.vocabulary = list(dict(train_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(train_dataset._vocab.freqs)\n",
    "        else:\n",
    "            self.samples = test_dataset._data\n",
    "            self.vocabulary = list(dict(test_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(test_dataset._vocab.freqs)\n",
    "            \n",
    "        self.vocabulary.insert(0,self.pad_token)\n",
    "        self.vocabulary.insert(1,self.unk_token)\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.size_of_longest_sentence = max([len(sample[1]) for sample in self.samples])\n",
    "        self.categories = ['World', 'Sports', 'Business', 'Sci/Tec']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, text = self.samples[idx]\n",
    "        text = torch.nn.functional.pad(text, \n",
    "                                       pad=(0,self.size_of_longest_sentence - len(text)),\n",
    "                                       mode='constant', \n",
    "                                       value=self.word_to_index[self.pad_token])\n",
    "        return text, label\n",
    "    \n",
    "class AgNewsSkipGram(torch.utils.data.Dataset):\n",
    "    \n",
    "    unk_token = 'UNK_TOKEN'\n",
    "    pad_token = 'PAD_TOKEN'\n",
    "    \n",
    "    def __init__(self, root_dir='./AG_NEWS', n_grams=2, train=True, context_size=2):\n",
    "        \n",
    "        super(AgNewsSkipGram, self).__init__()\n",
    "        train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "            root=root_dir, ngrams=n_grams, vocab=None)\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        \n",
    "        if train:\n",
    "            dataset = train_dataset\n",
    "            n_sentences = len(train_dataset._data)\n",
    "            self.vocabulary = list(dict(train_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(train_dataset._vocab.freqs)\n",
    "        else:\n",
    "            dataset = test_dataset\n",
    "            n_sentences = len(test_dataset._data)\n",
    "            self.vocabulary = list(dict(test_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(test_dataset._vocab.freqs)\n",
    "            \n",
    "        \n",
    "        self.vocabulary.insert(0,self.pad_token)\n",
    "        self.vocabulary.insert(1,self.unk_token)\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        \n",
    "        self.samples = self.getSamples([dataset[i][1] for i in range(n_sentences)])\n",
    "    \n",
    "    def getSamples(self, corpus):\n",
    "        samples = []\n",
    "        for sentence_idx in corpus:\n",
    "            sentence = [self.index_to_word[int(j)] for j in sentence_idx]\n",
    "            for i, word in enumerate(sentence):\n",
    "                first_context_word_index = max(0,i-self.context_size)\n",
    "                last_context_word_index = min(i+self.context_size+1, len(sentence))\n",
    "                context = [self.pad_token for j in range(i-self.context_size,first_context_word_index)] + \\\n",
    "                          sentence[first_context_word_index:i] + \\\n",
    "                          sentence[i+1:last_context_word_index] + \\\n",
    "                          [self.pad_token for j in range(last_context_word_index,i+self.context_size+1)]\n",
    "                samples.append((word, context))\n",
    "                \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word, context = self.samples[idx]\n",
    "        idx_context = torch.empty(len(context), dtype=torch.long)\n",
    "        idx_word = torch.tensor(self.word_to_index[word], dtype=torch.long)\n",
    "        for i, w in enumerate(context):\n",
    "            idx_context[i] = self.word_to_index[w]\n",
    "\n",
    "        return idx_word, idx_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AgNewsSkipGram(root_dir='../AG_NEWS', n_grams=2, train=True)\n",
    "val_dataset = AgNewsSkipGram(root_dir='../AG_NEWS', n_grams=2, train=True)\n",
    "test_dataset = AgNewsSkipGram(root_dir='../AG_NEWS', n_grams=2, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # Tamaño del batch\n",
    "val_size = .02\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "sampler = lambda start, end: torch.utils.data.SubsetRandomSampler(range(start, end)) # Función para mezclar aleatoriamente las muestras\n",
    "\n",
    "\n",
    "# Dataloader para las muestras de entrenamiento:\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               sampler=sampler(0, NUM_TRAIN))\n",
    "\n",
    "# Dataloader para las muestras de validación:\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             sampler=sampler(NUM_TRAIN, NUM_TRAIN+NUM_VAL))\n",
    "\n",
    "# Dataloader para las muestras de testeo:\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vectors, embedding_dim):\n",
    "        \n",
    "        super(SkipGram, self).__init__()\n",
    "        self.emb = nn.Embedding(n_vectors, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, n_vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        scores = self.linear(emb)\n",
    "        return scores\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        mean_loss = 0 \n",
    "        context_size = target.size()[1]\n",
    "        for i in range(context_size):\n",
    "            mean_loss += cross_entropy(scores, target[:,i])\n",
    "        return mean_loss / context_size\n",
    "    \n",
    "embedding_dim = 50\n",
    "tokens = train_dataset.word_to_index\n",
    "nWords = len(train_dataset.vocabulary)\n",
    "model = SkipGram(nWords, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  \n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(dim=1)\n",
    "            num_correct += torch.tensor([(preds == y[:,i]).sum() for i in range(y.size()[1])]).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainModel(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_size = len(train_dataloader)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x) \n",
    "            \n",
    "            # Backward pass\n",
    "            loss = model.loss(scores,y)                 \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (e * batch_size + t) % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Accuracy: 2/205345 \n",
      "Epoch: 0, Iteration: 100, Accuracy: 4/205345 \n",
      "Epoch: 0, Iteration: 200, Accuracy: 1468/205345 \n",
      "Epoch: 0, Iteration: 300, Accuracy: 2344/205345 \n",
      "Epoch: 0, Iteration: 400, Accuracy: 3720/205345 \n",
      "Epoch: 0, Iteration: 500, Accuracy: 6056/205345 \n",
      "Epoch: 0, Iteration: 600, Accuracy: 7760/205345 \n",
      "Epoch: 0, Iteration: 700, Accuracy: 11290/205345 \n",
      "Epoch: 0, Iteration: 800, Accuracy: 12799/205345 \n",
      "Epoch: 0, Iteration: 900, Accuracy: 14259/205345 \n",
      "Epoch: 0, Iteration: 1000, Accuracy: 15723/205345 \n",
      "Epoch: 0, Iteration: 1100, Accuracy: 17766/205345 \n",
      "Epoch: 0, Iteration: 1200, Accuracy: 18487/205345 \n",
      "Epoch: 0, Iteration: 1300, Accuracy: 19529/205345 \n",
      "Epoch: 0, Iteration: 1400, Accuracy: 20004/205345 \n",
      "Epoch: 0, Iteration: 1500, Accuracy: 20613/205345 \n",
      "Epoch: 0, Iteration: 1600, Accuracy: 21164/205345 \n",
      "Epoch: 0, Iteration: 1700, Accuracy: 21241/205345 \n",
      "Epoch: 0, Iteration: 1800, Accuracy: 21518/205345 \n",
      "Epoch: 0, Iteration: 1900, Accuracy: 22116/205345 \n",
      "Epoch: 0, Iteration: 2000, Accuracy: 22404/205345 \n",
      "Epoch: 0, Iteration: 2100, Accuracy: 22460/205345 \n",
      "Epoch: 0, Iteration: 2200, Accuracy: 22759/205345 \n",
      "Epoch: 0, Iteration: 2300, Accuracy: 22609/205345 \n",
      "Epoch: 0, Iteration: 2400, Accuracy: 22814/205345 \n",
      "Epoch: 0, Iteration: 2500, Accuracy: 22663/205345 \n",
      "Epoch: 0, Iteration: 2600, Accuracy: 22700/205345 \n",
      "Epoch: 0, Iteration: 2700, Accuracy: 22697/205345 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b432b27212a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Entrenamiento:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mperformance_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_loss_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-d5249d3b07d1>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, data, epochs, learning_rate, sample_loss_every)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_loss_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mnum_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mperformance_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mperformance_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-d5249d3b07d1>\u001b[0m in \u001b[0;36mCheckAccuracy\u001b[0;34m(loader, model, device, input_dtype, target_dtype)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mnum_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long,\n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 100 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-1 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainModel(model, data, epochs, learning_rate, sample_loss_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 26231.72lines/s]\n",
      "120000lines [00:08, 14630.60lines/s]\n",
      "7600lines [00:00, 14621.57lines/s]\n",
      "120000lines [00:04, 26211.73lines/s]\n",
      "120000lines [00:08, 14918.09lines/s]\n",
      "7600lines [00:00, 14787.54lines/s]\n",
      "120000lines [00:04, 26580.81lines/s]\n",
      "120000lines [00:08, 14889.02lines/s]\n",
      "7600lines [00:00, 14977.02lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AgNewsClassification(root_dir='../AG_NEWS', n_grams=2, train=True)\n",
    "val_dataset = AgNewsClassification(root_dir='../AG_NEWS', n_grams=2, train=True)\n",
    "test_dataset = AgNewsClassification(root_dir='../AG_NEWS', n_grams=2, train=False)\n",
    "\n",
    "batch_size = 64 # Tamaño del batch\n",
    "val_size = .02\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "sampler = lambda start, end: torch.utils.data.SubsetRandomSampler(range(start, end)) # Función para mezclar aleatoriamente las muestras\n",
    "\n",
    "\n",
    "# Dataloader para las muestras de entrenamiento:\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               sampler=sampler(0, NUM_TRAIN))\n",
    "\n",
    "# Dataloader para las muestras de validación:\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             sampler=sampler(NUM_TRAIN, NUM_TRAIN+NUM_VAL))\n",
    "\n",
    "# Dataloader para las muestras de testeo:\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, embedding_dim, embedding_layer):\n",
    "        \n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        self.emb = embedding_layer\n",
    "        self.linear = nn.Linear(embedding_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        scores = self.linear(emb)\n",
    "        return scores\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        return cross_entropy(scores, target)\n",
    "    \n",
    "embedding_dim = 50\n",
    "tokens = train_dataset.word_to_index\n",
    "nWords = len(train_dataset.vocabulary)\n",
    "Classifier = SoftmaxClassifier(len(train_dataset.categories), embedding_dim, model.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  \n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(dim=1)\n",
    "            num_correct += (preds == y).sum() \n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainModel(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_size = len(train_dataloader)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x) \n",
    "            \n",
    "            # Backward pass\n",
    "            loss = model.loss(scores,y)                 \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (e * batch_size + t) % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 128.88 GiB (GPU 0; 10.91 GiB total capacity; 1.62 GiB already allocated; 7.70 GiB free; 288.17 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-b432b27212a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Entrenamiento:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mperformance_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_loss_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-893f9e0bb84f>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, data, epochs, learning_rate, sample_loss_every)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-ae715d80a960>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 128.88 GiB (GPU 0; 10.91 GiB total capacity; 1.62 GiB already allocated; 7.70 GiB free; 288.17 MiB cached)"
     ]
    }
   ],
   "source": [
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long,\n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 100 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-1 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainModel(model, data, epochs, learning_rate, sample_loss_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
