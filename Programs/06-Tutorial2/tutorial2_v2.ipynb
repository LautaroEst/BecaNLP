{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing en Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Implementación de Skip-Gram Word2Vec\n",
    "\n",
    "### 1. Repaso del Modelo Skip-Gram\n",
    "\n",
    "El objetivo es calcular la probabilidad de $P(\\mathbf{O}|\\mathbf{C})$ (probabilidad de las palabras de alrededor dada la palabra centra).\n",
    "\n",
    "Podemos aproximar esta probabilidad con un modelo de Softmax:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{O}=o|\\mathbf{C}=c) = \\frac{\\exp(\\Theta x_c)}{\\sum_{w\\in Vocab}\\exp(\\theta_w^T x_c)}\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\Theta = \n",
    "\\begin{bmatrix}\n",
    "-\\;\\theta_1^T \\;- \\\\\n",
    "-\\; \\theta_2^T \\;- \\\\\n",
    "\\vdots \\\\\n",
    "-\\; \\theta_{|V|}^T \\; -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "y $x_c$ es un *one-hot* vector en la palabra $c$ del vocabulario $V$.\n",
    "Si se define\n",
    "\n",
    "$$\n",
    "\\Theta = U V\n",
    "$$\n",
    "\n",
    "con \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U = \n",
    "\\begin{bmatrix}\n",
    "- \\; u_1^T \\; - \\\\\n",
    "- \\; u_2^T \\; - \\\\\n",
    "\\vdots \\\\\n",
    "- \\; u_{|V|}^T \\; -\n",
    "\\end{bmatrix} & &\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "v_1 & v_2 & \\cdots & v_{|V|} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix} & & \n",
    "u_i , v_i \\in \\mathbb{R}^{n} \\; i= 1, \\ldots, |V|\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "puede verse que la expresión de la probabilidad condicional anterior queda\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathbf{O}=o|\\mathbf{C}=c) &= \\frac{\\exp(\\Theta c)}{\\sum_{w\\in V}\\exp(\\theta_w^T c)} \\\\[.5em]\n",
    "&= \\frac{\\exp(u_o^T v_c)}{\\sum_{w\\in V}\\exp(u_w^T v_c)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $v_c$ es la columna de $V$ correspondiente a la plabra $c$ del vocabulario y $u_o^T$ es la fila $o$ de $U$ correspondiente a la palabra $o$ del vocabulario.\n",
    "\n",
    "El siguiente paso es encontrar los parámetros de la distribución anterior, lo cual se hará minimizando la función de costo *Negative LogLikelihood* por gradiente estocástico. Para eso, se puede utilizar las relaciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial U} = \\frac{\\partial J}{\\partial \\Theta} V^T & \\hspace{3em} &\n",
    "\\frac{\\partial J}{\\partial V} = U^T \\frac{\\partial J}{\\partial \\Theta} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "con $J(U,V) = NLL(P(o|c))$, o hacer el cálculo a mano a partir de la expresión completa:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(U,V) &= - \\log \\left( \\prod_{i=1}^N \\prod_{j=1}^{|V|} P(o_i = j | x_i)^{\\mathbb{1}_{\\{y_i = j\\} }} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "En nuestro caso, vamos a hacer la implementación del gradiente descendiente con Pytorch, así que en principio no importan mucho estas fórmulas.\n",
    "\n",
    "### 2. Corpus AG NEWS\n",
    "\n",
    "Para los experimentos se va a usar el corpus de texto **AG NEWS**, que consiste en un conjunto de noticias junto con sus respectivas categorías. Cada una de estas noticias puede pertenecer a exactamente una de las siguientes categorías:\n",
    "\n",
    "* Internacional (*World*)\n",
    "* Deportes (*Sports*)\n",
    "* Negocios (*Business*)\n",
    "* Ciencia y Tecnología (*Sci/Tec*)\n",
    "\n",
    "En el Tutorial 1 se explicó cómo entrenar un clasificador Softmax sin utilizar word embeddings (o entrenándolos junto con el modelo). El objetivo de este tutorial va a consistir en dos partes:\n",
    "\n",
    "1. Entrenar los word embeddings de las palabras del vocabulario del corpus con el modelo Skip-Gram\n",
    "2. Clasificar una cantidad de frases de las noticias según su respectiva categoría con un clasificador lineal con activación Softmax. \n",
    "\n",
    "Primero, definimos el dataset con las muestras de clasificación y el dataset para entrenar el modelo Skip-Gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('../AG_NEWS'):\n",
    "    os.mkdir('../AG_NEWS')\n",
    "\n",
    "\n",
    "class AgNewsClassification(torch.utils.data.Dataset):\n",
    "    \n",
    "    unk_token = 'UNK_TOKEN'\n",
    "    pad_token = 'PAD_TOKEN'\n",
    "    \n",
    "    def __init__(self, root_dir='./AG_NEWS', n_grams=2, train=True):\n",
    "        \n",
    "        super(AgNewsClassification, self).__init__()\n",
    "        train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "            root=root_dir, ngrams=n_grams, vocab=None)\n",
    "        \n",
    "        if train:\n",
    "            self.samples = train_dataset._data\n",
    "            self.vocabulary = list(dict(train_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(train_dataset._vocab.freqs)\n",
    "        else:\n",
    "            self.samples = test_dataset._data\n",
    "            self.vocabulary = list(dict(test_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(test_dataset._vocab.freqs)\n",
    "            \n",
    "        self.vocabulary.insert(0,self.pad_token)\n",
    "        self.vocabulary.insert(1,self.unk_token)\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.size_of_longest_sentence = max([len(sample[1]) for sample in self.samples])\n",
    "        self.categories = ['World', 'Sports', 'Business', 'Sci/Tec']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, text = self.samples[idx]\n",
    "        text = torch.nn.functional.pad(text, \n",
    "                                       pad=(0,self.size_of_longest_sentence - len(text)),\n",
    "                                       mode='constant', \n",
    "                                       value=self.word_to_index[self.pad_token])\n",
    "        return text, label\n",
    "    \n",
    "class AgNewsSkipGram(torch.utils.data.Dataset):\n",
    "    \n",
    "    unk_token = 'UNK_TOKEN'\n",
    "    pad_token = 'PAD_TOKEN'\n",
    "    \n",
    "    def __init__(self, root_dir='./AG_NEWS', n_grams=2, train=True, context_size=2):\n",
    "        \n",
    "        super(AgNewsSkipGram, self).__init__()\n",
    "        train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "            root=root_dir, ngrams=n_grams, vocab=None)\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        \n",
    "        if train:\n",
    "            dataset = train_dataset\n",
    "            n_sentences = len(train_dataset._data)\n",
    "            self.vocabulary = list(dict(train_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(train_dataset._vocab.freqs)\n",
    "        else:\n",
    "            dataset = test_dataset\n",
    "            n_sentences = len(test_dataset._data)\n",
    "            self.vocabulary = list(dict(test_dataset._vocab.freqs).keys())\n",
    "            self.freqs = dict(test_dataset._vocab.freqs)\n",
    "            \n",
    "        \n",
    "        self.vocabulary.insert(0,self.pad_token)\n",
    "        self.vocabulary.insert(1,self.unk_token)\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        \n",
    "        self.samples = self.getSamples([dataset[i][1] for i in range(n_sentences)])\n",
    "    \n",
    "    def getSamples(self, corpus):\n",
    "        samples = []\n",
    "        for sentence_idx in corpus:\n",
    "            sentence = [self.index_to_word[int(j)] for j in sentence_idx]\n",
    "            for i, word in enumerate(sentence):\n",
    "                first_context_word_index = max(0,i-self.context_size)\n",
    "                last_context_word_index = min(i+self.context_size+1, len(sentence))\n",
    "                context = [self.pad_token for j in range(i-self.context_size,first_context_word_index)] + \\\n",
    "                          sentence[first_context_word_index:i] + \\\n",
    "                          sentence[i+1:last_context_word_index] + \\\n",
    "                          [self.pad_token for j in range(last_context_word_index,i+self.context_size+1)]\n",
    "                samples.append((word, context))\n",
    "                \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word, context = self.samples[idx]\n",
    "        idx_context = torch.empty(len(context), dtype=torch.long)\n",
    "        idx_word = torch.tensor(self.word_to_index[word], dtype=torch.long)\n",
    "        for i, w in enumerate(context):\n",
    "            idx_context[i] = self.word_to_index[w]\n",
    "\n",
    "        return idx_word, idx_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets con las muestras para entrenar el modelo Skip-Gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 25902.36lines/s]\n",
      "120000lines [00:08, 14242.72lines/s]\n",
      "7600lines [00:00, 14548.79lines/s]\n",
      "120000lines [00:04, 26238.38lines/s]\n",
      "120000lines [00:08, 14335.69lines/s]\n",
      "7600lines [00:00, 14314.11lines/s]\n",
      "120000lines [00:04, 26311.80lines/s]\n",
      "120000lines [00:08, 14496.14lines/s]\n",
      "7600lines [00:00, 14458.83lines/s]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "skg_train_dataset = AgNewsSkipGram(root_dir='../AG_NEWS', n_grams=2, train=True, context_size=context_size)\n",
    "skg_val_dataset = AgNewsSkipGram(root_dir='../AG_NEWS', n_grams=2, train=True, context_size=context_size)\n",
    "skg_test_dataset = AgNewsSkipGram(root_dir='../AG_NEWS', n_grams=2, train=False, context_size=context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # Tamaño del batch\n",
    "val_size = .02\n",
    "NUM_TRAIN = int((1 - val_size) * len(skg_train_dataset))\n",
    "NUM_VAL = len(skg_train_dataset) - NUM_TRAIN\n",
    "sampler = lambda start, end: torch.utils.data.SubsetRandomSampler(range(start, end)) # Función para mezclar aleatoriamente las muestras\n",
    "\n",
    "\n",
    "# Dataloader para las muestras de entrenamiento:\n",
    "train_dataloader = torch.utils.data.DataLoader(skg_train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               sampler=sampler(0, NUM_TRAIN))\n",
    "\n",
    "# Dataloader para las muestras de validación:\n",
    "val_dataloader = torch.utils.data.DataLoader(skg_val_dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             sampler=sampler(NUM_TRAIN, NUM_TRAIN+NUM_VAL))\n",
    "\n",
    "# Dataloader para las muestras de testeo:\n",
    "test_dataloader = torch.utils.data.DataLoader(skg_test_dataset, \n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definición del modelo\n",
    "\n",
    "Arquitectura del modelo:\n",
    "\n",
    "1. Un Layer `nn.Embedding` que busca en una tabla los word embeddings de los índices ingresados a la entrada. \n",
    "2. Un Layer `nn.Linear` que devuelve los scores de cada categoría.\n",
    "3. Una activación softmax para calcular las probabilidades de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vectors, embedding_dim):\n",
    "        \n",
    "        super(SkipGram, self).__init__()\n",
    "        self.emb = nn.Embedding(n_vectors, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, n_vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        scores = self.linear(emb)\n",
    "        return scores\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        mean_loss = 0 \n",
    "        context_size = target.size()[1]\n",
    "        for i in range(context_size):\n",
    "            mean_loss += cross_entropy(scores, target[:,i])\n",
    "        return mean_loss / context_size\n",
    "    \n",
    "embedding_dim = 10\n",
    "tokens = skg_train_dataset.word_to_index\n",
    "nWords = len(skg_train_dataset.vocabulary)\n",
    "skg_model = SkipGram(nWords, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento de los word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Accuracy: 1/205345 \n",
      "Epoch: 0, Iteration: 10, Accuracy: 1/205345 \n",
      "Epoch: 0, Iteration: 20, Accuracy: 1/205345 \n",
      "Epoch: 0, Iteration: 30, Accuracy: 24/205345 \n",
      "Epoch: 0, Iteration: 40, Accuracy: 121/205345 \n",
      "Epoch: 0, Iteration: 50, Accuracy: 406/205345 \n",
      "Epoch: 0, Iteration: 60, Accuracy: 1171/205345 \n",
      "Epoch: 0, Iteration: 70, Accuracy: 2224/205345 \n",
      "Epoch: 0, Iteration: 80, Accuracy: 4471/205345 \n",
      "Epoch: 0, Iteration: 90, Accuracy: 7521/205345 \n",
      "Epoch: 0, Iteration: 100, Accuracy: 12940/205345 \n",
      "Epoch: 0, Iteration: 110, Accuracy: 17145/205345 \n",
      "Epoch: 0, Iteration: 120, Accuracy: 21386/205345 \n",
      "Epoch: 0, Iteration: 130, Accuracy: 26719/205345 \n",
      "Epoch: 0, Iteration: 140, Accuracy: 30753/205345 \n",
      "Epoch: 0, Iteration: 150, Accuracy: 33363/205345 \n",
      "Epoch: 0, Iteration: 160, Accuracy: 36058/205345 \n",
      "Epoch: 0, Iteration: 170, Accuracy: 37760/205345 \n",
      "Epoch: 0, Iteration: 180, Accuracy: 40012/205345 \n",
      "Epoch: 0, Iteration: 190, Accuracy: 40764/205345 \n",
      "Epoch: 0, Iteration: 200, Accuracy: 42204/205345 \n",
      "Epoch: 0, Iteration: 210, Accuracy: 43298/205345 \n",
      "Epoch: 0, Iteration: 220, Accuracy: 43573/205345 \n",
      "Epoch: 0, Iteration: 230, Accuracy: 44174/205345 \n",
      "Epoch: 0, Iteration: 240, Accuracy: 44353/205345 \n",
      "Epoch: 0, Iteration: 250, Accuracy: 44811/205345 \n",
      "Epoch: 0, Iteration: 260, Accuracy: 45238/205345 \n",
      "Epoch: 0, Iteration: 270, Accuracy: 46216/205345 \n",
      "Epoch: 0, Iteration: 280, Accuracy: 46461/205345 \n",
      "Epoch: 0, Iteration: 290, Accuracy: 46644/205345 \n",
      "Epoch: 0, Iteration: 300, Accuracy: 46966/205345 \n",
      "Epoch: 0, Iteration: 310, Accuracy: 47524/205345 \n",
      "Epoch: 0, Iteration: 320, Accuracy: 47960/205345 \n",
      "Epoch: 0, Iteration: 330, Accuracy: 47820/205345 \n",
      "Epoch: 0, Iteration: 340, Accuracy: 47955/205345 \n",
      "Epoch: 0, Iteration: 350, Accuracy: 47896/205345 \n",
      "Epoch: 0, Iteration: 360, Accuracy: 47345/205345 \n",
      "Epoch: 0, Iteration: 370, Accuracy: 47621/205345 \n",
      "Epoch: 0, Iteration: 380, Accuracy: 47786/205345 \n",
      "Epoch: 0, Iteration: 390, Accuracy: 47897/205345 \n",
      "Epoch: 0, Iteration: 400, Accuracy: 47804/205345 \n",
      "Epoch: 0, Iteration: 410, Accuracy: 47893/205345 \n",
      "Epoch: 0, Iteration: 420, Accuracy: 47922/205345 \n",
      "Epoch: 0, Iteration: 430, Accuracy: 47935/205345 \n",
      "Epoch: 0, Iteration: 440, Accuracy: 47950/205345 \n",
      "Epoch: 0, Iteration: 450, Accuracy: 47836/205345 \n",
      "Epoch: 0, Iteration: 460, Accuracy: 47956/205345 \n",
      "Epoch: 0, Iteration: 470, Accuracy: 48331/205345 \n",
      "Epoch: 0, Iteration: 480, Accuracy: 48281/205345 \n",
      "Epoch: 0, Iteration: 490, Accuracy: 48299/205345 \n",
      "Epoch: 0, Iteration: 500, Accuracy: 48626/205345 \n",
      "Epoch: 0, Iteration: 510, Accuracy: 48795/205345 \n",
      "Epoch: 0, Iteration: 520, Accuracy: 48870/205345 \n",
      "Epoch: 0, Iteration: 530, Accuracy: 48794/205345 \n",
      "Epoch: 0, Iteration: 540, Accuracy: 48381/205345 \n",
      "Epoch: 0, Iteration: 550, Accuracy: 48628/205345 \n",
      "Epoch: 0, Iteration: 560, Accuracy: 48848/205345 \n",
      "Epoch: 0, Iteration: 570, Accuracy: 48506/205345 \n",
      "Epoch: 0, Iteration: 580, Accuracy: 48597/205345 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ed8dacdb8885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Entrenamiento:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mperformance_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_loss_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-ed8dacdb8885>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, data, epochs, learning_rate, sample_loss_every)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_loss_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mnum_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mperformance_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mperformance_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-ed8dacdb8885>\u001b[0m in \u001b[0;36mCheckAccuracy\u001b[0;34m(loader, model, device, input_dtype, target_dtype)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14330b8372e5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0midx_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0midx_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  \n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(dim=1)\n",
    "            num_correct += torch.tensor([(preds == y[:,i]).sum() for i in range(y.size()[1])]).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainModel(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_size = len(train_dataloader)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x) \n",
    "            \n",
    "            # Backward pass\n",
    "            loss = model.loss(scores,y)                 \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (e * batch_size + t) % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history\n",
    "\n",
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long,\n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 10 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 5e-1 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainModel(skg_model, data, epochs, learning_rate, sample_loss_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrenamiento del modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 26250.52lines/s]\n",
      "120000lines [00:08, 14708.33lines/s]\n",
      "7600lines [00:00, 14600.29lines/s]\n",
      "120000lines [00:04, 26221.32lines/s]\n",
      "120000lines [00:08, 14453.33lines/s]\n",
      "7600lines [00:00, 14422.32lines/s]\n",
      "120000lines [00:04, 26269.79lines/s]\n",
      "120000lines [00:08, 14503.17lines/s]\n",
      "7600lines [00:00, 14342.76lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AgNewsClassification(root_dir='../AG_NEWS', n_grams=2, train=True)\n",
    "val_dataset = AgNewsClassification(root_dir='../AG_NEWS', n_grams=2, train=True)\n",
    "test_dataset = AgNewsClassification(root_dir='../AG_NEWS', n_grams=2, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024 # Tamaño del batch\n",
    "val_size = .02\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "sampler = lambda start, end: torch.utils.data.SubsetRandomSampler(range(start, end)) # Función para mezclar aleatoriamente las muestras\n",
    "\n",
    "\n",
    "# Dataloader para las muestras de entrenamiento:\n",
    "model_train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               sampler=sampler(0, NUM_TRAIN))\n",
    "\n",
    "# Dataloader para las muestras de validación:\n",
    "model_val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             sampler=sampler(NUM_TRAIN, NUM_TRAIN+NUM_VAL))\n",
    "\n",
    "# Dataloader para las muestras de testeo:\n",
    "model_test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_classes, embedding_dim, pre_trained_embeddings):\n",
    "        \n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        if pre_trained_embeddings is not None:\n",
    "            self.emb = nn.Embedding(pre_trained_embeddings.size(0), pre_trained_embeddings.size(1))\n",
    "            self.emb.weight = nn.Parameter(pre_trained_embeddings)\n",
    "            self.emb.weight.requires_grad = False\n",
    "        else:\n",
    "            self.emb = nn.Embedding(n_vocab, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).mean(dim=1)\n",
    "        scores = self.linear(emb)\n",
    "        return scores\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        return cross_entropy(scores, target)\n",
    "    \n",
    "embedding_dim = 10\n",
    "tokens = train_dataset.word_to_index\n",
    "nWords = len(train_dataset.vocabulary)\n",
    "Classifier = SoftmaxClassifier(nWords, len(train_dataset.categories), embedding_dim, skg_model.emb.weight)\n",
    "#Classifier = SoftmaxClassifier(nWords, len(train_dataset.categories), embedding_dim, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Accuracy: 665/2400 \n",
      "Epoch: 0, Iteration: 100, Accuracy: 665/2400 \n",
      "Epoch: 1, Iteration: 85, Accuracy: 665/2400 \n",
      "Epoch: 2, Iteration: 70, Accuracy: 665/2400 \n",
      "Epoch: 3, Iteration: 55, Accuracy: 665/2400 \n",
      "Epoch: 4, Iteration: 40, Accuracy: 665/2400 \n",
      "Epoch: 5, Iteration: 25, Accuracy: 665/2400 \n",
      "Epoch: 6, Iteration: 10, Accuracy: 665/2400 \n",
      "Epoch: 6, Iteration: 110, Accuracy: 665/2400 \n",
      "Epoch: 7, Iteration: 95, Accuracy: 665/2400 \n",
      "Epoch: 8, Iteration: 80, Accuracy: 666/2400 \n",
      "Epoch: 9, Iteration: 65, Accuracy: 666/2400 \n",
      "Epoch: 10, Iteration: 50, Accuracy: 665/2400 \n",
      "Epoch: 11, Iteration: 35, Accuracy: 663/2400 \n",
      "Epoch: 12, Iteration: 20, Accuracy: 660/2400 \n",
      "Epoch: 13, Iteration: 5, Accuracy: 659/2400 \n",
      "Epoch: 13, Iteration: 105, Accuracy: 650/2400 \n",
      "Epoch: 14, Iteration: 90, Accuracy: 649/2400 \n",
      "Epoch: 15, Iteration: 75, Accuracy: 652/2400 \n",
      "Epoch: 16, Iteration: 60, Accuracy: 657/2400 \n",
      "Epoch: 17, Iteration: 45, Accuracy: 659/2400 \n",
      "Epoch: 18, Iteration: 30, Accuracy: 664/2400 \n",
      "Epoch: 19, Iteration: 15, Accuracy: 678/2400 \n",
      "Epoch: 20, Iteration: 0, Accuracy: 667/2400 \n",
      "Epoch: 20, Iteration: 100, Accuracy: 655/2400 \n",
      "Epoch: 21, Iteration: 85, Accuracy: 651/2400 \n",
      "Epoch: 22, Iteration: 70, Accuracy: 653/2400 \n",
      "Epoch: 23, Iteration: 55, Accuracy: 653/2400 \n",
      "Epoch: 24, Iteration: 40, Accuracy: 658/2400 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-34dcc02a0a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Entrenamiento:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mperformance_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_loss_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-34dcc02a0a67>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, data, epochs, learning_rate, sample_loss_every)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14330b8372e5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                        \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_of_longest_sentence\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                        \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                        value=self.word_to_index[self.pad_token])\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   2846\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding length too large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  \n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(dim=1)\n",
    "            num_correct += (preds == y).sum() \n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainModel(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_size = len(train_dataloader)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x) \n",
    "            \n",
    "            # Backward pass\n",
    "            loss = model.loss(scores,y)                 \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (e * batch_size + t) % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history\n",
    "\n",
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long,\n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': model_train_dataloader,\n",
    "    'val_dataloader': model_val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 100 # Cantidad de epochs\n",
    "sample_loss_every = 100 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-3 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainModel(Classifier, data, epochs, learning_rate, sample_loss_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
