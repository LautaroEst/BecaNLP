{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import re\n",
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link de ayuda: [Word2Vec](https://rguigoures.github.io/word2vec_pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo.** Nuestro objetivo es utilizar un corpus de texto para entender el significado de los gramas que aparecen en el mismo. Para hacer eso, hay que definir un **modelo de lenguaje**, es decir una serie de reglas que nos van a conducir a entender el texto. \n",
    "\n",
    "El algoritmo *word2vec* define dos modelos de lenguaje distintos, pero basados en el hecho de que el significado de las palabras puede deducirse de su contexto (**hipótesis distribucional**). Esto se hace para entender el texto a través de su segmentación en palabras y, posteriormente, de la asignación de un vector de $\\mathbb{R}^{n}$ a cada palabra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "\n",
    "El primer algoritmo *word2vec* propone un modelo de lenguaje de la siguiente manera:\n",
    "\n",
    "* El corpus de texto con el que se trabaja consiste en una sucesión ordenada de palabras y/o signos de puntuación, tabulación, fin de página, etc., que para generalizar, llamaremos \"gramas\". \n",
    "\n",
    "* Los gramas que aparecen en el texto pertenecen a un vocabulario $V = \\{ w_1, w_2, \\ldots, w_{|V|} \\}$. Este vocabulario contiene a los gramas codificadss con *one-hot* vectors, es decir, \n",
    "\n",
    "$$\n",
    "w_{i_j} = \n",
    "\\begin{cases} \n",
    "  1 & \\mbox{si} & j = i \\\\\n",
    "  0 & \\mbox{si} & j \\neq i \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "* Se definen las variables aleatorias $\\mathbf{X}_{-m}, \\mathbf{X}_{-m+1}, \\ldots, \\mathbf{X}_{-1}, \\mathbf{X}_0, \\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{m}$, todas ellas con realizaciones en $V$, y para cada sucesión contigua de gramas encontrada en el texto, se tiene una probabilidad conjunta \n",
    "\n",
    "$$\n",
    "P(\\mathbf{X}_{-m} = x_{-m}, \\mathbf{X}_{-m+1} = x_{-m+1}, \\ldots, \\mathbf{X}_{-1} = x_{-1}, \\mathbf{X}_0 = x_{0}, \\mathbf{X}_{1} = x_1, \\ldots, \\mathbf{X}_{m-1} =x_{m-1}, \\mathbf{X}_{m} = x_{m})\n",
    "$$\n",
    "\n",
    "* Se desea estimar la probabilidad \n",
    "$$\n",
    "P(\\mathbf{X}_0 = w_i | \\mathbf{X}_{-m} = x_{-m}, \\mathbf{X}_{-m+1} = x_{-m+1}, \\ldots, \\mathbf{X}_{-1} = x_{-1}, \\mathbf{X}_{1} = x_1, \\ldots, \\mathbf{X}_{m-1} =x_{m-1}, \\mathbf{X}_{m} = x_{m})\n",
    "$$\n",
    "para todo $i=1,\\ldots,|V|$ y para cada conjunto posible de $x_{-m},\\ldots,x_{m}$ de vectores pertenecientes a $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "El modelo *Skip-gram* propone algo similar, aunque esta vez se busca estimar\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X}_{-m} = x_{-m}, \\mathbf{X}_{-m+1} = x_{-m+1}, \\ldots, \\mathbf{X}_{-1} = x_{-1}, \\mathbf{X}_{1} = x_1, \\ldots, \\mathbf{X}_{m-1} =x_{m-1}, \\mathbf{X}_{m} = x_{m} | \\mathbf{X}_0 = x_0)\n",
    "$$\n",
    "\n",
    "Si se asume idependencia condicional, esta probabilidad es igual a\n",
    "\n",
    "$$\n",
    "\\prod_{i=-m\\\\i\\neq 0}^{m} P(\\mathbf{X}_{i} = x_i | \\mathbf{X}_0 = x_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulación\n",
    "\n",
    "El corpus de texto *Brown* es un conjunto de archivos de texto divididos por categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/anaconda3/envs/TorchEnv/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown', download_dir='/home/lestien/anaconda3/envs/TorchEnv/nltk_data')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus categories:\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Corpus categories:')\n",
    "print(brown.categories())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un corpus para cada categoría. Un corpus es un conjunto de frases que aparecen en el conjunto de textos en la categoría. Un corpus es una lista de listas de palabras. Por ejemplo, para la categoría \"news\" se tiene el siguiente corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de frases en estas categorías:  4623\n",
      "\n",
      "Algunos ejemplos:\n",
      "\n",
      "Frase 1:\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "Frase 2:\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "\n",
      "Frase 3:\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "\n",
      "Frase 4:\n",
      "['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "\n",
      "Frase 5:\n",
      "['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['news']\n",
    "corpus_unpreproceced = brown.sents(categories=categories)\n",
    "print('Cantidad de frases en estas categorías: ',len(corpus_unpreproceced))\n",
    "print()\n",
    "\n",
    "print('Algunos ejemplos:')\n",
    "print()\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    print('Frase {}:'.format(i+1))\n",
    "    print(corpus_unpreproceced[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando texto...\n",
      "\n",
      "Nuevo texto:\n",
      "\n",
      "Frase 1:\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "Frase 2:\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "\n",
      "Frase 3:\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "\n",
      "Frase 4:\n",
      "['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "\n",
      "Frase 5:\n",
      "['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "print('Procesando texto...')\n",
    "for sentence in corpus_unpreproceced:\n",
    "    text = ' '.join(sentence)\n",
    "    text = text.lower()\n",
    "    text.replace('\\n', ' ')\n",
    "    text = re.sub('[^a-z ]+', '', text)\n",
    "    corpus.append([w for w in text.split() if w != ''])\n",
    "    \n",
    "print()\n",
    "print('Nuevo texto:')\n",
    "print()\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    print('Frase {}:'.format(i+1))\n",
    "    print(corpus_unpreproceced[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el vocabulario para este corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sentence = '<NS>'\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "vocabulary.add(no_sentence)\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las muestras en forma de tupla `(contexto, palabra central)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de muestras disponibles:  87004\n"
     ]
    }
   ],
   "source": [
    "m = 2\n",
    "samples = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        first_context_word_index = max(0,i-m)\n",
    "        last_context_word_index = min(i+m+1, len(sentence))\n",
    "        \n",
    "        \n",
    "        context = [no_sentence for j in range(i-m,first_context_word_index)] + sentence[first_context_word_index:i] \\\n",
    "                + sentence[i+1:last_context_word_index] + [no_sentence for j in range(last_context_word_index,i+m+1)]\n",
    "        \n",
    "        samples.append((context, word))\n",
    "        \n",
    "print('Cantidad de muestras disponibles: ', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/anaconda3/envs/TorchEnv/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/anaconda3/envs/TorchEnv/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de muestas: 1005119\n",
      "\n",
      "context:\n",
      "<NS>\n",
      "<NS>\n",
      "fulton\n",
      "county\n",
      "\n",
      "Palabra central:\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "class BrownDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, categories, root='./', preprocessing=None, context_size=2):\n",
    "        nltk.download('brown', download_dir=root)\n",
    "        from nltk.corpus import brown\n",
    "        self.corpus_unpreproceced = brown.sents(categories=categories)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        if self.preprocessing:\n",
    "            self.corpus = self.preprocessing(self.corpus_unpreproceced)\n",
    "        else:\n",
    "            self.corpus = self.corpus_unpreproceced\n",
    "        \n",
    "        no_sentence = '<NS>'\n",
    "        self.vocabulary = set(itertools.chain.from_iterable(self.corpus))\n",
    "        self.vocabulary.add(no_sentence)\n",
    "\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        \n",
    "        samples = []\n",
    "        for sentence in self.corpus:\n",
    "            for i, word in enumerate(sentence):\n",
    "                first_context_word_index = max(0,i-self.context_size)\n",
    "                last_context_word_index = min(i+self.context_size+1, len(sentence))\n",
    "                \n",
    "                context = [no_sentence for j in range(i-self.context_size,first_context_word_index)] + \\\n",
    "                          sentence[first_context_word_index:i] + \\\n",
    "                          sentence[i+1:last_context_word_index] + \\\n",
    "                          [no_sentence for j in range(last_context_word_index,i+self.context_size+1)]\n",
    "                \n",
    "                samples.append((context, word))\n",
    "        \n",
    "        self.samples = samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        context, word = self.samples[idx]\n",
    "        idx_context = torch.empty(len(context), dtype=torch.long)\n",
    "        idx_word = torch.tensor(self.word_to_index[word], dtype=torch.long)\n",
    "        for i, w in enumerate(context):\n",
    "            idx_context[i] = self.word_to_index[w]\n",
    "\n",
    "        return idx_context, idx_word\n",
    "       \n",
    "\n",
    "        \n",
    "class PreprocessBrown(object):\n",
    "    \n",
    "    def __call__(self,corpus_unpreproceced):\n",
    "        corpus = []\n",
    "        for sentence in corpus_unpreproceced:\n",
    "            text = ' '.join(sentence)\n",
    "            text = text.lower()\n",
    "            text.replace('\\n', ' ')\n",
    "            text = re.sub('[^a-z ]+', '', text)\n",
    "            corpus.append([w for w in text.split() if w != ''])\n",
    "        return corpus\n",
    "\n",
    "\n",
    "categories = ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', \\\n",
    "              'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', \\\n",
    "              'reviews', 'romance', 'science_fiction']\n",
    "context_size = 2\n",
    "train_dataset = BrownDataset(categories=categories,\n",
    "                             root='/home/lestien/anaconda3/envs/TorchEnv/nltk_data',\n",
    "                             preprocessing=PreprocessBrown(),\n",
    "                             context_size=context_size)\n",
    "\n",
    "val_dataset = BrownDataset(categories=categories,\n",
    "                             root='/home/lestien/anaconda3/envs/TorchEnv/nltk_data',\n",
    "                             preprocessing=PreprocessBrown(),\n",
    "                             context_size=context_size)\n",
    "\n",
    "print('Cantidad de muestas:', len(train_dataset))\n",
    "print()\n",
    "c, w = train_dataset[0]\n",
    "print('context:')\n",
    "for i in c:\n",
    "    print(train_dataset.index_to_word[i.tolist()])\n",
    "print()\n",
    "print('Palabra central:')\n",
    "print(train_dataset.index_to_word[w.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora falta hacer el resto del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "\n",
    "val_size = .02\n",
    "batch_size = 64\n",
    "\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler=SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype, lm):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            if lm == 'CBOW':\n",
    "                scores = model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "                \n",
    "            elif lm == 'SkipGram':\n",
    "                y = y.view(-1,1)\n",
    "                y = y.expand(-1,x.size()[1])\n",
    "                scores = model(y)[:,:,0]\n",
    "                _, preds = scores.max(1)\n",
    "                for i in range(x.size()[1]):\n",
    "                    num_correct += (preds == x[:,i]).sum()\n",
    "                    num_samples += preds.size(0)\n",
    "        \n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainWord2Vec(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100, lm='CBOW'):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model.train()\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            if lm == 'CBOW':\n",
    "                scores = model(x) # Forward pass\n",
    "                loss = model.loss(scores,y) # Backward pass\n",
    "                \n",
    "            elif lm == 'SkipGram':\n",
    "                y = y.view(-1,1)\n",
    "                y = y.expand(-1,x.size()[1])\n",
    "                scores = model(y)\n",
    "                loss = model.loss(scores,x)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype, lm=lm)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype, lm)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Word2VecCBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2VecCBOW,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word).mean(dim=1)\n",
    "        return self.linear(emb)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        return m(scores,target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "embedding_size = 50\n",
    "model = Word2VecCBOW(vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2VecSkipGram,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, word):\n",
    "        emb = self.embeddings(word)\n",
    "        lin = self.linear(emb).permute(0,2,1)\n",
    "        return lin\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        return m(scores,target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "embedding_size = 50\n",
    "model = Word2VecSkipGram(vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Accuracy: 947/20103 \n",
      "Epoch: 0, Iteration: 200, Accuracy: 1041/20103 \n",
      "Epoch: 0, Iteration: 400, Accuracy: 1104/20103 \n",
      "Epoch: 0, Iteration: 600, Accuracy: 1142/20103 \n",
      "Epoch: 0, Iteration: 800, Accuracy: 1174/20103 \n",
      "Epoch: 0, Iteration: 1000, Accuracy: 1201/20103 \n",
      "Epoch: 0, Iteration: 1200, Accuracy: 1208/20103 \n",
      "Epoch: 0, Iteration: 1400, Accuracy: 1228/20103 \n",
      "Epoch: 0, Iteration: 1600, Accuracy: 1245/20103 \n",
      "Epoch: 0, Iteration: 1800, Accuracy: 1252/20103 \n",
      "Epoch: 0, Iteration: 2000, Accuracy: 1254/20103 \n",
      "Epoch: 0, Iteration: 2200, Accuracy: 1262/20103 \n",
      "Epoch: 0, Iteration: 2400, Accuracy: 1272/20103 \n",
      "Epoch: 0, Iteration: 2600, Accuracy: 1279/20103 \n",
      "Epoch: 0, Iteration: 2800, Accuracy: 1287/20103 \n",
      "Epoch: 0, Iteration: 3000, Accuracy: 1296/20103 \n",
      "Epoch: 0, Iteration: 3200, Accuracy: 1302/20103 \n",
      "Epoch: 0, Iteration: 3400, Accuracy: 1306/20103 \n",
      "Epoch: 0, Iteration: 3600, Accuracy: 1309/20103 \n",
      "Epoch: 0, Iteration: 3800, Accuracy: 1313/20103 \n",
      "Epoch: 0, Iteration: 4000, Accuracy: 1317/20103 \n",
      "Epoch: 0, Iteration: 4200, Accuracy: 1322/20103 \n",
      "Epoch: 0, Iteration: 4400, Accuracy: 1321/20103 \n",
      "Epoch: 0, Iteration: 4600, Accuracy: 1326/20103 \n",
      "Epoch: 0, Iteration: 4800, Accuracy: 1327/20103 \n",
      "Epoch: 0, Iteration: 5000, Accuracy: 1332/20103 \n",
      "Epoch: 0, Iteration: 5200, Accuracy: 1333/20103 \n",
      "Epoch: 0, Iteration: 5400, Accuracy: 1336/20103 \n",
      "Epoch: 0, Iteration: 5600, Accuracy: 1336/20103 \n",
      "Epoch: 0, Iteration: 5800, Accuracy: 1336/20103 \n",
      "Epoch: 0, Iteration: 6000, Accuracy: 1338/20103 \n",
      "Epoch: 0, Iteration: 6200, Accuracy: 1338/20103 \n",
      "Epoch: 0, Iteration: 6400, Accuracy: 1336/20103 \n",
      "Epoch: 0, Iteration: 6600, Accuracy: 1337/20103 \n",
      "Epoch: 0, Iteration: 6800, Accuracy: 1337/20103 \n",
      "Epoch: 0, Iteration: 7000, Accuracy: 1338/20103 \n",
      "Epoch: 0, Iteration: 7200, Accuracy: 1343/20103 \n",
      "Epoch: 0, Iteration: 7400, Accuracy: 1344/20103 \n",
      "Epoch: 0, Iteration: 7600, Accuracy: 1346/20103 \n",
      "Epoch: 0, Iteration: 7800, Accuracy: 1346/20103 \n",
      "Epoch: 0, Iteration: 8000, Accuracy: 1347/20103 \n",
      "Epoch: 0, Iteration: 8200, Accuracy: 1348/20103 \n",
      "Epoch: 0, Iteration: 8400, Accuracy: 1348/20103 \n",
      "Epoch: 0, Iteration: 8600, Accuracy: 1350/20103 \n",
      "Epoch: 0, Iteration: 8800, Accuracy: 1349/20103 \n",
      "Epoch: 0, Iteration: 9000, Accuracy: 1355/20103 \n",
      "Epoch: 0, Iteration: 9200, Accuracy: 1366/20103 \n",
      "Epoch: 0, Iteration: 9400, Accuracy: 1367/20103 \n",
      "Epoch: 0, Iteration: 9600, Accuracy: 1367/20103 \n",
      "Epoch: 0, Iteration: 9800, Accuracy: 1371/20103 \n",
      "Epoch: 0, Iteration: 10000, Accuracy: 1372/20103 \n",
      "Epoch: 0, Iteration: 10200, Accuracy: 1373/20103 \n",
      "Epoch: 0, Iteration: 10400, Accuracy: 1375/20103 \n",
      "Epoch: 0, Iteration: 10600, Accuracy: 1376/20103 \n",
      "Epoch: 0, Iteration: 10800, Accuracy: 1371/20103 \n",
      "Epoch: 0, Iteration: 11000, Accuracy: 1376/20103 \n",
      "Epoch: 0, Iteration: 11200, Accuracy: 1377/20103 \n",
      "Epoch: 0, Iteration: 11400, Accuracy: 1374/20103 \n",
      "Epoch: 0, Iteration: 11600, Accuracy: 1377/20103 \n",
      "Epoch: 0, Iteration: 11800, Accuracy: 1378/20103 \n",
      "Epoch: 0, Iteration: 12000, Accuracy: 1374/20103 \n",
      "Epoch: 0, Iteration: 12200, Accuracy: 1374/20103 \n",
      "Epoch: 0, Iteration: 12400, Accuracy: 1368/20103 \n",
      "Epoch: 0, Iteration: 12600, Accuracy: 1370/20103 \n",
      "Epoch: 0, Iteration: 12800, Accuracy: 1371/20103 \n",
      "Epoch: 0, Iteration: 13000, Accuracy: 1372/20103 \n",
      "Epoch: 0, Iteration: 13200, Accuracy: 1375/20103 \n",
      "Epoch: 0, Iteration: 13400, Accuracy: 1378/20103 \n",
      "Epoch: 0, Iteration: 13600, Accuracy: 1382/20103 \n",
      "Epoch: 0, Iteration: 13800, Accuracy: 1384/20103 \n",
      "Epoch: 0, Iteration: 14000, Accuracy: 1375/20103 \n",
      "Epoch: 0, Iteration: 14200, Accuracy: 1376/20103 \n",
      "Epoch: 0, Iteration: 14400, Accuracy: 1380/20103 \n",
      "Epoch: 0, Iteration: 14600, Accuracy: 1382/20103 \n",
      "Epoch: 0, Iteration: 14800, Accuracy: 1386/20103 \n",
      "Epoch: 0, Iteration: 15000, Accuracy: 1386/20103 \n",
      "Epoch: 0, Iteration: 15200, Accuracy: 1390/20103 \n",
      "Epoch: 1, Iteration: 0, Accuracy: 1391/20103 \n",
      "Epoch: 1, Iteration: 200, Accuracy: 1390/20103 \n",
      "Epoch: 1, Iteration: 400, Accuracy: 1390/20103 \n",
      "Epoch: 1, Iteration: 600, Accuracy: 1392/20103 \n",
      "Epoch: 1, Iteration: 800, Accuracy: 1393/20103 \n",
      "Epoch: 1, Iteration: 1000, Accuracy: 1393/20103 \n",
      "Epoch: 1, Iteration: 1200, Accuracy: 1395/20103 \n",
      "Epoch: 1, Iteration: 1400, Accuracy: 1397/20103 \n",
      "Epoch: 1, Iteration: 1600, Accuracy: 1399/20103 \n",
      "Epoch: 1, Iteration: 1800, Accuracy: 1397/20103 \n",
      "Epoch: 1, Iteration: 2000, Accuracy: 1395/20103 \n",
      "Epoch: 1, Iteration: 2200, Accuracy: 1401/20103 \n",
      "Epoch: 1, Iteration: 2400, Accuracy: 1401/20103 \n",
      "Epoch: 1, Iteration: 2600, Accuracy: 1400/20103 \n",
      "Epoch: 1, Iteration: 2800, Accuracy: 1398/20103 \n",
      "Epoch: 1, Iteration: 3000, Accuracy: 1396/20103 \n",
      "Epoch: 1, Iteration: 3200, Accuracy: 1397/20103 \n",
      "Epoch: 1, Iteration: 3400, Accuracy: 1395/20103 \n",
      "Epoch: 1, Iteration: 3600, Accuracy: 1393/20103 \n",
      "Epoch: 1, Iteration: 3800, Accuracy: 1395/20103 \n",
      "Epoch: 1, Iteration: 4000, Accuracy: 1398/20103 \n",
      "Epoch: 1, Iteration: 4200, Accuracy: 1396/20103 \n",
      "Epoch: 1, Iteration: 4400, Accuracy: 1396/20103 \n",
      "Epoch: 1, Iteration: 4600, Accuracy: 1395/20103 \n",
      "Epoch: 1, Iteration: 4800, Accuracy: 1396/20103 \n",
      "Epoch: 1, Iteration: 5000, Accuracy: 1397/20103 \n",
      "Epoch: 1, Iteration: 5200, Accuracy: 1400/20103 \n",
      "Epoch: 1, Iteration: 5400, Accuracy: 1395/20103 \n",
      "Epoch: 1, Iteration: 5600, Accuracy: 1397/20103 \n",
      "Epoch: 1, Iteration: 5800, Accuracy: 1401/20103 \n",
      "Epoch: 1, Iteration: 6000, Accuracy: 1401/20103 \n",
      "Epoch: 1, Iteration: 6200, Accuracy: 1406/20103 \n",
      "Epoch: 1, Iteration: 6400, Accuracy: 1400/20103 \n",
      "Epoch: 1, Iteration: 6600, Accuracy: 1400/20103 \n",
      "Epoch: 1, Iteration: 6800, Accuracy: 1407/20103 \n",
      "Epoch: 1, Iteration: 7000, Accuracy: 1403/20103 \n",
      "Epoch: 1, Iteration: 7200, Accuracy: 1406/20103 \n",
      "Epoch: 1, Iteration: 7400, Accuracy: 1405/20103 \n",
      "Epoch: 1, Iteration: 7600, Accuracy: 1409/20103 \n",
      "Epoch: 1, Iteration: 7800, Accuracy: 1407/20103 \n",
      "Epoch: 1, Iteration: 8000, Accuracy: 1406/20103 \n",
      "Epoch: 1, Iteration: 8200, Accuracy: 1409/20103 \n",
      "Epoch: 1, Iteration: 8400, Accuracy: 1409/20103 \n",
      "Epoch: 1, Iteration: 8600, Accuracy: 1412/20103 \n",
      "Epoch: 1, Iteration: 8800, Accuracy: 1415/20103 \n",
      "Epoch: 1, Iteration: 9000, Accuracy: 1412/20103 \n",
      "Epoch: 1, Iteration: 9200, Accuracy: 1408/20103 \n",
      "Epoch: 1, Iteration: 9400, Accuracy: 1410/20103 \n",
      "Epoch: 1, Iteration: 9600, Accuracy: 1409/20103 \n",
      "Epoch: 1, Iteration: 9800, Accuracy: 1416/20103 \n",
      "Epoch: 1, Iteration: 10000, Accuracy: 1417/20103 \n",
      "Epoch: 1, Iteration: 10200, Accuracy: 1419/20103 \n",
      "Epoch: 1, Iteration: 10400, Accuracy: 1418/20103 \n",
      "Epoch: 1, Iteration: 10600, Accuracy: 1417/20103 \n",
      "Epoch: 1, Iteration: 10800, Accuracy: 1425/20103 \n",
      "Epoch: 1, Iteration: 11000, Accuracy: 1426/20103 \n",
      "Epoch: 1, Iteration: 11200, Accuracy: 1431/20103 \n",
      "Epoch: 1, Iteration: 11400, Accuracy: 1432/20103 \n",
      "Epoch: 1, Iteration: 11600, Accuracy: 1427/20103 \n",
      "Epoch: 1, Iteration: 11800, Accuracy: 1428/20103 \n",
      "Epoch: 1, Iteration: 12000, Accuracy: 1427/20103 \n",
      "Epoch: 1, Iteration: 12200, Accuracy: 1433/20103 \n",
      "Epoch: 1, Iteration: 12400, Accuracy: 1430/20103 \n",
      "Epoch: 1, Iteration: 12600, Accuracy: 1432/20103 \n",
      "Epoch: 1, Iteration: 12800, Accuracy: 1429/20103 \n",
      "Epoch: 1, Iteration: 13000, Accuracy: 1429/20103 \n",
      "Epoch: 1, Iteration: 13200, Accuracy: 1429/20103 \n",
      "Epoch: 1, Iteration: 13400, Accuracy: 1428/20103 \n",
      "Epoch: 1, Iteration: 13600, Accuracy: 1431/20103 \n",
      "Epoch: 1, Iteration: 13800, Accuracy: 1430/20103 \n",
      "Epoch: 1, Iteration: 14000, Accuracy: 1432/20103 \n",
      "Epoch: 1, Iteration: 14200, Accuracy: 1429/20103 \n",
      "Epoch: 1, Iteration: 14400, Accuracy: 1432/20103 \n",
      "Epoch: 1, Iteration: 14600, Accuracy: 1434/20103 \n",
      "Epoch: 1, Iteration: 14800, Accuracy: 1437/20103 \n",
      "Epoch: 1, Iteration: 15000, Accuracy: 1435/20103 \n",
      "Epoch: 1, Iteration: 15200, Accuracy: 1433/20103 \n",
      "Epoch: 2, Iteration: 0, Accuracy: 1438/20103 \n",
      "Epoch: 2, Iteration: 200, Accuracy: 1438/20103 \n",
      "Epoch: 2, Iteration: 400, Accuracy: 1434/20103 \n",
      "Epoch: 2, Iteration: 600, Accuracy: 1435/20103 \n",
      "Epoch: 2, Iteration: 800, Accuracy: 1430/20103 \n",
      "Epoch: 2, Iteration: 1000, Accuracy: 1433/20103 \n",
      "Epoch: 2, Iteration: 1200, Accuracy: 1432/20103 \n",
      "Epoch: 2, Iteration: 1400, Accuracy: 1434/20103 \n",
      "Epoch: 2, Iteration: 1600, Accuracy: 1436/20103 \n",
      "Epoch: 2, Iteration: 1800, Accuracy: 1436/20103 \n",
      "Epoch: 2, Iteration: 2000, Accuracy: 1435/20103 \n",
      "Epoch: 2, Iteration: 2200, Accuracy: 1436/20103 \n",
      "Epoch: 2, Iteration: 2400, Accuracy: 1437/20103 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 2600, Accuracy: 1434/20103 \n",
      "Epoch: 2, Iteration: 2800, Accuracy: 1436/20103 \n",
      "Epoch: 2, Iteration: 3000, Accuracy: 1433/20103 \n",
      "Epoch: 2, Iteration: 3200, Accuracy: 1435/20103 \n",
      "Epoch: 2, Iteration: 3400, Accuracy: 1440/20103 \n",
      "Epoch: 2, Iteration: 3600, Accuracy: 1441/20103 \n",
      "Epoch: 2, Iteration: 3800, Accuracy: 1444/20103 \n",
      "Epoch: 2, Iteration: 4000, Accuracy: 1448/20103 \n",
      "Epoch: 2, Iteration: 4200, Accuracy: 1442/20103 \n",
      "Epoch: 2, Iteration: 4400, Accuracy: 1441/20103 \n",
      "Epoch: 2, Iteration: 4600, Accuracy: 1442/20103 \n",
      "Epoch: 2, Iteration: 4800, Accuracy: 1445/20103 \n",
      "Epoch: 2, Iteration: 5000, Accuracy: 1449/20103 \n",
      "Epoch: 2, Iteration: 5200, Accuracy: 1440/20103 \n",
      "Epoch: 2, Iteration: 5400, Accuracy: 1440/20103 \n",
      "Epoch: 2, Iteration: 5600, Accuracy: 1442/20103 \n",
      "Epoch: 2, Iteration: 5800, Accuracy: 1447/20103 \n",
      "Epoch: 2, Iteration: 6000, Accuracy: 1445/20103 \n",
      "Epoch: 2, Iteration: 6200, Accuracy: 1445/20103 \n",
      "Epoch: 2, Iteration: 6400, Accuracy: 1447/20103 \n",
      "Epoch: 2, Iteration: 6600, Accuracy: 1450/20103 \n",
      "Epoch: 2, Iteration: 6800, Accuracy: 1447/20103 \n",
      "Epoch: 2, Iteration: 7000, Accuracy: 1450/20103 \n",
      "Epoch: 2, Iteration: 7200, Accuracy: 1449/20103 \n",
      "Epoch: 2, Iteration: 7400, Accuracy: 1450/20103 \n",
      "Epoch: 2, Iteration: 7600, Accuracy: 1446/20103 \n",
      "Epoch: 2, Iteration: 7800, Accuracy: 1443/20103 \n",
      "Epoch: 2, Iteration: 8000, Accuracy: 1444/20103 \n",
      "Epoch: 2, Iteration: 8200, Accuracy: 1449/20103 \n",
      "Epoch: 2, Iteration: 8400, Accuracy: 1451/20103 \n",
      "Epoch: 2, Iteration: 8600, Accuracy: 1449/20103 \n",
      "Epoch: 2, Iteration: 8800, Accuracy: 1449/20103 \n",
      "Epoch: 2, Iteration: 9000, Accuracy: 1450/20103 \n",
      "Epoch: 2, Iteration: 9200, Accuracy: 1453/20103 \n",
      "Epoch: 2, Iteration: 9400, Accuracy: 1449/20103 \n",
      "Epoch: 2, Iteration: 9600, Accuracy: 1447/20103 \n",
      "Epoch: 2, Iteration: 9800, Accuracy: 1455/20103 \n",
      "Epoch: 2, Iteration: 10000, Accuracy: 1448/20103 \n",
      "Epoch: 2, Iteration: 10200, Accuracy: 1450/20103 \n",
      "Epoch: 2, Iteration: 10400, Accuracy: 1450/20103 \n",
      "Epoch: 2, Iteration: 10600, Accuracy: 1455/20103 \n",
      "Epoch: 2, Iteration: 10800, Accuracy: 1454/20103 \n",
      "Epoch: 2, Iteration: 11000, Accuracy: 1458/20103 \n",
      "Epoch: 2, Iteration: 11200, Accuracy: 1453/20103 \n",
      "Epoch: 2, Iteration: 11400, Accuracy: 1455/20103 \n",
      "Epoch: 2, Iteration: 11600, Accuracy: 1464/20103 \n",
      "Epoch: 2, Iteration: 11800, Accuracy: 1465/20103 \n",
      "Epoch: 2, Iteration: 12000, Accuracy: 1460/20103 \n",
      "Epoch: 2, Iteration: 12200, Accuracy: 1461/20103 \n",
      "Epoch: 2, Iteration: 12400, Accuracy: 1460/20103 \n",
      "Epoch: 2, Iteration: 12600, Accuracy: 1460/20103 \n",
      "Epoch: 2, Iteration: 12800, Accuracy: 1458/20103 \n",
      "Epoch: 2, Iteration: 13000, Accuracy: 1458/20103 \n",
      "Epoch: 2, Iteration: 13200, Accuracy: 1459/20103 \n",
      "Epoch: 2, Iteration: 13400, Accuracy: 1460/20103 \n",
      "Epoch: 2, Iteration: 13600, Accuracy: 1463/20103 \n",
      "Epoch: 2, Iteration: 13800, Accuracy: 1464/20103 \n",
      "Epoch: 2, Iteration: 14000, Accuracy: 1466/20103 \n",
      "Epoch: 2, Iteration: 14200, Accuracy: 1466/20103 \n",
      "Epoch: 2, Iteration: 14400, Accuracy: 1461/20103 \n",
      "Epoch: 2, Iteration: 14600, Accuracy: 1460/20103 \n",
      "Epoch: 2, Iteration: 14800, Accuracy: 1457/20103 \n",
      "Epoch: 2, Iteration: 15000, Accuracy: 1458/20103 \n",
      "Epoch: 2, Iteration: 15200, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 0, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 200, Accuracy: 1458/20103 \n",
      "Epoch: 3, Iteration: 400, Accuracy: 1457/20103 \n",
      "Epoch: 3, Iteration: 600, Accuracy: 1456/20103 \n",
      "Epoch: 3, Iteration: 800, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 1000, Accuracy: 1458/20103 \n",
      "Epoch: 3, Iteration: 1200, Accuracy: 1451/20103 \n",
      "Epoch: 3, Iteration: 1400, Accuracy: 1454/20103 \n",
      "Epoch: 3, Iteration: 1600, Accuracy: 1456/20103 \n",
      "Epoch: 3, Iteration: 1800, Accuracy: 1456/20103 \n",
      "Epoch: 3, Iteration: 2000, Accuracy: 1450/20103 \n",
      "Epoch: 3, Iteration: 2200, Accuracy: 1452/20103 \n",
      "Epoch: 3, Iteration: 2400, Accuracy: 1451/20103 \n",
      "Epoch: 3, Iteration: 2600, Accuracy: 1452/20103 \n",
      "Epoch: 3, Iteration: 2800, Accuracy: 1448/20103 \n",
      "Epoch: 3, Iteration: 3000, Accuracy: 1448/20103 \n",
      "Epoch: 3, Iteration: 3200, Accuracy: 1454/20103 \n",
      "Epoch: 3, Iteration: 3400, Accuracy: 1457/20103 \n",
      "Epoch: 3, Iteration: 3600, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 3800, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 4000, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 4200, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 4400, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 4600, Accuracy: 1461/20103 \n",
      "Epoch: 3, Iteration: 4800, Accuracy: 1465/20103 \n",
      "Epoch: 3, Iteration: 5000, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 5200, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 5400, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 5600, Accuracy: 1461/20103 \n",
      "Epoch: 3, Iteration: 5800, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 6000, Accuracy: 1461/20103 \n",
      "Epoch: 3, Iteration: 6200, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 6400, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 6600, Accuracy: 1465/20103 \n",
      "Epoch: 3, Iteration: 6800, Accuracy: 1468/20103 \n",
      "Epoch: 3, Iteration: 7000, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 7200, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 7400, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 7600, Accuracy: 1465/20103 \n",
      "Epoch: 3, Iteration: 7800, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 8000, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 8200, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 8400, Accuracy: 1470/20103 \n",
      "Epoch: 3, Iteration: 8600, Accuracy: 1468/20103 \n",
      "Epoch: 3, Iteration: 8800, Accuracy: 1465/20103 \n",
      "Epoch: 3, Iteration: 9000, Accuracy: 1467/20103 \n",
      "Epoch: 3, Iteration: 9200, Accuracy: 1464/20103 \n",
      "Epoch: 3, Iteration: 9400, Accuracy: 1467/20103 \n",
      "Epoch: 3, Iteration: 9600, Accuracy: 1465/20103 \n",
      "Epoch: 3, Iteration: 9800, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 10000, Accuracy: 1457/20103 \n",
      "Epoch: 3, Iteration: 10200, Accuracy: 1457/20103 \n",
      "Epoch: 3, Iteration: 10400, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 10600, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 10800, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 11000, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 11200, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 11400, Accuracy: 1459/20103 \n",
      "Epoch: 3, Iteration: 11600, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 11800, Accuracy: 1457/20103 \n",
      "Epoch: 3, Iteration: 12000, Accuracy: 1462/20103 \n",
      "Epoch: 3, Iteration: 12200, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 12400, Accuracy: 1460/20103 \n",
      "Epoch: 3, Iteration: 12600, Accuracy: 1470/20103 \n",
      "Epoch: 3, Iteration: 12800, Accuracy: 1465/20103 \n",
      "Epoch: 3, Iteration: 13000, Accuracy: 1464/20103 \n",
      "Epoch: 3, Iteration: 13200, Accuracy: 1463/20103 \n",
      "Epoch: 3, Iteration: 13400, Accuracy: 1470/20103 \n",
      "Epoch: 3, Iteration: 13600, Accuracy: 1475/20103 \n",
      "Epoch: 3, Iteration: 13800, Accuracy: 1478/20103 \n",
      "Epoch: 3, Iteration: 14000, Accuracy: 1474/20103 \n",
      "Epoch: 3, Iteration: 14200, Accuracy: 1477/20103 \n",
      "Epoch: 3, Iteration: 14400, Accuracy: 1473/20103 \n",
      "Epoch: 3, Iteration: 14600, Accuracy: 1474/20103 \n",
      "Epoch: 3, Iteration: 14800, Accuracy: 1471/20103 \n",
      "Epoch: 3, Iteration: 15000, Accuracy: 1474/20103 \n",
      "Epoch: 3, Iteration: 15200, Accuracy: 1468/20103 \n",
      "Epoch: 4, Iteration: 0, Accuracy: 1472/20103 \n",
      "Epoch: 4, Iteration: 200, Accuracy: 1475/20103 \n",
      "Epoch: 4, Iteration: 400, Accuracy: 1473/20103 \n",
      "Epoch: 4, Iteration: 600, Accuracy: 1470/20103 \n",
      "Epoch: 4, Iteration: 800, Accuracy: 1471/20103 \n",
      "Epoch: 4, Iteration: 1000, Accuracy: 1470/20103 \n",
      "Epoch: 4, Iteration: 1200, Accuracy: 1472/20103 \n",
      "Epoch: 4, Iteration: 1400, Accuracy: 1475/20103 \n",
      "Epoch: 4, Iteration: 1600, Accuracy: 1471/20103 \n",
      "Epoch: 4, Iteration: 1800, Accuracy: 1474/20103 \n",
      "Epoch: 4, Iteration: 2000, Accuracy: 1476/20103 \n",
      "Epoch: 4, Iteration: 2200, Accuracy: 1477/20103 \n",
      "Epoch: 4, Iteration: 2400, Accuracy: 1476/20103 \n",
      "Epoch: 4, Iteration: 2600, Accuracy: 1475/20103 \n",
      "Epoch: 4, Iteration: 2800, Accuracy: 1474/20103 \n",
      "Epoch: 4, Iteration: 3000, Accuracy: 1483/20103 \n",
      "Epoch: 4, Iteration: 3200, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 3400, Accuracy: 1484/20103 \n",
      "Epoch: 4, Iteration: 3600, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 3800, Accuracy: 1476/20103 \n",
      "Epoch: 4, Iteration: 4000, Accuracy: 1473/20103 \n",
      "Epoch: 4, Iteration: 4200, Accuracy: 1469/20103 \n",
      "Epoch: 4, Iteration: 4400, Accuracy: 1473/20103 \n",
      "Epoch: 4, Iteration: 4600, Accuracy: 1475/20103 \n",
      "Epoch: 4, Iteration: 4800, Accuracy: 1474/20103 \n",
      "Epoch: 4, Iteration: 5000, Accuracy: 1479/20103 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Iteration: 5200, Accuracy: 1481/20103 \n",
      "Epoch: 4, Iteration: 5400, Accuracy: 1478/20103 \n",
      "Epoch: 4, Iteration: 5600, Accuracy: 1479/20103 \n",
      "Epoch: 4, Iteration: 5800, Accuracy: 1481/20103 \n",
      "Epoch: 4, Iteration: 6000, Accuracy: 1475/20103 \n",
      "Epoch: 4, Iteration: 6200, Accuracy: 1473/20103 \n",
      "Epoch: 4, Iteration: 6400, Accuracy: 1478/20103 \n",
      "Epoch: 4, Iteration: 6600, Accuracy: 1481/20103 \n",
      "Epoch: 4, Iteration: 6800, Accuracy: 1483/20103 \n",
      "Epoch: 4, Iteration: 7000, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 7200, Accuracy: 1483/20103 \n",
      "Epoch: 4, Iteration: 7400, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 7600, Accuracy: 1484/20103 \n",
      "Epoch: 4, Iteration: 7800, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 8000, Accuracy: 1484/20103 \n",
      "Epoch: 4, Iteration: 8200, Accuracy: 1488/20103 \n",
      "Epoch: 4, Iteration: 8400, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 8600, Accuracy: 1484/20103 \n",
      "Epoch: 4, Iteration: 8800, Accuracy: 1482/20103 \n",
      "Epoch: 4, Iteration: 9000, Accuracy: 1479/20103 \n",
      "Epoch: 4, Iteration: 9200, Accuracy: 1474/20103 \n",
      "Epoch: 4, Iteration: 9400, Accuracy: 1477/20103 \n",
      "Epoch: 4, Iteration: 9600, Accuracy: 1481/20103 \n",
      "Epoch: 4, Iteration: 9800, Accuracy: 1474/20103 \n",
      "Epoch: 4, Iteration: 10000, Accuracy: 1477/20103 \n",
      "Epoch: 4, Iteration: 10200, Accuracy: 1480/20103 \n",
      "Epoch: 4, Iteration: 10400, Accuracy: 1478/20103 \n",
      "Epoch: 4, Iteration: 10600, Accuracy: 1488/20103 \n",
      "Epoch: 4, Iteration: 10800, Accuracy: 1493/20103 \n",
      "Epoch: 4, Iteration: 11000, Accuracy: 1487/20103 \n",
      "Epoch: 4, Iteration: 11200, Accuracy: 1491/20103 \n",
      "Epoch: 4, Iteration: 11400, Accuracy: 1492/20103 \n",
      "Epoch: 4, Iteration: 11600, Accuracy: 1494/20103 \n",
      "Epoch: 4, Iteration: 11800, Accuracy: 1495/20103 \n",
      "Epoch: 4, Iteration: 12000, Accuracy: 1490/20103 \n",
      "Epoch: 4, Iteration: 12200, Accuracy: 1495/20103 \n",
      "Epoch: 4, Iteration: 12400, Accuracy: 1495/20103 \n",
      "Epoch: 4, Iteration: 12600, Accuracy: 1497/20103 \n",
      "Epoch: 4, Iteration: 12800, Accuracy: 1490/20103 \n",
      "Epoch: 4, Iteration: 13000, Accuracy: 1485/20103 \n",
      "Epoch: 4, Iteration: 13200, Accuracy: 1483/20103 \n",
      "Epoch: 4, Iteration: 13400, Accuracy: 1484/20103 \n",
      "Epoch: 4, Iteration: 13600, Accuracy: 1495/20103 \n",
      "Epoch: 4, Iteration: 13800, Accuracy: 1495/20103 \n",
      "Epoch: 4, Iteration: 14000, Accuracy: 1496/20103 \n",
      "Epoch: 4, Iteration: 14200, Accuracy: 1494/20103 \n",
      "Epoch: 4, Iteration: 14400, Accuracy: 1495/20103 \n",
      "Epoch: 4, Iteration: 14600, Accuracy: 1496/20103 \n",
      "Epoch: 4, Iteration: 14800, Accuracy: 1497/20103 \n",
      "Epoch: 4, Iteration: 15000, Accuracy: 1488/20103 \n",
      "Epoch: 4, Iteration: 15200, Accuracy: 1490/20103 \n",
      "Final accuracy: 7.43%\n"
     ]
    }
   ],
   "source": [
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long, \n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 5 # Cantidad de epochs\n",
    "sample_loss_every = 200 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-2 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainWord2Vec(model, data, epochs, learning_rate, sample_loss_every, lm='CBOW')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
