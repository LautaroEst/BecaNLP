{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class AGNewsDataset(object):\n",
    "    \n",
    "    token_pad = '<PAD>'\n",
    "    token_unk = '<UNK>'\n",
    "    token_sep = '<TS>'\n",
    "    special_tokens = [token_pad, token_unk]\n",
    "    \n",
    "    def __init__(self, root, train=True):\n",
    "        \n",
    "        if not os.path.exists(root):\n",
    "            raise IOError('Carpeta {} no encontrada'.format(root))\n",
    "        self.root = root + '/' if root[-1] != '/' else root\n",
    "        \n",
    "        print('Buscando archivos train.csv test.csv...')\n",
    "        self.train_path = '{}train.csv'.format(self.root)\n",
    "        self.test_path = '{}test.csv'.format(self.root)\n",
    "        if not os.path.exists(self.train_path):\n",
    "            raise IOError('Archivo train.csv no encontrado en el directorio {}'.format(self.train_path))\n",
    "        elif not os.path.exists(self.train_path):\n",
    "            raise IOError('Archivo test.csv no encontrado en el directorio {}'.format(self.test_path))\n",
    "        \n",
    "        print('Obteniendo el dataset...')\n",
    "        train_data_df = self._read_data(self.train_path)\n",
    "        test_data_df = self._read_data(self.test_path)\n",
    "                \n",
    "        print('Obteniendo el vocabulario...')\n",
    "        self.vocabulary = self._get_vocabulary(train_data_df)\n",
    "        self.vocabulary = self._get_vocabulary(test_data_df, vocabulary=self.vocabulary)\n",
    "        self.n_tokens = len(self.vocabulary)\n",
    "        \n",
    "        self._data = train_data_df if train else test_data_df\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        if isinstance(idx,torch.Tensor):\n",
    "            index = idx.tolist()\n",
    "        else:\n",
    "            index = idx\n",
    "        try:\n",
    "            text, cls_idx = self._data.iloc[index,:]\n",
    "        except IndexError:\n",
    "            raise IndexError('{} exceeds index of dataset'.format(index))\n",
    "            return\n",
    "        \n",
    "        cls_idx = torch.tensor(cls_idx - 1, dtype=torch.long)\n",
    "        \n",
    "        text = self._string_to_tokens(text)\n",
    "        text_idx = torch.tensor([self.vocabulary.token_to_index(word) for word in text], dtype=torch.long)\n",
    "        text_one_hot = torch.zeros(self.n_tokens, dtype=torch.float)\n",
    "        text_one_hot[text_idx] = 1\n",
    "        \n",
    "        return text_one_hot, cls_idx\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "\n",
    "    def _read_data(self, filename):\n",
    "                \n",
    "        pp_filename = [token for token in filename.split('/')]\n",
    "        pp_filename[-1] = 'preprocessed_' + pp_filename[-1]\n",
    "        pp_filename = '/'.join(pp_filename)\n",
    "        if not os.path.exists(pp_filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            with open(pp_filename, 'w') as pp_f:\n",
    "                pp_f.write('Title,Class label\\n')\n",
    "                for index, row in df.iterrows():\n",
    "                    text = re.sub( r'\"', r\"'\", row[1])\n",
    "                    text = re.sub( r' ', self.token_sep, text)\n",
    "                    new_row = re.sub( r' ', self.token_sep, '\\\"{0}\\\",{1:}\\n'.format(text,int(row[0])) )\n",
    "                    pp_f.write(new_row)\n",
    "        \n",
    "        data_df = pd.read_csv(pp_filename)\n",
    "        return data_df\n",
    "                \n",
    "        \n",
    "    def _get_vocabulary(self,df,vocabulary=None):\n",
    "        \n",
    "        if vocabulary is None:\n",
    "            vocabulary = AGNewsVocabulary()\n",
    "        \n",
    "        for token in self.special_tokens:\n",
    "            idx = vocabulary.add_token(token)\n",
    "            vocabulary._idx_to_freq[idx] -= 1\n",
    "\n",
    "        for title in df.iloc[:,0]:\n",
    "            title = self._string_to_tokens(title)\n",
    "            for word in title:\n",
    "                vocabulary.add_token(word)\n",
    "\n",
    "        return vocabulary\n",
    "        \n",
    "    def _string_to_tokens(self,string):\n",
    "        return string.split(self.token_sep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AGNewsVocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, token_or_index):\n",
    "        freqs = []\n",
    "        try:\n",
    "            length = len(token_or_index)\n",
    "        except TypeError:\n",
    "            tk_or_idx_list = [token_or_index]\n",
    "        \n",
    "        for tk_or_idx in tk_or_idx_list:\n",
    "            if isinstance(tk_or_idx, int):\n",
    "                if tk_or_idx not in self._idx_to_token:\n",
    "                    raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "                freqs.append(self._idx_to_freq[tk_or_idx])\n",
    "            if isinstance(tk_or_idx, str):\n",
    "                if tk_or_idx not in self._token_to_idx:\n",
    "                    raise KeyError('the token {} is not in the Vocabulary'.format(tk_or_idx))\n",
    "                freqs.append(self._idx_to_freq[self._token_to_idx[tk_or_idx]])\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        if len(freqs) == 1 and not isinstance(token_or_index, list):\n",
    "            return freqs[0]\n",
    "        return freqs\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de datos en NLP\n",
    "\n",
    "Todo esto está sacado de https://github.com/joosthub/PyTorchNLPBook, que es el github del libro que usan en cs224n. El capítulo 3 tiene un ejemplo \"Classifying Sentiment of Restaurant Reviews\" que es la fuente de toda esta información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivos train.csv test.csv...\n",
      "Obteniendo el dataset...\n",
      "Obteniendo el vocabulario...\n",
      "Buscando archivos train.csv test.csv...\n",
      "Obteniendo el dataset...\n",
      "Obteniendo el vocabulario...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AGNewsDataset(root='./AG_NEWS/', train=True)\n",
    "test_dataset = AGNewsDataset(root='./AG_NEWS/', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from TorchDataUtils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Cantidad de muestras de entrenamiento: 119999\n",
      "Tamaño de cada muestra: 73916\n",
      "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Test Dataset:\n",
      "Cantidad de muestras de testeo: 7599\n",
      "Tamaño de cada muestra: 73916\n",
      "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Tamaño del vocabulario: 73916\n",
      "Se usan las palabras del train y del test\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Train Dataset:\n",
    "Cantidad de muestras de entrenamiento: {}\n",
    "Tamaño de cada muestra: {}\n",
    "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\"\"\".format(len(train_dataset), len(train_dataset[0][0])))\n",
    "\n",
    "print(\"\"\"Test Dataset:\n",
    "Cantidad de muestras de testeo: {}\n",
    "Tamaño de cada muestra: {}\n",
    "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\"\"\".format(len(test_dataset), len(test_dataset[0][0])))\n",
    "\n",
    "print(\"Tamaño del vocabulario: {}\".format(len(train_dataset.vocabulary)))\n",
    "print(\"Se usan las palabras del train y del test\")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = generate_data_batches(train_dataset, test_dataset,\n",
    "                                                                         batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch number: 0\n",
      "Accuracy on validation dataset: 1269/2400 (52.88%)\n",
      "Accuracy on train dataset: 61421/117599 (52.23%)\n",
      "\n",
      "Epoch: 0, Batch number: 50\n",
      "Accuracy on validation dataset: 1269/2400 (52.88%)\n",
      "Exiting training...\n",
      "Final accuracy registered on validation dataset: 1269/2400 (52.88%)\n",
      "Final accuracy registered on train dataset: 61421/117599 (52.23%)\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.float, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 1 # Cantidad de epochs\n",
    "sample_loss_every = 50 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-3 # Tasa de aprendizaje\n",
    "check_on_train = True # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
