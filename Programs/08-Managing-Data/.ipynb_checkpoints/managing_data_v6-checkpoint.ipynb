{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class AGNewsDataset(object):\n",
    "    \n",
    "    token_pad = '<PAD>'\n",
    "    token_unk = '<UNK>'\n",
    "    token_sep = '<TS>'\n",
    "    special_tokens = [token_pad, token_unk]\n",
    "    \n",
    "    def __init__(self, root, train=True, cutoff_freq = 10):\n",
    "        \n",
    "        if not os.path.exists(root):\n",
    "            raise IOError('Carpeta {} no encontrada'.format(root))\n",
    "        self.root = root + '/' if root[-1] != '/' else root\n",
    "        \n",
    "        print('Buscando archivos train.csv test.csv...')\n",
    "        self.train_path = '{}train.csv'.format(self.root)\n",
    "        self.test_path = '{}test.csv'.format(self.root)\n",
    "        if not os.path.exists(self.train_path):\n",
    "            raise IOError('Archivo train.csv no encontrado en el directorio {}'.format(self.train_path))\n",
    "        elif not os.path.exists(self.train_path):\n",
    "            raise IOError('Archivo test.csv no encontrado en el directorio {}'.format(self.test_path))\n",
    "        \n",
    "        print('Obteniendo el dataset...')\n",
    "        train_data_df = self._read_data(self.train_path)\n",
    "        test_data_df = self._read_data(self.test_path)\n",
    "                \n",
    "        print('Obteniendo el vocabulario...')\n",
    "        self.vocabulary = self._get_vocabulary(train_data_df)\n",
    "        self.vocabulary = self._get_vocabulary(test_data_df, vocabulary=self.vocabulary)\n",
    "        self.n_tokens = len(self.vocabulary)\n",
    "        \n",
    "        self._data = train_data_df if train else test_data_df\n",
    "        self.cf = cutoff_freq\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        if isinstance(idx,torch.Tensor):\n",
    "            index = idx.tolist()\n",
    "        else:\n",
    "            index = idx\n",
    "        try:\n",
    "            text, cls_idx = self._data.iloc[index,:]\n",
    "        except IndexError:\n",
    "            raise IndexError('{} exceeds index of dataset'.format(index))\n",
    "            return\n",
    "        \n",
    "        cls_idx = torch.tensor(cls_idx - 1, dtype=torch.long)\n",
    "        \n",
    "        text = self._string_to_tokens(text)\n",
    "        text_idx = torch.tensor([self.vocabulary.token_to_index(word) if \\\n",
    "                                 self.vocabulary.get_freq(word) > self.cf \\\n",
    "                                 else self.vocabulary.token_to_index(self.token_unk) for word in text], dtype=torch.long)\n",
    "        text_one_hot = torch.zeros(self.n_tokens, dtype=torch.float)\n",
    "        text_one_hot[text_idx] = 1\n",
    "        \n",
    "        return text_one_hot, cls_idx\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "\n",
    "    def _read_data(self, filename):\n",
    "                \n",
    "        pp_filename = [token for token in filename.split('/')]\n",
    "        pp_filename[-1] = 'preprocessed_' + pp_filename[-1]\n",
    "        pp_filename = '/'.join(pp_filename)\n",
    "        if not os.path.exists(pp_filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            with open(pp_filename, 'w') as pp_f:\n",
    "                pp_f.write('Title,Class label\\n')\n",
    "                for index, row in df.iterrows():\n",
    "                    text = re.sub( r'\"', r\"'\", row[1])\n",
    "                    text = re.sub( r' ', self.token_sep, text)\n",
    "                    new_row = re.sub( r' ', self.token_sep, '\\\"{0}\\\",{1:}\\n'.format(text,int(row[0])) )\n",
    "                    pp_f.write(new_row)\n",
    "        \n",
    "        data_df = pd.read_csv(pp_filename)\n",
    "        return data_df\n",
    "                \n",
    "        \n",
    "    def _get_vocabulary(self,df,vocabulary=None):\n",
    "        \n",
    "        if vocabulary is None:\n",
    "            vocabulary = AGNewsVocabulary()\n",
    "        \n",
    "        for token in self.special_tokens:\n",
    "            idx = vocabulary.add_token(token)\n",
    "            vocabulary._idx_to_freq[idx] -= 1\n",
    "\n",
    "        for title in df.iloc[:,0]:\n",
    "            title = self._string_to_tokens(title)\n",
    "            for word in title:\n",
    "                vocabulary.add_token(word)\n",
    "\n",
    "        return vocabulary\n",
    "        \n",
    "    def _string_to_tokens(self,string):\n",
    "        return string.split(self.token_sep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AGNewsVocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, token):\n",
    "        idx = self.token_to_index(token)\n",
    "        return self._idx_to_freq[idx]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de datos en NLP\n",
    "\n",
    "Todo esto está sacado de https://github.com/joosthub/PyTorchNLPBook, que es el github del libro que usan en cs224n. El capítulo 3 tiene un ejemplo \"Classifying Sentiment of Restaurant Reviews\" que es la fuente de toda esta información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivos train.csv test.csv...\n",
      "Obteniendo el dataset...\n",
      "Obteniendo el vocabulario...\n",
      "Buscando archivos train.csv test.csv...\n",
      "Obteniendo el dataset...\n",
      "Obteniendo el vocabulario...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AGNewsDataset(root='./AG_NEWS/', train=True, cutoff_freq=50)\n",
    "test_dataset = AGNewsDataset(root='./AG_NEWS/', train=False, cutoff_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from TorchDataUtils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Cantidad de muestras de entrenamiento: 119999\n",
      "Tamaño de cada muestra: 73916\n",
      "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Test Dataset:\n",
      "Cantidad de muestras de testeo: 7599\n",
      "Tamaño de cada muestra: 73916\n",
      "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Tamaño del vocabulario: 73916\n",
      "Se usan las palabras del train y del test\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Train Dataset:\n",
    "Cantidad de muestras de entrenamiento: {}\n",
    "Tamaño de cada muestra: {}\n",
    "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\"\"\".format(len(train_dataset), len(train_dataset[0][0])))\n",
    "\n",
    "print(\"\"\"Test Dataset:\n",
    "Cantidad de muestras de testeo: {}\n",
    "Tamaño de cada muestra: {}\n",
    "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\"\"\".format(len(test_dataset), len(test_dataset[0][0])))\n",
    "\n",
    "print(\"Tamaño del vocabulario: {}\".format(len(train_dataset.vocabulary)))\n",
    "print(\"Se usan las palabras del train y del test\")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = generate_data_batches(train_dataset, test_dataset,\n",
    "                                                                         batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch number: 0\n",
      "Accuracy on validation dataset: 1235/2400 (51.46%)\n",
      "\n",
      "Epoch: 0, Batch number: 200\n",
      "Accuracy on validation dataset: 1234/2400 (51.42%)\n",
      "\n",
      "Epoch: 0, Batch number: 400\n",
      "Accuracy on validation dataset: 1234/2400 (51.42%)\n",
      "\n",
      "Epoch: 0, Batch number: 600\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 0, Batch number: 800\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 1, Batch number: 81\n",
      "Accuracy on validation dataset: 1234/2400 (51.42%)\n",
      "\n",
      "Epoch: 1, Batch number: 281\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 1, Batch number: 481\n",
      "Accuracy on validation dataset: 1235/2400 (51.46%)\n",
      "\n",
      "Epoch: 1, Batch number: 681\n",
      "Accuracy on validation dataset: 1234/2400 (51.42%)\n",
      "\n",
      "Epoch: 1, Batch number: 881\n",
      "Accuracy on validation dataset: 1233/2400 (51.38%)\n",
      "\n",
      "Epoch: 2, Batch number: 162\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 2, Batch number: 362\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 2, Batch number: 562\n",
      "Accuracy on validation dataset: 1238/2400 (51.58%)\n",
      "\n",
      "Epoch: 2, Batch number: 762\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 3, Batch number: 43\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 3, Batch number: 243\n",
      "Accuracy on validation dataset: 1235/2400 (51.46%)\n",
      "\n",
      "Epoch: 3, Batch number: 443\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 3, Batch number: 643\n",
      "Accuracy on validation dataset: 1235/2400 (51.46%)\n",
      "\n",
      "Epoch: 3, Batch number: 843\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 4, Batch number: 124\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 4, Batch number: 324\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 4, Batch number: 524\n",
      "Accuracy on validation dataset: 1239/2400 (51.62%)\n",
      "\n",
      "Epoch: 4, Batch number: 724\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 5, Batch number: 5\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 5, Batch number: 205\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 5, Batch number: 405\n",
      "Accuracy on validation dataset: 1236/2400 (51.50%)\n",
      "\n",
      "Epoch: 5, Batch number: 605\n",
      "Accuracy on validation dataset: 1235/2400 (51.46%)\n",
      "\n",
      "Epoch: 5, Batch number: 805\n",
      "Accuracy on validation dataset: 1238/2400 (51.58%)\n",
      "\n",
      "Epoch: 6, Batch number: 86\n",
      "Accuracy on validation dataset: 1239/2400 (51.62%)\n",
      "\n",
      "Epoch: 6, Batch number: 286\n",
      "Accuracy on validation dataset: 1238/2400 (51.58%)\n",
      "\n",
      "Epoch: 6, Batch number: 486\n",
      "Accuracy on validation dataset: 1237/2400 (51.54%)\n",
      "\n",
      "Epoch: 6, Batch number: 686\n",
      "Accuracy on validation dataset: 1239/2400 (51.62%)\n",
      "\n",
      "Epoch: 6, Batch number: 886\n",
      "Accuracy on validation dataset: 1239/2400 (51.62%)\n",
      "\n",
      "Epoch: 7, Batch number: 167\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 7, Batch number: 367\n",
      "Accuracy on validation dataset: 1239/2400 (51.62%)\n",
      "\n",
      "Epoch: 7, Batch number: 567\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 7, Batch number: 767\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 8, Batch number: 48\n",
      "Accuracy on validation dataset: 1242/2400 (51.75%)\n",
      "\n",
      "Epoch: 8, Batch number: 248\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 8, Batch number: 448\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 8, Batch number: 648\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 8, Batch number: 848\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 9, Batch number: 129\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 9, Batch number: 329\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 9, Batch number: 529\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 9, Batch number: 729\n",
      "Accuracy on validation dataset: 1242/2400 (51.75%)\n",
      "\n",
      "Epoch: 10, Batch number: 10\n",
      "Accuracy on validation dataset: 1238/2400 (51.58%)\n",
      "\n",
      "Epoch: 10, Batch number: 210\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 10, Batch number: 410\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 10, Batch number: 610\n",
      "Accuracy on validation dataset: 1239/2400 (51.62%)\n",
      "\n",
      "Epoch: 10, Batch number: 810\n",
      "Accuracy on validation dataset: 1243/2400 (51.79%)\n",
      "\n",
      "Epoch: 11, Batch number: 91\n",
      "Accuracy on validation dataset: 1243/2400 (51.79%)\n",
      "\n",
      "Epoch: 11, Batch number: 291\n",
      "Accuracy on validation dataset: 1242/2400 (51.75%)\n",
      "\n",
      "Epoch: 11, Batch number: 491\n",
      "Accuracy on validation dataset: 1243/2400 (51.79%)\n",
      "\n",
      "Epoch: 11, Batch number: 691\n",
      "Accuracy on validation dataset: 1240/2400 (51.67%)\n",
      "\n",
      "Epoch: 11, Batch number: 891\n",
      "Accuracy on validation dataset: 1243/2400 (51.79%)\n",
      "\n",
      "Epoch: 12, Batch number: 172\n",
      "Accuracy on validation dataset: 1242/2400 (51.75%)\n",
      "\n",
      "Epoch: 12, Batch number: 372\n",
      "Accuracy on validation dataset: 1245/2400 (51.88%)\n",
      "\n",
      "Epoch: 12, Batch number: 572\n",
      "Accuracy on validation dataset: 1245/2400 (51.88%)\n",
      "\n",
      "Epoch: 12, Batch number: 772\n",
      "Accuracy on validation dataset: 1241/2400 (51.71%)\n",
      "\n",
      "Epoch: 13, Batch number: 53\n",
      "Accuracy on validation dataset: 1244/2400 (51.83%)\n",
      "\n",
      "Epoch: 13, Batch number: 253\n",
      "Accuracy on validation dataset: 1247/2400 (51.96%)\n",
      "\n",
      "Epoch: 13, Batch number: 453\n",
      "Accuracy on validation dataset: 1246/2400 (51.92%)\n",
      "\n",
      "Epoch: 13, Batch number: 653\n",
      "Accuracy on validation dataset: 1246/2400 (51.92%)\n",
      "\n",
      "Epoch: 13, Batch number: 853\n",
      "Accuracy on validation dataset: 1244/2400 (51.83%)\n",
      "\n",
      "Epoch: 14, Batch number: 134\n",
      "Accuracy on validation dataset: 1242/2400 (51.75%)\n",
      "\n",
      "Epoch: 14, Batch number: 334\n",
      "Accuracy on validation dataset: 1243/2400 (51.79%)\n",
      "\n",
      "Epoch: 14, Batch number: 534\n",
      "Accuracy on validation dataset: 1242/2400 (51.75%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.lib.c_is_list_like'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lestien/anaconda3/envs/TorchEnv/lib/python3.7/abc.py\", line 137, in __instancecheck__\n",
      "    def __instancecheck__(cls, instance):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch number: 734\n",
      "Accuracy on validation dataset: 1244/2400 (51.83%)\n",
      "\n",
      "Epoch: 15, Batch number: 15\n",
      "Accuracy on validation dataset: 1244/2400 (51.83%)\n",
      "\n",
      "Exiting training...\n",
      "Final accuracy registered on validation dataset: 1244/2400 (51.83%)\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.float, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 20 # Cantidad de epochs\n",
    "sample_loss_every = 200 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-4 # Tasa de aprendizaje\n",
    "check_on_train = False # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
