{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class AGNewsDataset(object):\n",
    "    \n",
    "    token_pad = '<PAD>'\n",
    "    token_unk = '<UNK>'\n",
    "    token_sep = '<TS>'\n",
    "    special_tokens = [token_pad, token_unk]\n",
    "    \n",
    "    def __init__(self, root, train=True):\n",
    "        \n",
    "        if not os.path.exists(root):\n",
    "            raise IOError('Carpeta {} no encontrada'.format(root))\n",
    "        self.root = root + '/' if root[-1] != '/' else root\n",
    "        \n",
    "        print('Buscando archivos train.csv test.csv...')\n",
    "        self.train_path = '{}train.csv'.format(self.root)\n",
    "        self.test_path = '{}test.csv'.format(self.root)\n",
    "        if not os.path.exists(self.train_path):\n",
    "            raise IOError('Archivo train.csv no encontrado en el directorio {}'.format(self.train_path))\n",
    "        elif not os.path.exists(self.train_path):\n",
    "            raise IOError('Archivo test.csv no encontrado en el directorio {}'.format(self.test_path))\n",
    "        \n",
    "        print('Obteniendo el dataset...')\n",
    "        train_data_df = self._read_data(self.train_path)\n",
    "        test_data_df = self._read_data(self.test_path)\n",
    "                \n",
    "        print('Obteniendo el vocabulario...')\n",
    "        self.vocabulary = self._get_vocabulary(train_data_df)\n",
    "        self.vocabulary = self._get_vocabulary(test_data_df, vocabulary=self.vocabulary)\n",
    "        self.n_tokens = len(self.vocabulary)\n",
    "        \n",
    "        self._data = train_data_df if train else test_data_df\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        if isinstance(idx,torch.Tensor):\n",
    "            index = idx.tolist()\n",
    "        else:\n",
    "            index = idx\n",
    "        try:\n",
    "            text, cls_idx = self._data.iloc[index,:]\n",
    "        except IndexError:\n",
    "            raise IndexError('{} exceeds index of dataset'.format(index))\n",
    "            return\n",
    "        \n",
    "        cls_idx = torch.tensor(cls_idx - 1, dtype=torch.long)\n",
    "        \n",
    "        text = self._string_to_tokens(text)\n",
    "        text_idx = torch.tensor([self.vocabulary.token_to_index(word) for word in text], dtype=torch.long)\n",
    "        text_one_hot = torch.zeros(self.n_tokens, dtype=torch.long)\n",
    "        text_one_hot[text_idx] = 1\n",
    "        \n",
    "        return text_one_hot, cls_idx\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "\n",
    "    def _read_data(self, filename):\n",
    "                \n",
    "        pp_filename = [token for token in filename.split('/')]\n",
    "        pp_filename[-1] = 'preprocessed_' + pp_filename[-1]\n",
    "        pp_filename = '/'.join(pp_filename)\n",
    "\n",
    "        if not os.path.exists(pp_filename):\n",
    "            with open(pp_filename, 'w') as pp_f:\n",
    "                pp_f.write('Title,Class label\\n')\n",
    "                for index, row in df.iterrows():\n",
    "                    text = re.sub( r'\"', r\"'\", row[1])\n",
    "                    text = re.sub( r' ', self.token_sep, text)\n",
    "                    new_row = re.sub( r' ', self.token_sep, '\\\"{0}\\\",{1:}\\n'.format(text,int(row[0])) )\n",
    "                    pp_f.write(new_row)\n",
    "        \n",
    "        data_df = pd.read_csv(pp_filename)\n",
    "        return data_df\n",
    "                \n",
    "        \n",
    "    def _get_vocabulary(self,df,vocabulary=None):\n",
    "        \n",
    "        if vocabulary is None:\n",
    "            vocabulary = AGNewsVocabulary()\n",
    "        \n",
    "        for token in self.special_tokens:\n",
    "            idx = vocabulary.add_token(token)\n",
    "            vocabulary._idx_to_freq[idx] -= 1\n",
    "\n",
    "        for title in df.iloc[:,0]:\n",
    "            title = self._string_to_tokens(title)\n",
    "            for word in title:\n",
    "                vocabulary.add_token(word)\n",
    "\n",
    "        return vocabulary\n",
    "        \n",
    "    def _string_to_tokens(self,string):\n",
    "        return string.split(self.token_sep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AGNewsVocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, token_or_index):\n",
    "        freqs = []\n",
    "        try:\n",
    "            length = len(token_or_index)\n",
    "        except TypeError:\n",
    "            tk_or_idx_list = [token_or_index]\n",
    "        \n",
    "        for tk_or_idx in tk_or_idx_list:\n",
    "            if isinstance(tk_or_idx, int):\n",
    "                if tk_or_idx not in self._idx_to_token:\n",
    "                    raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "                freqs.append(self._idx_to_freq[tk_or_idx])\n",
    "            if isinstance(tk_or_idx, str):\n",
    "                if tk_or_idx not in self._token_to_idx:\n",
    "                    raise KeyError('the token {} is not in the Vocabulary'.format(tk_or_idx))\n",
    "                freqs.append(self._idx_to_freq[self._token_to_idx[tk_or_idx]])\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        if len(freqs) == 1 and not isinstance(token_or_index, list):\n",
    "            return freqs[0]\n",
    "        return freqs\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de datos en NLP\n",
    "\n",
    "Todo esto está sacado de https://github.com/joosthub/PyTorchNLPBook, que es el github del libro que usan en cs224n. El capítulo 3 tiene un ejemplo \"Classifying Sentiment of Restaurant Reviews\" que es la fuente de toda esta información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AGNewsDataset(root='./AG_NEWS/', train=True)\n",
    "test_dataset = AGNewsDataset(root='./AG_NEWS/', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from TorchDataUtils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Cantidad de muestras de entrenamiento: 120000\n",
      "Tamaño de cada muestra: 9\n",
      "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Test Dataset:\n",
      "Cantidad de muestras de testeo: 7600\n",
      "Tamaño de cada muestra: 7\n",
      "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Tamaño del vocabulario: 73916\n",
      "Se usan las palabras del train y del test\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Train Dataset:\n",
    "Cantidad de muestras de entrenamiento: {}\n",
    "Tamaño de cada muestra: {}\n",
    "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\"\"\".format(len(train_dataset), len(train_dataset[0][0])))\n",
    "\n",
    "print(\"\"\"Test Dataset:\n",
    "Cantidad de muestras de testeo: {}\n",
    "Tamaño de cada muestra: {}\n",
    "Categorías: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\"\"\".format(len(test_dataset), len(test_dataset[0][0])))\n",
    "\n",
    "print(\"Tamaño del vocabulario: {}\".format(len(train_dataset.vocabulary)))\n",
    "print(\"Se usan las palabras del train y del test\")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = generate_data_batches(train_dataset, test_dataset,\n",
    "                                                                         batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10]), tensor(2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 6 and 5 in dimension 1 at /opt/conda/conda-bld/pytorch_1570910687650/work/aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d7b3bfc6f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Entrenamiento:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mperformance_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_loss_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_on_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/disco.lautaro/home/lestien/Documents/BecaNLP/Programs/08-Managing-Data/TorchDataUtils.py\u001b[0m in \u001b[0;36mSGDTrainModel\u001b[0;34m(model, data, epochs, learning_rate, sample_loss_every, check_on_train)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 6 and 5 in dimension 1 at /opt/conda/conda-bld/pytorch_1570910687650/work/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.long, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 1 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-1 # Tasa de aprendizaje\n",
    "check_on_train = False # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
