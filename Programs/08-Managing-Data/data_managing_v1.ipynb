{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de los datos en NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG's News Topic Classification Dataset\r\n",
      "\r\n",
      "Version 3, Updated 09/09/2015\r\n",
      "\r\n",
      "\r\n",
      "ORIGIN\r\n",
      "\r\n",
      "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\r\n",
      "\r\n",
      "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\r\n",
      "\r\n",
      "\r\n",
      "DESCRIPTION\r\n",
      "\r\n",
      "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\r\n",
      "\r\n",
      "The file classes.txt contains a list of classes corresponding to each label.\r\n",
      "\r\n",
      "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 4), title and description. The title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../AG_NEWS/ag_news_csv/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\"\r\n",
      "\"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\"\r\n",
      "\"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\"\r\n",
      "\"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\"\r\n",
      "\"3\",\"Oil prices soar to all-time record, posing new menace to US economy (AFP)\",\"AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\"\r\n",
      "\"3\",\"Stocks End Up, But Near Year Lows (Reuters)\",\"Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\"\r\n",
      "\"3\",\"Money Funds Fell in Latest Week (AP)\",\"AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\"\r\n",
      "\"3\",\"Fed minutes show dissent over inflation (USATODAY.com)\",\"USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.\"\r\n",
      "\"3\",\"Safety Net (Forbes.com)\",\"Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"\"buying insurance was the furthest thing from my mind,\"\" says Riley.\"\r\n",
      "\"3\",\"Wall St. Bears Claw Back Into the Black\",\" NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\"\r\n"
     ]
    }
   ],
   "source": [
    "!head ../AG_NEWS/ag_news_csv/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import csv\n",
    "import re\n",
    "\n",
    "class AGNEWS(Dataset):\n",
    "    \n",
    "    def _get_categories(self):\n",
    "        with open(self.root_path + 'classes.txt', 'r') as f:\n",
    "            categories = [line[:-2] for line in f]\n",
    "        return categories\n",
    "    \n",
    "    \n",
    "    def preprocessing(self, sentence):\n",
    "        \"\"\"\n",
    "        Función para preprocesar los datos\n",
    "        \"\"\"\n",
    "        return sentence.split()\n",
    "    \n",
    "    def _get_size_of_longest_sentence(self):\n",
    "        length = 0\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            csv_file = csv.reader(f)\n",
    "            for line in csv_file:\n",
    "                class_idx, title, description = line\n",
    "                text_length = len(self.preprocessing(title))\n",
    "                if  text_length > length:\n",
    "                    length = text_length \n",
    "                    \n",
    "        return length\n",
    "                \n",
    "    \n",
    "    def _get_vocab(self):\n",
    "        \"\"\"\n",
    "        Devuelve un diccionario con las palabras del vocabulario\n",
    "        y la cantidad de veces que aparece en el corpus.\n",
    "        \"\"\"\n",
    "        \n",
    "        special_tokens = ['<PAD>', '<UNK>']\n",
    "        vocabulary = {token: i for i, token in enumerate(special_tokens)}\n",
    "        \n",
    "        filenames = [self.root_path + 'train.csv', self.root_path + 'test.csv']\n",
    "        for filename in filenames:\n",
    "            with open(filename, 'r') as f:\n",
    "                csv_file = csv.reader(f)\n",
    "                for i, line in enumerate(csv_file):\n",
    "                    class_idx, title, description = line\n",
    "                    title = self.preprocessing(title)\n",
    "                    for word in title:\n",
    "                        if word in vocabulary:\n",
    "                            vocabulary[word] += 1\n",
    "                        else:\n",
    "                            vocabulary[word] = 1\n",
    "                            \n",
    "        return vocabulary\n",
    "        \n",
    "    \n",
    "    def __init__(self, root_path, train=True):\n",
    "        \n",
    "        # Directorio de raíz de los datos:\n",
    "        self.root_path = root_path \n",
    "        \n",
    "        # Elección de datos (entrenamiento o testeo):\n",
    "        if train:\n",
    "            self.data_filename = root_path + 'train.csv'\n",
    "        else:\n",
    "            self.data_filename = root_path + 'test.csv'\n",
    "        \n",
    "        # Obtención de las categorías:\n",
    "        self.categories = self._get_categories()\n",
    "        \n",
    "        # Obtención del vocabulario:\n",
    "        self.vocabulary = self._get_vocab() # Contiene las frecuencias\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: word for idx, word in enumerate(self.vocabulary)}\n",
    "        self.size_of_longest_sentence = self._get_size_of_longest_sentence()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            csv_file = csv.reader(f)\n",
    "            for i, line in enumerate(csv_file):\n",
    "                pass\n",
    "        return i+1\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            csv_file = csv.reader(f)\n",
    "            for i, line in enumerate(csv_file):\n",
    "                if i == idx:\n",
    "                    class_idx, title, description = line\n",
    "                    title = self.preprocessing(title)\n",
    "                    class_idx = int(class_idx) - 1\n",
    "                    break\n",
    "        \n",
    "        title = torch.tensor([self.word_to_index[word] for word in title], dtype=torch.long)\n",
    "        class_idx = torch.tensor(class_idx, dtype=torch.long)\n",
    "        \n",
    "        title = torch.nn.functional.pad(title,\n",
    "                                        pad=(0,self.size_of_longest_sentence - len(title)),\n",
    "                                        mode='constant', \n",
    "                                        value=self.word_to_index['<PAD>'])\n",
    "        \n",
    "        return title, class_idx\n",
    "            \n",
    "        \n",
    "root_path = '../AG_NEWS/ag_news_csv/'\n",
    "train_dataset = AGNEWS(root_path, train=True)\n",
    "val_dataset = AGNEWS(root_path, train=True)\n",
    "test_dataset = AGNEWS(root_path, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# LEVANTADO EL TEXTO DE UNA #\n",
    "#############################\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import csv\n",
    "import re\n",
    "\n",
    "class AGNEWS(Dataset):\n",
    "    \n",
    "    def _get_categories(self):\n",
    "        with open(self.root_path + 'classes.txt', 'r') as f:\n",
    "            categories = [line[:-2] for line in f]\n",
    "        return categories\n",
    "    \n",
    "    \n",
    "    def preprocessing(self, sentence):\n",
    "        \"\"\"\n",
    "        Función para preprocesar los datos\n",
    "        \"\"\"\n",
    "        return sentence.split()\n",
    "    \n",
    "    def _get_size_of_longest_sentence(self):\n",
    "        length = 0\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            csv_file = csv.reader(f)\n",
    "            for line in csv_file:\n",
    "                class_idx, title, description = line\n",
    "                text_length = len(self.preprocessing(title))\n",
    "                if  text_length > length:\n",
    "                    length = text_length \n",
    "                    \n",
    "        return length\n",
    "                \n",
    "    \n",
    "    def _get_vocab(self):\n",
    "        \"\"\"\n",
    "        Devuelve un diccionario con las palabras del vocabulario\n",
    "        y la cantidad de veces que aparece en el corpus.\n",
    "        \"\"\"\n",
    "        \n",
    "        special_tokens = ['<PAD>', '<UNK>']\n",
    "        vocabulary = {token: i for i, token in enumerate(special_tokens)}\n",
    "        \n",
    "        filenames = [self.root_path + 'train.csv', self.root_path + 'test.csv']\n",
    "        for filename in filenames:\n",
    "            with open(filename, 'r') as f:\n",
    "                csv_file = csv.reader(f)\n",
    "                for i, line in enumerate(csv_file):\n",
    "                    class_idx, title, description = line\n",
    "                    title = self.preprocessing(title)\n",
    "                    for word in title:\n",
    "                        if word in vocabulary:\n",
    "                            vocabulary[word] += 1\n",
    "                        else:\n",
    "                            vocabulary[word] = 1\n",
    "                            \n",
    "        return vocabulary\n",
    "        \n",
    "    \n",
    "    def __init__(self, root_path, train=True):\n",
    "        \n",
    "        # Directorio de raíz de los datos:\n",
    "        self.root_path = root_path \n",
    "        \n",
    "        # Elección de datos (entrenamiento o testeo):\n",
    "        if train:\n",
    "            self.data_filename = root_path + 'train.csv'\n",
    "        else:\n",
    "            self.data_filename = root_path + 'test.csv'\n",
    "        \n",
    "        # Obtención del texto:\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            self.text = f.read()\n",
    "        \n",
    "        # Obtención de las categorías:\n",
    "        self.categories = self._get_categories()\n",
    "        \n",
    "        # Obtención del vocabulario:\n",
    "        self.vocabulary = self._get_vocab() # Contiene las frecuencias\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: word for idx, word in enumerate(self.vocabulary)}\n",
    "        self.size_of_longest_sentence = self._get_size_of_longest_sentence()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            csv_file = csv.reader(f)\n",
    "            for i, line in enumerate(csv_file):\n",
    "                pass\n",
    "        return i+1\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        with open(self.data_filename, 'r') as f:\n",
    "            csv_file = csv.reader(f)\n",
    "            for i, line in enumerate(csv_file):\n",
    "                if i == idx:\n",
    "                    class_idx, title, description = line\n",
    "                    title = self.preprocessing(title)\n",
    "                    class_idx = int(class_idx) - 1\n",
    "                    break\n",
    "        \n",
    "        title = torch.tensor([self.word_to_index[word] for word in title], dtype=torch.long)\n",
    "        class_idx = torch.tensor(class_idx, dtype=torch.long)\n",
    "        \n",
    "        title = torch.nn.functional.pad(title,\n",
    "                                        pad=(0,self.size_of_longest_sentence - len(title)),\n",
    "                                        mode='constant', \n",
    "                                        value=self.word_to_index['<PAD>'])\n",
    "        \n",
    "        return title, class_idx\n",
    "            \n",
    "        \n",
    "root_path = '../AG_NEWS/ag_news_csv/'\n",
    "train_dataset = AGNEWS(root_path, train=True)\n",
    "val_dataset = AGNEWS(root_path, train=True)\n",
    "test_dataset = AGNEWS(root_path, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset:  120000\n",
      "Tamaño del vocabulario:  73957\n",
      "(tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño del dataset: ', len(train_dataset))\n",
    "print('Tamaño del vocabulario: ', len(train_dataset.vocabulary))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # Tamaño del batch\n",
    "val_size = .02 # Proporción de muestras utilizadas para validación \n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset)) # Cantidad de muestras de entrenamiento\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN # Cantidad de muestras para validación\n",
    "sampler = lambda start, end: torch.utils.data.SubsetRandomSampler(range(start, end)) # Función para mezclar aleatoriamente las muestras\n",
    "\n",
    "\n",
    "# Dataloader para las muestras de entrenamiento:\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               sampler=sampler(0, NUM_TRAIN))\n",
    "\n",
    "# Dataloader para las muestras de validación:\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             sampler=sampler(NUM_TRAIN, NUM_TRAIN+NUM_VAL))\n",
    "\n",
    "# Dataloader para las muestras de testeo:\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingSoftmaxClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vectors, embedding_dim, n_classes):\n",
    "        \n",
    "        super(EmbeddingSoftmaxClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(n_vectors, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).mean(dim=1)\n",
    "        scores = self.linear(emb)\n",
    "        return scores\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "n_classes = len(train_dataset.categories) # Cantidad de categorías\n",
    "n_vectors = len(train_dataset.vocabulary) # Cantidad de palabras que contiene la frase\n",
    "embedding_dim = 50\n",
    "model = EmbeddingSoftmaxClassifier(n_vectors, embedding_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  \n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainModel(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_size = len(train_dataloader)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x) \n",
    "            \n",
    "            # Backward pass\n",
    "            loss = model.loss(scores,y)                 \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (e * batch_size + t) % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history\n",
    "\n",
    "\n",
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long,\n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 1 # Cantidad de epochs\n",
    "sample_loss_every = 1 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-1 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "input_dtype = data['input_dtype'] \n",
    "target_dtype = data['target_dtype']\n",
    "device = data['device']\n",
    "train_dataloader = data['train_dataloader']\n",
    "val_dataloader = data['val_dataloader']\n",
    "\n",
    "performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "\n",
    "model = model.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, epoch = 0\n",
      "t = 1, epoch = 0\n",
      "t = 2, epoch = 0\n",
      "t = 3, epoch = 0\n",
      "t = 4, epoch = 0\n",
      "t = 5, epoch = 0\n",
      "t = 6, epoch = 0\n",
      "t = 7, epoch = 0\n",
      "t = 8, epoch = 0\n",
      "t = 9, epoch = 0\n",
      "t = 10, epoch = 0\n",
      "t = 11, epoch = 0\n",
      "t = 12, epoch = 0\n",
      "t = 13, epoch = 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1f6151372765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-476439b903ce>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mclass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_batches = len(train_dataloader)\n",
    "for e in range(epochs):\n",
    "    for t, (x,y) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        x = x.to(device=device, dtype=input_dtype)\n",
    "        y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(x) \n",
    "\n",
    "        # Backward pass\n",
    "        loss = model.loss(scores,y)                 \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (e * n_batches + t) % sample_loss_every == 0:\n",
    "            print('t = {}, epoch = {}'.format(t, e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
