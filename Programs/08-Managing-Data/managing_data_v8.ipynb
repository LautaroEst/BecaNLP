{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from TorchDataUtils import *\n",
    "from NLPDataUtils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNewsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root='./AG_NEWS/', preprocess=lambda x: x, train=True):\n",
    "        \n",
    "        target = 'train.csv' if train else 'test.csv'\n",
    "        df = pd.read_csv(root + target, header=None, names=['class_idx', 'title', 'description'])\n",
    "        \n",
    "        # Etiquetas:\n",
    "        self.cls_indeces = torch.tensor(df['class_idx'].tolist(), dtype=torch.long) - 1\n",
    "        \n",
    "        # DataSeries con las muestras de entradas:\n",
    "        data = df['title']\n",
    "        self.data = preprocess(data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        return self.vectorizer.vectorize(self.data.iloc[idx]), self.cls_indeces[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cls_indeces)\n",
    "    \n",
    "    \n",
    "def GetAGNewsDataset(root, preprocess, cutoff=25):\n",
    "    \n",
    "    # Datasets:\n",
    "    train_dataset = AGNewsDataset(root, preprocess=preprocess, train=True)\n",
    "    train_dataset.vectorizer = Vectorizer(train_dataset.data, cutoff=cutoff)\n",
    "    test_dataset = AGNewsDataset(root, preprocess=preprocess, train=False)\n",
    "    test_dataset.vectorizer = train_dataset.vectorizer\n",
    "    \n",
    "    # Dataloaders:\n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_data_batches(train_dataset, \n",
    "                                                                              test_dataset,    \n",
    "                                                                              batch_size=64)\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 1\n",
    "\n",
    "* Cutoff: 25\n",
    "* Preprocesamiento: sólo split\n",
    "* Muestras de entrada: Title\n",
    "* Modelo de clasificación: Layer lineal + CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    sep_token = ' '\n",
    "    splitted_data = data.str.split(sep_token)\n",
    "    return splitted_data\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = GetAGNewsDataset(root='./AG_NEWS/', \n",
    "                                                                     preprocess=preprocess, \n",
    "                                                                     cutoff=25)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "\n",
    "vocab_size = len(train_dataset.vectorizer.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch number: 0\n",
      "Accuracy on validation dataset: 560/2400 (23.33%)\n",
      "\n",
      "Epoch: 0, Batch number: 500\n",
      "Accuracy on validation dataset: 564/2400 (23.50%)\n",
      "\n",
      "Epoch: 0, Batch number: 1000\n",
      "Accuracy on validation dataset: 572/2400 (23.83%)\n",
      "\n",
      "Epoch: 0, Batch number: 1500\n",
      "Accuracy on validation dataset: 581/2400 (24.21%)\n",
      "\n",
      "Exiting training...\n",
      "Final accuracy registered on validation dataset: 581/2400 (24.21%)\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.float, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 500 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-5 # Tasa de aprendizaje\n",
    "check_on_train = False # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 2\n",
    "\n",
    "* Cutoff: 50\n",
    "* Preprocesamiento: sólo split\n",
    "* Muestras de entrada: Title\n",
    "* Modelo de clasificación: Layer lineal + CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    sep_token = ' '\n",
    "    splitted_data = data.str.split(sep_token)\n",
    "    return splitted_data\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = GetAGNewsDataset(root='./AG_NEWS/', \n",
    "                                                                     preprocess=preprocess, \n",
    "                                                                     cutoff=50)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "\n",
    "vocab_size = len(train_dataset.vectorizer.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch number: 0\n",
      "Accuracy on validation dataset: 1260/2400 (52.50%)\n",
      "\n",
      "Epoch: 0, Batch number: 500\n",
      "Accuracy on validation dataset: 1260/2400 (52.50%)\n",
      "\n",
      "Epoch: 0, Batch number: 1000\n",
      "Accuracy on validation dataset: 1260/2400 (52.50%)\n",
      "\n",
      "Epoch: 0, Batch number: 1500\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 1, Batch number: 162\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 1, Batch number: 662\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 1, Batch number: 1162\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 1, Batch number: 1662\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 2, Batch number: 324\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 2, Batch number: 824\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 2, Batch number: 1324\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 2, Batch number: 1824\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 3, Batch number: 486\n",
      "Accuracy on validation dataset: 1263/2400 (52.62%)\n",
      "\n",
      "Epoch: 3, Batch number: 986\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 3, Batch number: 1486\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 4, Batch number: 148\n",
      "Accuracy on validation dataset: 1263/2400 (52.62%)\n",
      "\n",
      "Epoch: 4, Batch number: 648\n",
      "Accuracy on validation dataset: 1265/2400 (52.71%)\n",
      "\n",
      "Epoch: 4, Batch number: 1148\n",
      "Accuracy on validation dataset: 1265/2400 (52.71%)\n",
      "\n",
      "Epoch: 4, Batch number: 1648\n",
      "Accuracy on validation dataset: 1263/2400 (52.62%)\n",
      "\n",
      "Epoch: 5, Batch number: 310\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 5, Batch number: 810\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 5, Batch number: 1310\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 5, Batch number: 1810\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 6, Batch number: 472\n",
      "Accuracy on validation dataset: 1264/2400 (52.67%)\n",
      "\n",
      "Epoch: 6, Batch number: 972\n",
      "Accuracy on validation dataset: 1263/2400 (52.62%)\n",
      "\n",
      "Exiting training...\n",
      "Final accuracy registered on validation dataset: 1263/2400 (52.62%)\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.float, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 500 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-5 # Tasa de aprendizaje\n",
    "check_on_train = False # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 3\n",
    "\n",
    "* Cutoff: 25\n",
    "* Preprocesamiento: Un poco más trabajado\n",
    "* Muestras de entrada: Title\n",
    "* Modelo de clasificación: Layer lineal + CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    df = data.str.replace(r'\\(AP\\)','')\n",
    "    df = df.str.replace(r'\\(Reuters\\)','')\n",
    "    df = df.str.replace(r'\\(AFP\\)','')\n",
    "    df = df.str.replace(r'\\(SPACE\\.com\\)','')\n",
    "    df = df.str.replace(r'\\ba\\b','')\n",
    "    df = df.str.replace(r'\\bthe\\b','')\n",
    "    df = df.str.replace(r'\\bis\\b','')\n",
    "    df = df.str.replace(r'\\bof\\b','')\n",
    "    df = df.str.replace(r'\\bto\\b','')\n",
    "    df = df.str.replace(r'[,:;\\?\\!\\\"]','')\n",
    "    df = df.str.replace(r'\\s+','<SEP>')\n",
    "    df = df.str.replace(r\"'s<SEP>\",\"<SEP>'s<SEP>\")\n",
    "    df = df.str.split('<SEP>')\n",
    "    return df\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = GetAGNewsDataset(root='./AG_NEWS/', \n",
    "                                                                     preprocess=preprocess, \n",
    "                                                                     cutoff=25)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "\n",
    "vocab_size = len(train_dataloader.dataset.vectorizer.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch number: 0\n",
      "Accuracy on validation dataset: 1298/2400 (54.08%)\n",
      "\n",
      "Epoch: 0, Batch number: 500\n",
      "Accuracy on validation dataset: 1297/2400 (54.04%)\n",
      "\n",
      "Epoch: 0, Batch number: 1000\n",
      "Accuracy on validation dataset: 1297/2400 (54.04%)\n",
      "\n",
      "Epoch: 0, Batch number: 1500\n",
      "Accuracy on validation dataset: 1297/2400 (54.04%)\n",
      "\n",
      "Epoch: 1, Batch number: 162\n",
      "Accuracy on validation dataset: 1298/2400 (54.08%)\n",
      "\n",
      "Epoch: 1, Batch number: 662\n",
      "Accuracy on validation dataset: 1297/2400 (54.04%)\n",
      "\n",
      "Epoch: 1, Batch number: 1162\n",
      "Accuracy on validation dataset: 1298/2400 (54.08%)\n",
      "\n",
      "Epoch: 1, Batch number: 1662\n",
      "Accuracy on validation dataset: 1298/2400 (54.08%)\n",
      "\n",
      "Epoch: 2, Batch number: 324\n",
      "Accuracy on validation dataset: 1298/2400 (54.08%)\n",
      "\n",
      "Epoch: 2, Batch number: 824\n",
      "Accuracy on validation dataset: 1299/2400 (54.12%)\n",
      "\n",
      "Epoch: 2, Batch number: 1324\n",
      "Accuracy on validation dataset: 1299/2400 (54.12%)\n",
      "\n",
      "Epoch: 2, Batch number: 1824\n",
      "Accuracy on validation dataset: 1299/2400 (54.12%)\n",
      "\n",
      "Epoch: 3, Batch number: 486\n",
      "Accuracy on validation dataset: 1300/2400 (54.17%)\n",
      "\n",
      "Epoch: 3, Batch number: 986\n",
      "Accuracy on validation dataset: 1299/2400 (54.12%)\n",
      "\n",
      "Epoch: 3, Batch number: 1486\n",
      "Accuracy on validation dataset: 1299/2400 (54.12%)\n",
      "\n",
      "Epoch: 4, Batch number: 148\n",
      "Accuracy on validation dataset: 1300/2400 (54.17%)\n",
      "\n",
      "Epoch: 4, Batch number: 648\n",
      "Accuracy on validation dataset: 1300/2400 (54.17%)\n",
      "\n",
      "Epoch: 4, Batch number: 1148\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 4, Batch number: 1648\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 5, Batch number: 310\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 5, Batch number: 810\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 5, Batch number: 1310\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 5, Batch number: 1810\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 6, Batch number: 472\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 6, Batch number: 972\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 6, Batch number: 1472\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 7, Batch number: 134\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 7, Batch number: 634\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 7, Batch number: 1134\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 7, Batch number: 1634\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 8, Batch number: 296\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 8, Batch number: 796\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 8, Batch number: 1296\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n",
      "Epoch: 8, Batch number: 1796\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 9, Batch number: 458\n",
      "Accuracy on validation dataset: 1302/2400 (54.25%)\n",
      "\n",
      "Epoch: 9, Batch number: 958\n",
      "Accuracy on validation dataset: 1300/2400 (54.17%)\n",
      "\n",
      "Epoch: 9, Batch number: 1458\n",
      "Accuracy on validation dataset: 1301/2400 (54.21%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.float, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 500 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-5 # Tasa de aprendizaje\n",
    "check_on_train = False # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 4\n",
    "\n",
    "* Cutoff: 75\n",
    "* Preprocesamiento: Un poco más trabajado\n",
    "* Muestras de entrada: Title\n",
    "* Modelo de clasificación: Layer lineal + CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    df = data.str.replace(r'\\(AP\\)','')\n",
    "    df = df.str.replace(r'\\(Reuters\\)','')\n",
    "    df = df.str.replace(r'\\(AFP\\)','')\n",
    "    df = df.str.replace(r'\\(SPACE\\.com\\)','')\n",
    "    df = df.str.replace(r'\\ba\\b','')\n",
    "    df = df.str.replace(r'\\bthe\\b','')\n",
    "    df = df.str.replace(r'\\bis\\b','')\n",
    "    df = df.str.replace(r'\\bof\\b','')\n",
    "    df = df.str.replace(r'\\bto\\b','')\n",
    "    df = df.str.replace(r'[,:;\\?\\!\\\"]','')\n",
    "    df = df.str.replace(r'\\s+','<SEP>')\n",
    "    df = df.str.replace(r\"'s<SEP>\",\"<SEP>'s<SEP>\")\n",
    "    df = df.str.split('<SEP>')\n",
    "    return df\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = GetAGNewsDataset(root='./AG_NEWS/', \n",
    "                                                                     preprocess=preprocess, \n",
    "                                                                     cutoff=75)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Linear(vocab_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores, target)\n",
    "    \n",
    "\n",
    "vocab_size = len(train_dataloader.dataset.vectorizer.vocabulary)\n",
    "n_classes = 4\n",
    "model = TextClassifier(vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch number: 0\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 0, Batch number: 500\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 0, Batch number: 1000\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 0, Batch number: 1500\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 1, Batch number: 162\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 1, Batch number: 662\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 1, Batch number: 1162\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 1, Batch number: 1662\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 2, Batch number: 324\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 2, Batch number: 824\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 2, Batch number: 1324\n",
      "Accuracy on validation dataset: 1262/2400 (52.58%)\n",
      "\n",
      "Epoch: 2, Batch number: 1824\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 3, Batch number: 486\n",
      "Accuracy on validation dataset: 1260/2400 (52.50%)\n",
      "\n",
      "Epoch: 3, Batch number: 986\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 3, Batch number: 1486\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 4, Batch number: 148\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 4, Batch number: 648\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 4, Batch number: 1148\n",
      "Accuracy on validation dataset: 1261/2400 (52.54%)\n",
      "\n",
      "Epoch: 4, Batch number: 1648\n",
      "Accuracy on validation dataset: 1260/2400 (52.50%)\n",
      "\n",
      "Exiting training...\n",
      "Final accuracy registered on validation dataset: 1260/2400 (52.50%)\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de las muestras:\n",
    "data = {\n",
    "    'use_gpu': True, # Trasladar o no las muestras a la GPU\n",
    "    'input_dtype': torch.float, # Tipo de dato de las muestras de entrada\n",
    "    'target_dtype': torch.long, # Tipo de dato de las muestras de salida\n",
    "    'train_dataloader': train_dataloader, # Dataset de entrenamiento\n",
    "    'val_dataloader': val_dataloader # Dataset de validación\n",
    "}\n",
    "\n",
    "# Parámetros de optimización:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 500 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-5 # Tasa de aprendizaje\n",
    "check_on_train = False # Queremos ver los resultados también en el train set\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = SGDTrainModel(model, data, epochs, learning_rate, sample_loss_every, check_on_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 4049/7600 (53.28%)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() and data['use_gpu'] else torch.device('cpu')\n",
    "num_correct_val, num_samples_val = CheckAccuracy(test_dataloader, \n",
    "                                                 model, \n",
    "                                                 device, \n",
    "                                                 data['input_dtype'], \n",
    "                                                 data['target_dtype'])\n",
    "print('Accuracy on test dataset: {}/{} ({:.2f}%)'.format(num_correct_val, num_samples_val, 100 * float(num_correct_val) / num_samples_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
