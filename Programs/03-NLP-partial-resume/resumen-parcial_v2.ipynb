{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen Parcial de NLP\n",
    "\n",
    "Este resumen muestra una de las metodologías utilizadas actualmente para diseñar un algoritmo de NLP. En este caso, la tarea a resolver será la de clasificación de texto. \n",
    "\n",
    "Esta metodología consiste en lo siguiente: para una tarea determinada, primero se realiza una extracción del significado de los elementos que componen el texto y luego se diseña un modelo que toma como entrada dicha representación del significado. Este modelo es el que realiza la tarea propiamente dicha, pero no lo hace sobre los símbolos que componen el texto original, sino sobre una representación de los mismos en la que pueden verse algunas características semánticas entre ellas. \n",
    "\n",
    "Por ejemplo, para el [corpus de texto *Brown*](https://www.nltk.org/book/ch02.html), que contiene textos de 15 categorías diferentes (news, editorial, reviews, religion, hobbies, lore, belles lettres, government, learned, fiction, mystery, science fiction, adventure, romanc, humor), se buscará realizar un algoritmo que clasifique una frase en alguna de estas categorías. Este procedimiento se realiza de dos maneras distintas:\n",
    "\n",
    "* Se entrena un clasificador de una capa (con salida Softmax) que tiene como entrada una frase, y como salida la probabilidad de cada una de las categorías mencionadas anteriormente.\n",
    "\n",
    "* Se entrena el mismo clasificador que antes, con la diferencia que la entrada del modelo no son las palabras que componen la frase, sino la representación del significado de cada una de esas palabras (*word embedding*).\n",
    "\n",
    "**TODO: EXPLICAR UN POCO MEJOR QUE EL SIGNIFICADO SE EXTRAE CON UN MODELO DE LENGUAJE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/Documents/BecaNLP/Programs/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importamos PyTorch\n",
    "import torch\n",
    "\n",
    "# Importamos la libraría de utils de NLP\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from nlp_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera forma (sin extracción del significado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownClassificationDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, preprocessing, train=True):\n",
    "        \n",
    "        self.root = root\n",
    "        \n",
    "        file_train = open(root + 'train.txt', 'r')\n",
    "        file_test = open(root + 'test.txt', 'r')\n",
    "        text_train = file_train.read()\n",
    "        text_test = file_test.read()\n",
    "        file_train.close()\n",
    "        file_test.close()\n",
    "\n",
    "        samples = []\n",
    "        vocab = []\n",
    "        \n",
    "        if train:\n",
    "            text = text_train.split('<ENDLABEL>')\n",
    "            extended_text = text_test.split('<ENDLABEL>')\n",
    "        else:\n",
    "            text = text_test.split('<ENDLABEL>')\n",
    "            extended_text = text_train.split('<ENDLABEL>')\n",
    "            \n",
    "        for i in range(len(text)-1):\n",
    "            sample = text[i].split('<BEGINLABEL>')\n",
    "            text_splitted = sample[0].split('<TS>')\n",
    "            samples.append((text_splitted, sample[1]))\n",
    "        \n",
    "        if preprocessing:\n",
    "            samples_preprocessed = preprocessing(samples)\n",
    "        else:\n",
    "            samples_preprocessed = samples\n",
    "            \n",
    "        vocab.append([sample[0] for sample in samples_preprocessed])\n",
    "        vocab.append([extended_text[i].split('<BEGINLABEL>')[0].split('<TS>') for i in range(len(extended_text)-1)])\n",
    "        \n",
    "        self.samples = samples_preprocessed\n",
    "        it = itertools.chain.from_iterable\n",
    "        self.vocabulary = set(it(list(it(vocab))))\n",
    "        self.vocabulary.add('<NS>')\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        \n",
    "        self.size_of_longest_sample = max([len(sample[0]) for sample in self.samples])\n",
    "        self.categories = list(set([sample[1] for sample in self.samples]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, category = self.samples[idx]\n",
    "        idx_sentence = torch.empty(self.size_of_longest_sample, dtype=torch.long)\n",
    "        idx_category = torch.tensor(self.categories.index(category), dtype=torch.long)\n",
    "        ls = len(sentence)\n",
    "        for i in range(self.size_of_longest_sample):\n",
    "            if i < ls:\n",
    "                idx_sentence[i] = self.word_to_index[sentence[i]]\n",
    "            else:\n",
    "                idx_sentence[i] = self.word_to_index['<NS>']\n",
    "\n",
    "        return idx_sentence, idx_category\n",
    "    \n",
    "    \n",
    "# TODO: INVESTIGAR Y HACER UN BUEN PREPROCESAMIENTO!!\n",
    "class PreprocessBrown(object):\n",
    "    def __call__(self, samples):\n",
    "        samples_processed = []\n",
    "        for sample in samples:\n",
    "            text = ' '.join(sample[0])\n",
    "            text = text.lower()\n",
    "            text.replace('\\n', ' ')\n",
    "            text = re.sub('[^a-z ]+', '', text)\n",
    "            samples_processed.append(([w for w in text.split() if w != ''], sample[1]))\n",
    "        return samples_processed    \n",
    "    \n",
    "\n",
    "train_dataset = BrownClassificationDataset('./', PreprocessBrown(), train=True)\n",
    "val_dataset = BrownClassificationDataset('./', PreprocessBrown(), train=True)\n",
    "test_dataset = BrownClassificationDataset('./', PreprocessBrown(), train=False)\n",
    "\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "\n",
    "val_size = .02\n",
    "batch_size = 918\n",
    "\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler=SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "                \n",
    "    return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainClassifier(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100, lm='CBOW'):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            scores = model(x) # Forward pass\n",
    "            loss = model.loss(scores,y) # Backward pass\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d, loss: %.4f' % (e, t, num_correct, num_samples, loss.item()))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearSoftmaxClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_categories):\n",
    "        super(LinearSoftmaxClassifier,self).__init__()\n",
    "        self.linear = nn.Embedding(vocab_size, n_categories)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        score = self.linear(sentence).mean(dim=1)\n",
    "        return score\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        return m(scores,target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "n_categories = len(train_dataset.categories)\n",
    "model = LinearSoftmaxClassifier(vocab_size, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Accuracy: 0/918, loss: 3.4561\n",
      "Epoch: 1, Iteration: 0, Accuracy: 0/918, loss: 3.4444\n",
      "Epoch: 2, Iteration: 0, Accuracy: 0/918, loss: 3.4971\n",
      "Epoch: 3, Iteration: 0, Accuracy: 0/918, loss: 3.4490\n",
      "Epoch: 4, Iteration: 0, Accuracy: 0/918, loss: 3.4957\n",
      "Epoch: 5, Iteration: 0, Accuracy: 0/918, loss: 3.4712\n",
      "Epoch: 6, Iteration: 0, Accuracy: 0/918, loss: 3.4821\n",
      "Epoch: 7, Iteration: 0, Accuracy: 0/918, loss: 3.4626\n",
      "Epoch: 8, Iteration: 0, Accuracy: 0/918, loss: 3.4708\n",
      "Epoch: 9, Iteration: 0, Accuracy: 0/918, loss: 3.4894\n",
      "Final accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long, \n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 50 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-6 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainClassifier(model, data, epochs, learning_rate, sample_loss_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda forma (con extracción del significado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainWordVectors import *\n",
    "\n",
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long, \n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 5 # Cantidad de epochs\n",
    "sample_loss_every = 200 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-2 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "embeddings = TrainWord2Vec(model, data, epochs, learning_rate, sample_loss_every, lm='CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownClassificationDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, preprocessing, train=True):\n",
    "        \n",
    "        self.root = root\n",
    "        \n",
    "        file_train = open(root + 'train.txt', 'r')\n",
    "        file_test = open(root + 'test.txt', 'r')\n",
    "        text_train = file_train.read()\n",
    "        text_test = file_test.read()\n",
    "        file_train.close()\n",
    "        file_test.close()\n",
    "\n",
    "        samples = []\n",
    "        vocab = []\n",
    "        \n",
    "        if train:\n",
    "            text = text_train.split('<ENDLABEL>')\n",
    "            extended_text = text_test.split('<ENDLABEL>')\n",
    "        else:\n",
    "            text = text_test.split('<ENDLABEL>')\n",
    "            extended_text = text_train.split('<ENDLABEL>')\n",
    "            \n",
    "        for i in range(len(text)-1):\n",
    "            sample = text[i].split('<BEGINLABEL>')\n",
    "            text_splitted = sample[0].split('<TS>')\n",
    "            samples.append((text_splitted, sample[1]))\n",
    "        \n",
    "        if preprocessing:\n",
    "            samples_preprocessed = preprocessing(samples)\n",
    "        else:\n",
    "            samples_preprocessed = samples\n",
    "            \n",
    "        vocab.append([sample[0] for sample in samples_preprocessed])\n",
    "        vocab.append([extended_text[i].split('<BEGINLABEL>')[0].split('<TS>') for i in range(len(extended_text)-1)])\n",
    "        \n",
    "        self.samples = samples_preprocessed\n",
    "        it = itertools.chain.from_iterable\n",
    "        self.vocabulary = set(it(list(it(vocab))))\n",
    "        self.vocabulary.add('<NS>')\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        \n",
    "        self.size_of_longest_sample = max([len(sample[0]) for sample in self.samples])\n",
    "        self.categories = list(set([sample[1] for sample in self.samples]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, category = self.samples[idx]\n",
    "        idx_sentence = torch.empty(self.size_of_longest_sample, dtype=torch.long)\n",
    "        idx_category = torch.tensor(self.categories.index(category), dtype=torch.long)\n",
    "        ls = len(sentence)\n",
    "        for i in range(self.size_of_longest_sample):\n",
    "            if i < ls:\n",
    "                idx_sentence[i] = self.word_to_index[sentence[i]]\n",
    "            else:\n",
    "                idx_sentence[i] = self.word_to_index['<NS>']\n",
    "\n",
    "        return idx_sentence, idx_category\n",
    "    \n",
    "    \n",
    "# TODO: INVESTIGAR Y HACER UN BUEN PREPROCESAMIENTO!!\n",
    "class PreprocessBrown(object):\n",
    "    def __call__(self, samples):\n",
    "        samples_processed = []\n",
    "        for sample in samples:\n",
    "            text = ' '.join(sample[0])\n",
    "            text = text.lower()\n",
    "            text.replace('\\n', ' ')\n",
    "            text = re.sub('[^a-z ]+', '', text)\n",
    "            samples_processed.append(([w for w in text.split() if w != ''], sample[1]))\n",
    "        return samples_processed    \n",
    "    \n",
    "\n",
    "train_dataset = BrownClassificationDataset('./', PreprocessBrown(), train=True)\n",
    "val_dataset = BrownClassificationDataset('./', PreprocessBrown(), train=True)\n",
    "test_dataset = BrownClassificationDataset('./', PreprocessBrown(), train=False)\n",
    "\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "\n",
    "val_size = .02\n",
    "batch_size = 918\n",
    "\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler=SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device, input_dtype, target_dtype):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=input_dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "                \n",
    "    return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def TrainClassifier(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100, lm='CBOW'):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            scores = model(x) # Forward pass\n",
    "            loss = model.loss(scores,y) # Backward pass\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d, loss: %.4f' % (e, t, num_correct, num_samples, loss.item()))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device, input_dtype, target_dtype)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearSoftmaxClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_categories):\n",
    "        super(LinearSoftmaxClassifier,self).__init__()\n",
    "        self.linear = nn.Embedding(vocab_size, n_categories)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        score = self.linear(sentence).mean(dim=1)\n",
    "        return score\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        return m(scores,target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "n_categories = len(train_dataset.categories)\n",
    "model = LinearSoftmaxClassifier(vocab_size, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device\n",
    "\n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.long, \n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 10 # Cantidad de epochs\n",
    "sample_loss_every = 50 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-6 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = TrainClassifier(model, data, epochs, learning_rate, sample_loss_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
