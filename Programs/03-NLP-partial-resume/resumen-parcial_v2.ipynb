{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen Parcial de NLP\n",
    "\n",
    "Este resumen muestra una de las metodologías utilizadas actualmente para diseñar un algoritmo de NLP. En este caso, la tarea a resolver será la de clasificación de texto. \n",
    "\n",
    "Esta metodología consiste en lo siguiente: para una tarea determinada, primero se realiza una extracción del significado de los elementos que componen el texto y luego se diseña un modelo que toma como entrada dicha representación del significado. Este modelo es el que realiza la tarea propiamente dicha, pero no lo hace sobre los símbolos que componen el texto original, sino sobre una representación de los mismos en la que pueden verse algunas características semánticas entre ellas. \n",
    "\n",
    "Por ejemplo, para el [corpus de texto *Brown*](https://www.nltk.org/book/ch02.html), que contiene textos de 15 categorías diferentes (news, editorial, reviews, religion, hobbies, lore, belles lettres, government, learned, fiction, mystery, science fiction, adventure, romanc, humor), se buscará realizar un algoritmo que clasifique una frase en alguna de estas categorías. Este procedimiento se realiza de dos maneras distintas:\n",
    "\n",
    "* Se entrena un clasificador de una capa (con salida Softmax) que tiene como entrada una frase, y como salida la probabilidad de cada una de las categorías mencionadas anteriormente.\n",
    "\n",
    "* Se entrena el mismo clasificador que antes, con la diferencia que la entrada del modelo no son las palabras que componen la frase, sino la representación del significado de cada una de esas palabras (*word embedding*).\n",
    "\n",
    "**TODO: EXPLICAR UN POCO MEJOR QUE EL SIGNIFICADO SE EXTRAE CON UN MODELO DE LENGUAJE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/Documents/BecaNLP/Programs/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importamos PyTorch\n",
    "import torch\n",
    "\n",
    "# Importamos la libraría de utils de NLP\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from nlp_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prop = .2\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "categories = brown.categories()\n",
    "for c, category in enumerate(categories):\n",
    "    sents = brown.sents(categories=category)\n",
    "    categ_len = len(sents)\n",
    "    test_size = int(test_prop * categ_len)\n",
    "    train_size = categ_len - test_size\n",
    "    rand_idx = torch.randperm(categ_len)\n",
    "    for i in rand_idx[:train_size]:\n",
    "        train_samples.append((sents[i], '<BEGINLABEL>' + category + '<ENDLABEL>'))\n",
    "    for i in rand_idx[train_size:]:\n",
    "        test_samples.append((sents[i], '<BEGINLABEL>' + category + '<ENDLABEL>'))\n",
    "    \n",
    "train_file = open('train.txt', 'w+')\n",
    "test_file = open('test.txt', 'w+')\n",
    "\n",
    "for sample in train_samples:\n",
    "    text, label = sample\n",
    "    for i in range(len(text)-1):\n",
    "        train_file.write(text[i])\n",
    "        train_file.write('<TS>')\n",
    "    train_file.write(text[-1])\n",
    "    train_file.write(label)\n",
    "\n",
    "for sample in test_samples:\n",
    "    text, label = sample\n",
    "    for i in range(len(text)-1):\n",
    "        test_file.write(text[i])\n",
    "        test_file.write('<TS>')\n",
    "    test_file.write(text[-1])\n",
    "    test_file.write(label)\n",
    "    \n",
    "train_file.close()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open('train.txt', 'r')\n",
    "test_file = open('test.txt', 'r')\n",
    "\n",
    "train_s = []\n",
    "test_s = []\n",
    "\n",
    "train = train_file.read()\n",
    "train = train.split('<ENDLABEL>')\n",
    "for i in range(len(train)-1):\n",
    "    sample = train[i].split('<BEGINLABEL>')\n",
    "    train_s.append((sample[0].split('<TS>'), sample[1]))\n",
    "\n",
    "test = test_file.read()\n",
    "test = test.split('<ENDLABEL>')\n",
    "for i in range(len(test)-1):\n",
    "    sample = test[i].split('<BEGINLABEL>')\n",
    "    test_s.append((sample[0].split('<TS>'), sample[1]))\n",
    "\n",
    "train_file.close()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownClassificationDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, preprocessing, train=True):\n",
    "        \n",
    "        self.root = root\n",
    "        \n",
    "        if train:\n",
    "            file = open(root + 'train.txt', 'r')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "        else:\n",
    "            file = open(root + 'test.txt', 'r')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "\n",
    "        samples = []\n",
    "        text = text.split('<ENDLABEL>')\n",
    "        for i in range(len(text)-1):\n",
    "            sample = text[i].split('<BEGINLABEL>')\n",
    "            samples.append((sample[0].split('<TS>'), sample[1]))\n",
    "               \n",
    "        if preprocessing:\n",
    "            samples_preprocessed = preprocessing(samples)\n",
    "        else:\n",
    "            samples_preprocessed = samples\n",
    "            \n",
    "        self.samples = samples_preprocessed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    \n",
    "train_dataset = BrownClassificationDataset('./', None, train=True)\n",
    "val_dataset = BrownClassificationDataset('./', None, train=True)\n",
    "test_dataset = BrownClassificationDataset('./', None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45879\n",
      "[(['Why', ',', \"that's\", 'his', 'main', 'reason', 'for', 'making', 'the', 'dive', \"''\", '.'], 'adventure'), (['The', 'enemy', 'came', 'looming', 'around', 'a', 'bend', 'in', 'the', 'trail', 'and', 'Matsuo', 'took', 'a', 'hasty', 'shot', ',', 'then', 'fled', 'without', 'knowing', 'the', 'result', ',', 'ran', 'until', 'breath', 'was', 'a', 'pain', 'in', 'his', 'chest', 'and', 'his', 'legs', 'were', 'rubbery', '.'], 'adventure'), (['He', 'must', 'have', 'saturated', 'himself', 'in', 'the', 'drink', ',', 'for', 'the', 'bullet', 'not', 'to', 'shock', 'him', 'out', 'of', 'his', 'drunken', 'haze', '.'], 'adventure'), ([\"Montero's\", 'shot', 'had', 'caught', 'him', 'high', 'in', 'the', 'chest', ';', ';'], 'adventure'), (['``', \"Let's\", 'get', 'one', 'thing', 'straight', ',', 'you', 'and', 'me', '.'], 'adventure')]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera forma (sin extracción del significado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prop = .05\n",
    "test_prop = .15\n",
    "\n",
    "train, train_labels = [], []\n",
    "val, val_labels = [], []\n",
    "test, test_labels = [], []\n",
    "\n",
    "for c, category in enumerate(brown.categories()):\n",
    "    sents = brown.sents(categories=category)\n",
    "    categ_len = len(sents)\n",
    "    val_size = int(val_prop * categ_len)\n",
    "    test_size = int(test_prop * categ_len)\n",
    "    train_size = categ_len - val_size - test_size\n",
    "    rand_idx = torch.randperm(categ_len)\n",
    "    for i in rand_idx[:train_size]:\n",
    "        train.append(sents[i])\n",
    "        train_labels.append(c)\n",
    "    for i in rand_idx[train_size:(train_size+val_size)]:\n",
    "        val.append(sents[i])\n",
    "        val_labels.append(c)\n",
    "    for i in rand_idx[-test_size:]:\n",
    "        test.append(sents[i])\n",
    "        test_labels.append(c)\n",
    "\n",
    "train_samples, train_rand_idx = [], torch.randperm(len(train))\n",
    "val_samples, val_rand_idx = [], torch.randperm(len(val))\n",
    "test_samples, test_rand_idx = [], torch.randperm(len(test))\n",
    "\n",
    "for i in train_rand_idx:\n",
    "    train_samples.append((train[i], train_labels[i]))\n",
    "for i in val_rand_idx:\n",
    "    val_samples.append((val[i], val_labels[i]))\n",
    "for i in test_rand_idx:\n",
    "    test_samples.append((test[i], test_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_prop = .2\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for c, category in enumerate(brown.categories()):\n",
    "    sents = brown.sents(categories=category)\n",
    "    categ_len = len(sents)\n",
    "    test_size = int(test_prop * categ_len)\n",
    "    train_size = categ_len - test_size\n",
    "    rand_idx = torch.randperm(categ_len)\n",
    "    for i in rand_idx[:train_size]:\n",
    "        train_samples.append((sents[i], c))\n",
    "    for i in rand_idx[train_size:]:\n",
    "        test_samples.append((sents[i],c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.util.LazyCorpusLoader"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: INVESTIGAR Y HACER UN BUEN PREPROCESAMIENTO!!\n",
    "class PreprocessBrown(object):\n",
    "    def __call__(self,corpus_unpreproceced):\n",
    "        corpus = []\n",
    "        for sentence in corpus_unpreproceced:\n",
    "            text = ' '.join(sentence)\n",
    "            text = text.lower()\n",
    "            text.replace('\\n', ' ')\n",
    "            text = re.sub('[^a-z ]+', '', text)\n",
    "            corpus.append([w for w in text.split() if w != ''])\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda forma (con extracción del significado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
