{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen parcial de NLP\n",
    "\n",
    "La idea es hacer un pequeño resumen de lo que se vio hasta ahora de NLP. Esto incluiría:\n",
    "\n",
    "* Introducción al tema, ejemplos de tareas, estado del arte de cada una de ellas, y algo más.\n",
    "\n",
    "* Ciclo de trabajo en las tareas o en alguna de ellas (modelo de lenguaje, extracción de features en forma de significado, definición del modelo y cómo se miden los resultados).\n",
    "\n",
    "* Extracción de features con word embeddings. Ejemplos de algoritmos: frecuentistas, word2vec, GloVe, y no sé si alguno más.\n",
    "\n",
    "Dell primer capítulo hasta el 7 del [libro de Jurafsky](https://web.stanford.edu/~jurafsky/slp3/) hay mucha data sobre todo esto. Estaría bueno leerlo todo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a NLP\n",
    "\n",
    "Introducción al lenguaje natural y a las tareas de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciclo de trabajo en NLP (NLP y Machine Learning)\n",
    "\n",
    "El proceso de ejecución de un algoritmo típico de NLP fue cambiando a lo largo del tiempo. Hoy, la mayor parte de estos algoritmos se conforman de varias etapas:\n",
    "\n",
    "1. Etapa de codificación de las palabras. Esto era lo que más se estudiaba antes de que aparecieran las redes neuronales, y se refiere al proceso de pasar de tener un conjunto de palabras a tener una representación numérica de las mismas. El one-hot vector encoding y las tablas de significados (\"table meaning similarity\", como dice Manning en la lecture 1) como WordNet son ejemplos de este proceso de codificación. Esta representación se conoce como **denotacional**.\n",
    "No encontré por ningún lado que dijeran esto, pero la representación no tiene por qué ser de palabras, sino de gramas en general, es decir, de cualquier ente que tenga la característica de tener significado.\n",
    "\n",
    "2. Etapa de extracción del significado\n",
    "\n",
    "3. Etapa de ejecución de la tarea\n",
    "\n",
    "4. Etapa de evaluación de resultados.\n",
    "\n",
    "\n",
    "De acuerdo a los resultados que se fueron obteniendo, a lo largo de la historia se hizo hincapié en alguna de estas etapas en particular. De hecho, al principio había ciertas etapas que no existían (por ejemplo, extracción del significado). Por ejemplo, cuando todavía no se habían desarrollado modelos neuronales que funcionaran, ni había suficiente información como para realizar una representación densa del significado, se estudió mucho la codificación de las palabras según su significado. Google (hasta 2005) tenía *meaning similarity tables* en donde buscar las palabras con significados parecidos. De esta forma, se tenía una codificación parecida de palabras con significado similar. \n",
    "\n",
    "Después, los métodos frecuentistas distribucionales (matriz de coocurrencia, LSA, etc.) fueron tomando presencia dado que podían representar significados a partir de su contexto. Estos pertenencen a la etepa de extracción del significado, porque realizan una codificación automática de los one-hot vectors en un espacio en el que pueden medirse las características del significado. Acá también aparecen los modelos neuronales word2vec y toda esa historia. \n",
    "\n",
    "A las etapas de ejecución de la tarea y de evaluación de resultados siempre se les dio bola, porque es lo que realmente importa. Sin embargo, cuando aparecieron las NN, comenzaron a estudiarse con más cuidado y con muchas variaciones sobre lo mismo. Los transformers aparecen como consecuencia de esta etapa pero sirven para la etapa de extracción de significado. Siempre está la filosofía de \"cuanto mejor hagas la primera etapa de extracción de features, mejores resultados vas a tener\".\n",
    "\n",
    "Mi hipótesis es que actualmente se hace un modelo de lenguaje (en el sentido de estimar $P(o|c)$) para la extracción del significado y que antes, se hacía toda la tarea en un solo paso. Con los word2vec aparece la idea de hacer trasnfer learning para extraer el significado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning representation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
