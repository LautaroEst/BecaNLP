{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Word Vectors\n",
    "\n",
    "**Objetivo:** Representar en un espacio vectorial el significado de las palabras. \n",
    "\n",
    "En un contexto de aprendizaje, se va a buscar encontrar una representación que mejore la performance de una tarea específica. Por ejemplo, podemos hacer **predicción del lenguaje**: dado una palabra o serie de palabras, predigo cuál va salir a continuación. Eso se puede ver como un problema de clasificación en el que se tiene un conjunto $V = \\{ casa,\\; auto\\; el,\\; la\\} $ que representa el vocabulario y, dado una serie de palabras pertencecientes a $V$, se desea encontrar la siguiente palabra, también perteneciente a $V$, de la oración.\n",
    "\n",
    "Vamos a describir los algoritmos Word2vec que hacen esta tarea.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "\"*You should know a word by the company it keeps*\"\n",
    "\n",
    "Tanto para CBOW como para Skip-gram, la tarea está fija (y es la predicción del lenguaje). Lo que se va hacer es estimar la probabilidad $P(palabra|contexto)$ y medir la performance (definida como missclassification) con el set de validación. \n",
    "\n",
    "La diferencia entre los modelos está en que tengo diferentes entradas y salidas, y por lo tanto diferentes ejemplos de entrenamiento y diferentes modelos de $P(palabra|contexto)$.\n",
    "\n",
    "### CBOW\n",
    "\n",
    "1. La entrada va a ser una secuencia de $2m$ palabras y la salida va a ser una palabra sola.\n",
    "2. Los ejemplos de entrenamiento son contexto + palabra central, encontrados en un texto.\n",
    "3. La probabilidad $P(palabra|contexto)$ se va a calcular con una softmax y dos matrices $\\mathcal{U}$ y $\\mathcal{V}$.\n",
    "\n",
    "Concretamente:\n",
    "\n",
    "1. $m_c:(x_{c-m}, \\ldots, x_{c-1}, x_{c+1}, \\ldots, x_{c+m})$ es la muestra de entrada y $x_{c}$ es la muestra de salida. Se obtienen las muestras para todo $c$.\n",
    "2. Se obtienen $v_{c-m}=\\mathcal{V}x_{c-m}, \\ldots, v_{c+m}=\\mathcal{V}x_{c+m}$ y se calcula el promedio de ellos: \n",
    "$$\n",
    "\\hat{v} = \\frac{v_{c-m} + \\ldots + v_{c+m}}{2m}\n",
    "$$\n",
    "3. Se obtiene $z = \\mathcal{U}\\hat{v}$\n",
    "4. Se obtiene $p=\\hat{y}[j]$ con $\\hat{y} = softmax(z)$ y $j$ el índice en que $x_{c}=1$ como estimativo de la probabilidad $P(x_{c}|x_{c-m},\\ldots,x_{c-1},x_{c+1},\\ldots,x_{c+m};\\mathcal{U},\\mathcal{V})$\n",
    "5. Los parámetros de este modelo se estiman por ML o por minimización de la cross-entropy. Es exactamente la misma cuenta:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\mathcal{U}},\\hat{\\mathcal{V}} &= argmax_{\\mathcal{U},\\mathcal{V}} P(muestra) \\\\\n",
    "&= argmax_{\\mathcal{U},\\mathcal{V}} \\prod_{c=1}^{N}P(x_c|m_c)\\\\\n",
    "&= argmax_{\\mathcal{U},\\mathcal{V}} \\sum_{c=1}^{N}\\log P(x_c|m_c)\\\\\n",
    "&= argmax_{\\mathcal{U},\\mathcal{V}} \\sum_{c=1}^{N}\\log softmax(\\mathcal{U}mean(\\mathcal{V}m_c)^{\\mathbb{1}_{\\{ x_c\\}}}\\\\\n",
    "&= argmin_{\\mathcal{U},\\mathcal{V}} -\\sum_{c=1}^{N}\\sum_{i=1}^{|V|}y_i \\log softmax(\\mathcal{U}mean(\\mathcal{V}m_c)\\\\\n",
    "&= argmin_{\\mathcal{U},\\mathcal{V}}\\sum_{c=1}^{N}H(\\hat{y},y)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "con $y$ definida como: $y_i = 1$ si $x_c=i$ y 0 en otro caso, y $\\hat{y}= softmax(\\mathcal{U}mean(\\mathcal{V}m_c)$.\n",
    "\n",
    "**Disgresión computacional:** Me parece que la posta está en definir matemáticamente las matrices que se me dé la gana y después hacerlo eficiente en la práctica.\n",
    "\n",
    "La cuenta final de este modelo queda:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "```Python\n",
    "V_size = 100 # Tamaño del vocabulario\n",
    "n = 10 # Dimensión del espacio de wordvectors\n",
    "N = 1000 # Cantidad de muestras\n",
    "\n",
    "V = np.random.randn(n,V_size) # input matrix\n",
    "U = np.random.randn(V_size,n) # output matrix\n",
    "\n",
    "for c in range(X_train.shape[0]):\n",
    "    x_c = X_train[c,:] \n",
    "    m_c = X_train[c-m:c-1:c+1:c+m]\n",
    "\n",
    "    y_hat = np.softmax(U @ np.mean(V[:,m_c == 1]))\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
