{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebooks importantes**:\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/BecaNLP/01%20-%20WordVectors/WordVectors_v2.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/BecaNLP/01%20-%20WordVectors/WordVectors_v1.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/Cursos/cs224n-Natural-Language-Processing/assignments/assignment1/a1/exploring_word_vectors.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/Cursos/cs224n-Natural-Language-Processing/Notas-1-v1.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/Cursos/cs224n-Natural-Language-Processing/Vector-words/01%20-%20Notas_word_vector_v1.ipynb\n",
    "\n",
    "**Libro de Jurafsky**: https://web.stanford.edu/~jurafsky/slp3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del significado\n",
    "\n",
    "Las tareas de NLP implican tomar un texto (o un conjunto de textos) y obtener un resultado representado por algún objeto abstracto (categoría, número real, etc.).\n",
    "\n",
    "![alt text](nlp_task.png)\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "* **Clasificación de emails en SPAM o NO SPAM.** En este caso, la entrada va a ser el texto del email y la salida va a ser una de las dos posibles categorías \"SPAM\" o \"NO SPAM\".\n",
    "\n",
    "* **Sentiment analysis.** Esta tarea abarca muchas variantes, aunque la más común es la que consiste en clasificar un texto en \"positivo\", \"negativo\" o \"neutro\", dando lugar al mismo problema que antes (solo que en este caso existen más categorías). \n",
    "\n",
    "* **Predicción de texto.** Dada una serie de palabras, quiero predecir qué palabra voy a decir a continuación. También puede verse como un problema de clasificación, aunque con muchas más categorías. \n",
    "\n",
    "Algunas de estas tareas pueden resolverse (con un cierto porcentaje de efectividad) de manera directa, es decir, extrayendo características del texto ingresado, realizando algunos cálculos a partir de esas características y devolviendo la categoría a la que pertenecen según algún criterio.\n",
    "\n",
    "Durante muchos años se usó esta metodología, en todas sus variantes. Sin embargo, a medida que las tareas se complejizan cada vez es más necesario trabajar sobre la característica más importante del lenguaje: la semántica. Por lo tanto, surge esta idea de cuantificar el significado de las oraciones, las palabras o inclusive los conjuntos de caracteres que conforman un sufijo o un prefijo, y utilizar eso como punto de partida para resolver la tarea. \n",
    "\n",
    "![alt text](nlp_task2.png)\n",
    "\n",
    "La extracción del significado puede ser de distintas formas, pero la primera que vamos a ver es la que intenta representar el **significado de las palabras**. También hay formas de representar el significado de oraciones o inlcuso textos enteros. \n",
    "\n",
    "# Word Embeddings (Word Vectors)\n",
    "\n",
    "Desde un punto de vista de la *lexical semantic*, a la hora de considerar la representación del significado de una palabra, se pueden considerar varias cosas:\n",
    "\n",
    "* El sentido de la palabra\n",
    "* Los sinónimos de esa palabra\n",
    "* La similitud con otras palabras (similarity)\n",
    "* El parecido o la relación con otras palabras (relatednes)\n",
    "* Semantic Frames and Roles\n",
    "* Connotaciones semánticas (sentimientos).\n",
    "\n",
    "La idea de representar estas variables con un vector permite:\n",
    "\n",
    "* Diferentes \"direcciones\" de significado\n",
    "* Tener una noción de cercanía o lejanía entre palabras según su significado\n",
    "* Algo muy importante... se pueden entrenar automáticamente!!!\n",
    "\n",
    "La idea de representar con un vector el significado se llama *vector semantics* y comenzó en la década de 1950, pasando por varias etapas. VER https://web.stanford.edu/~jurafsky/slp3/6.pdf. En el curso de Stanford dice que en la era pre-deep learning se utilizaba la denotational semantic representation y después comienzan las representaciones distribucionales, que representan el significado de las palabras según su contexto.\n",
    "\n",
    "## Representación con vectores *one-hot* \n",
    "\n",
    "La primera representación del significado de las palabras es a partir de vectores *one-hot*. Es decir, dado un vocabulario $V$, de tamaño $|V|$, se define que\n",
    "\n",
    "$$\n",
    "h_j = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{|V|}\n",
    "$$\n",
    "\n",
    "es un vector *one-hot* con su j-ésima coordenada igual a 1, y cero en otro lado. Por ejemplo, para un vocabulario $V=\\{ casa, conjunto, el, diez, vacío, la, ellos, ellas, \\ldots \\}$, las palabras \"conjunto\" y \"vacío\" se representan por los vectores $h_2$ y $h_5$, respectivamente. También, puedo definir el significado de una oración a partir de la suma (o el promedio) de los diferentes vectores. \n",
    "\n",
    "Esta representación tiene muchas desventajas:\n",
    "\n",
    "* La única fuente de significado que contiene esta representación es tautológica. Es decir, el vector $h_2$ significa \"conjunto\", pero es lo único que significa.\n",
    "\n",
    "* No hay parecidos entre palabras, ya que son todos vectores ortogonales entre sí.\n",
    "\n",
    "* Las frases \"Esto es un perro, no un gato\" y \"Esto es un gato, no un perro\" tendrían el mismo significado con esta representación.\n",
    "\n",
    "* La longitud de los vectores suelen ser enormes, ya que el vocabulario también lo es.\n",
    "\n",
    "## Representación *sparse*: Matriz de co-ocurrencia\n",
    "\n",
    "\n",
    "\n",
    "## Representación *sparse*: TF-IDF\n",
    "\n",
    "## Representación *sparse*: Pointwise Mutual Information (PMI)\n",
    "\n",
    "## Representación densa: LSA (SVD)\n",
    "\n",
    "## Representación densa: Word2Vec\n",
    "\n",
    "## Representación densa: GloVe\n",
    "\n",
    "En el minuto 55 más o menos de la lecture 2 de stanford explica cómo llega desde el método de matriz de coocurrencia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
