{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebooks importantes**:\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/BecaNLP/01%20-%20WordVectors/WordVectors_v2.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/BecaNLP/01%20-%20WordVectors/WordVectors_v1.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/Cursos/cs224n-Natural-Language-Processing/assignments/assignment1/a1/exploring_word_vectors.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/Cursos/cs224n-Natural-Language-Processing/Notas-1-v1.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Documents/Cursos/cs224n-Natural-Language-Processing/Vector-words/01%20-%20Notas_word_vector_v1.ipynb\n",
    "\n",
    "**Libro de Jurafsky**: https://web.stanford.edu/~jurafsky/slp3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "NLP conforma un montón de tareas, algunas de las cuales son muy diversas entre sí. Sin embargo, muchas de ellas requieren un *procesamiento del significado* del lenguaje en forma inteligente, con lo cual vamos a empezar por dar una forma de representar el significado.\n",
    "\n",
    "![alt text](nlp_task.png)\n",
    "\n",
    "![alt text](nlp_task2.png)\n",
    "\n",
    "La extracción del significado puede ser de distintas formas, pero la primera que vamos a ver es la que intenta representar el **significado de las palabras**. También hay formas de representar el significado de oraciones o inlcuso textos enteros. \n",
    "\n",
    "# Word Embeddings (Word Vectors)\n",
    "\n",
    "Desde un punto de vista de la *lexical semantic*, a la hora de considerar la representación del significado de una palabra, se pueden considerar varias cosas:\n",
    "\n",
    "* El sentido de la palabra\n",
    "* Los sinónimos de esa palabra\n",
    "* La similitud con otras palabras (similarity)\n",
    "* El parecido o la relación con otras palabras (relatednes)\n",
    "* Semantic Frames and Roles\n",
    "* Connotaciones semánticas (sentimientos).\n",
    "\n",
    "La idea de representar estas variables con un vector permite:\n",
    "\n",
    "* Diferentes \"direcciones\" de significado\n",
    "* Tener una noción de cercanía o lejanía entre palabras según su significado\n",
    "* Algo muy importante... se pueden entrenar automáticamente!!!\n",
    "\n",
    "La idea de representar con un vector el significado se llama *vector semantics* y comenzó en la década de 1950, pasando por varias etapas. VER https://web.stanford.edu/~jurafsky/slp3/6.pdf. En el curso de Stanford dice que en la era pre-deep learning se utilizaba la denotational semantic representation y después comienzan las representaciones distribucionales, que representan el significado de las palabras según su contexto.\n",
    "\n",
    "## Representación con vectores *one-hot* \n",
    "\n",
    "La primera representación del significado de las palabras es a partir de vectores *one-hot*. Es decir, dado un vocabulario $V$, de tamaño $|V|$, se define que\n",
    "\n",
    "$$\n",
    "h_j = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{|V|}\n",
    "$$\n",
    "\n",
    "es un vector *one-hot* con su j-ésima coordenada igual a 1, y cero en otro lado. Por ejemplo, para un vocabulario $V=\\{ casa, conjunto, el, diez, vacío, la, ellos, ellas, \\ldots \\}$, las palabras \"conjunto\" y \"vacío\" se representan por los vectores $h_2$ y $h_5$, respectivamente. También, puedo definir el significado de una oración a partir de la suma (o el promedio) de los diferentes vectores. \n",
    "\n",
    "Esta representación tiene muchas desventajas:\n",
    "\n",
    "* La única fuente de significado que contiene esta representación es tautológica. Es decir, el vector $h_2$ significa \"conjunto\", pero es lo único que significa.\n",
    "\n",
    "* No hay parecidos entre palabras, ya que son todos vectores ortogonales entre sí.\n",
    "\n",
    "* Las frases \"Esto es un perro, no un gato\" y \"Esto es un gato, no un perro\" tendrían el mismo significado con esta representación.\n",
    "\n",
    "* La longitud de los vectores suelen ser enormes, ya que el vocabulario también lo es.\n",
    "\n",
    "## Representación *sparse*: Matriz de co-ocurrencia\n",
    "\n",
    "## Representación *sparse*: TF-IDF\n",
    "\n",
    "## Representación *sparse*: Pointwise Mutual Information (PMI)\n",
    "\n",
    "## Representación densa: LSA (SVD)\n",
    "\n",
    "## Representación densa: Word2Vec\n",
    "\n",
    "## Representación densa: GloVe\n",
    "\n",
    "En el minuto 55 más o menos de la lecture 2 de stanford explica cómo llega desde el método de matriz de coocurrencia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
