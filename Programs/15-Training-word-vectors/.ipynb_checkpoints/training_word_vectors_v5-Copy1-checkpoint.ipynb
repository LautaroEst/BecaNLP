{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NLPUtils import *\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('text8','r') as file:\n",
    "    corpus = file.read()\n",
    "    corpus = [corpus.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\tModel used: SkipGram\n",
      "    \tOptimization method: Stochastic Gradient Descent\n",
      "    \tLearning Rate: 0.9\n",
      "    \tNumber of epochs: 100\n",
      "    \tNumber of batches: 132854\n",
      "    \tNumber of samples per batch: 128\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 12.592589378356934\n",
      "Epoch: 1, Batch number: 100, Loss: 11.912079811096191\n",
      "Epoch: 1, Batch number: 200, Loss: 11.767644882202148\n",
      "Epoch: 1, Batch number: 300, Loss: 11.249785423278809\n",
      "Epoch: 1, Batch number: 400, Loss: 10.911205291748047\n",
      "Epoch: 1, Batch number: 500, Loss: 10.689004898071289\n",
      "Epoch: 1, Batch number: 600, Loss: 11.364336013793945\n",
      "Epoch: 1, Batch number: 700, Loss: 11.13320541381836\n",
      "Epoch: 1, Batch number: 800, Loss: 10.792654037475586\n",
      "Epoch: 1, Batch number: 900, Loss: 10.846970558166504\n",
      "Epoch: 1, Batch number: 1000, Loss: 10.805609703063965\n",
      "Epoch: 1, Batch number: 1100, Loss: 11.197412490844727\n",
      "Epoch: 1, Batch number: 1200, Loss: 10.595500946044922\n",
      "Epoch: 1, Batch number: 1300, Loss: 10.263575553894043\n",
      "Epoch: 1, Batch number: 1400, Loss: 11.014148712158203\n",
      "Epoch: 1, Batch number: 1500, Loss: 10.45898723602295\n",
      "Epoch: 1, Batch number: 1600, Loss: 10.582633018493652\n",
      "Epoch: 1, Batch number: 1700, Loss: 10.433961868286133\n",
      "Epoch: 1, Batch number: 1800, Loss: 10.232542991638184\n",
      "Epoch: 1, Batch number: 1900, Loss: 10.287493705749512\n",
      "Epoch: 1, Batch number: 2000, Loss: 10.225951194763184\n",
      "Epoch: 1, Batch number: 2100, Loss: 10.284497261047363\n",
      "Epoch: 1, Batch number: 2200, Loss: 9.842307090759277\n",
      "Epoch: 1, Batch number: 2300, Loss: 10.195769309997559\n",
      "Epoch: 1, Batch number: 2400, Loss: 10.13547420501709\n",
      "Epoch: 1, Batch number: 2500, Loss: 9.830700874328613\n",
      "Epoch: 1, Batch number: 2600, Loss: 10.06790828704834\n",
      "Epoch: 1, Batch number: 2700, Loss: 10.0191011428833\n",
      "Epoch: 1, Batch number: 2800, Loss: 9.746402740478516\n",
      "Epoch: 1, Batch number: 2900, Loss: 9.55278205871582\n",
      "Epoch: 1, Batch number: 3000, Loss: 10.134563446044922\n",
      "Epoch: 1, Batch number: 3100, Loss: 9.179839134216309\n",
      "Epoch: 1, Batch number: 3200, Loss: 9.418590545654297\n",
      "Epoch: 1, Batch number: 3300, Loss: 9.48208236694336\n",
      "Epoch: 1, Batch number: 3400, Loss: 9.897385597229004\n",
      "Epoch: 1, Batch number: 3500, Loss: 9.790679931640625\n",
      "Epoch: 1, Batch number: 3600, Loss: 9.625405311584473\n",
      "Epoch: 1, Batch number: 3700, Loss: 9.801619529724121\n",
      "Epoch: 1, Batch number: 3800, Loss: 9.652409553527832\n",
      "Epoch: 1, Batch number: 3900, Loss: 9.265228271484375\n",
      "Epoch: 1, Batch number: 4000, Loss: 9.293209075927734\n",
      "Epoch: 1, Batch number: 4100, Loss: 9.620012283325195\n",
      "Epoch: 1, Batch number: 4200, Loss: 9.241579055786133\n",
      "Epoch: 1, Batch number: 4300, Loss: 9.682528495788574\n",
      "Epoch: 1, Batch number: 4400, Loss: 9.79965877532959\n",
      "Epoch: 1, Batch number: 4500, Loss: 9.609410285949707\n",
      "Epoch: 1, Batch number: 4600, Loss: 9.370515823364258\n",
      "Epoch: 1, Batch number: 4700, Loss: 9.688528060913086\n",
      "Epoch: 1, Batch number: 4800, Loss: 9.567230224609375\n",
      "Epoch: 1, Batch number: 4900, Loss: 9.29733943939209\n",
      "Epoch: 1, Batch number: 5000, Loss: 9.141434669494629\n",
      "Epoch: 1, Batch number: 5100, Loss: 9.258527755737305\n",
      "Epoch: 1, Batch number: 5200, Loss: 9.45504093170166\n",
      "Epoch: 1, Batch number: 5300, Loss: 9.365876197814941\n",
      "Epoch: 1, Batch number: 5400, Loss: 9.529480934143066\n",
      "Epoch: 1, Batch number: 5500, Loss: 8.999293327331543\n",
      "Epoch: 1, Batch number: 5600, Loss: 8.970723152160645\n",
      "Epoch: 1, Batch number: 5700, Loss: 9.04184627532959\n",
      "Epoch: 1, Batch number: 5800, Loss: 9.194602966308594\n",
      "Epoch: 1, Batch number: 5900, Loss: 9.462087631225586\n",
      "Epoch: 1, Batch number: 6000, Loss: 9.614686965942383\n",
      "Epoch: 1, Batch number: 6100, Loss: 9.410310745239258\n",
      "Epoch: 1, Batch number: 6200, Loss: 9.56831169128418\n",
      "Epoch: 1, Batch number: 6300, Loss: 9.655435562133789\n",
      "Epoch: 1, Batch number: 6400, Loss: 8.94150447845459\n",
      "Epoch: 1, Batch number: 6500, Loss: 9.108490943908691\n",
      "Epoch: 1, Batch number: 6600, Loss: 9.302310943603516\n",
      "Epoch: 1, Batch number: 6700, Loss: 9.260235786437988\n",
      "Epoch: 1, Batch number: 6800, Loss: 9.013256072998047\n",
      "Epoch: 1, Batch number: 6900, Loss: 9.625773429870605\n",
      "Epoch: 1, Batch number: 7000, Loss: 9.258752822875977\n",
      "Epoch: 1, Batch number: 7100, Loss: 9.437320709228516\n",
      "Epoch: 1, Batch number: 7200, Loss: 9.183940887451172\n",
      "Epoch: 1, Batch number: 7300, Loss: 9.05278491973877\n",
      "Epoch: 1, Batch number: 7400, Loss: 9.068708419799805\n",
      "Epoch: 1, Batch number: 7500, Loss: 8.667396545410156\n",
      "Epoch: 1, Batch number: 7600, Loss: 9.391090393066406\n",
      "Epoch: 1, Batch number: 7700, Loss: 9.740601539611816\n",
      "Epoch: 1, Batch number: 7800, Loss: 8.828054428100586\n",
      "Epoch: 1, Batch number: 7900, Loss: 9.082996368408203\n",
      "Epoch: 1, Batch number: 8000, Loss: 9.094850540161133\n",
      "Epoch: 1, Batch number: 8100, Loss: 8.97293472290039\n",
      "Epoch: 1, Batch number: 8200, Loss: 8.845882415771484\n",
      "Epoch: 1, Batch number: 8300, Loss: 8.670473098754883\n",
      "Epoch: 1, Batch number: 8400, Loss: 9.223348617553711\n",
      "Epoch: 1, Batch number: 8500, Loss: 9.408269882202148\n",
      "Epoch: 1, Batch number: 8600, Loss: 9.183874130249023\n",
      "Epoch: 1, Batch number: 8700, Loss: 8.613999366760254\n",
      "Epoch: 1, Batch number: 8800, Loss: 9.168527603149414\n",
      "Epoch: 1, Batch number: 8900, Loss: 9.194190979003906\n",
      "Epoch: 1, Batch number: 9000, Loss: 9.194869041442871\n",
      "Epoch: 1, Batch number: 9100, Loss: 8.572846412658691\n",
      "Epoch: 1, Batch number: 9200, Loss: 8.913862228393555\n",
      "Epoch: 1, Batch number: 9300, Loss: 9.104430198669434\n",
      "Epoch: 1, Batch number: 9400, Loss: 8.830870628356934\n",
      "Epoch: 1, Batch number: 9500, Loss: 9.02275276184082\n",
      "Epoch: 1, Batch number: 9600, Loss: 8.949130058288574\n",
      "Epoch: 1, Batch number: 9700, Loss: 8.945876121520996\n",
      "Epoch: 1, Batch number: 9800, Loss: 8.946545600891113\n",
      "Epoch: 1, Batch number: 9900, Loss: 8.346468925476074\n",
      "Epoch: 1, Batch number: 10000, Loss: 8.654956817626953\n",
      "Epoch: 1, Batch number: 10100, Loss: 8.712204933166504\n",
      "Epoch: 1, Batch number: 10200, Loss: 9.052596092224121\n",
      "Epoch: 1, Batch number: 10300, Loss: 9.051812171936035\n",
      "Epoch: 1, Batch number: 10400, Loss: 8.694134712219238\n",
      "Epoch: 1, Batch number: 10500, Loss: 8.868335723876953\n",
      "Epoch: 1, Batch number: 10600, Loss: 8.76125717163086\n",
      "Epoch: 1, Batch number: 10700, Loss: 8.725067138671875\n",
      "Epoch: 1, Batch number: 10800, Loss: 9.657958984375\n",
      "Epoch: 1, Batch number: 10900, Loss: 8.874603271484375\n",
      "Epoch: 1, Batch number: 11000, Loss: 8.657444953918457\n",
      "Epoch: 1, Batch number: 11100, Loss: 8.838850975036621\n",
      "Epoch: 1, Batch number: 11200, Loss: 8.596697807312012\n",
      "Epoch: 1, Batch number: 11300, Loss: 8.799886703491211\n",
      "Epoch: 1, Batch number: 11400, Loss: 9.310403823852539\n",
      "Epoch: 1, Batch number: 11500, Loss: 8.702821731567383\n",
      "Epoch: 1, Batch number: 11600, Loss: 8.959959030151367\n",
      "Epoch: 1, Batch number: 11700, Loss: 8.675786972045898\n",
      "Epoch: 1, Batch number: 11800, Loss: 8.618745803833008\n",
      "Epoch: 1, Batch number: 11900, Loss: 8.732078552246094\n",
      "Epoch: 1, Batch number: 12000, Loss: 9.010990142822266\n",
      "Epoch: 1, Batch number: 12100, Loss: 7.960453987121582\n",
      "Epoch: 1, Batch number: 12200, Loss: 8.258106231689453\n",
      "Epoch: 1, Batch number: 12300, Loss: 8.042327880859375\n",
      "Epoch: 1, Batch number: 12400, Loss: 8.553339004516602\n",
      "Epoch: 1, Batch number: 12500, Loss: 8.707324028015137\n",
      "Epoch: 1, Batch number: 12600, Loss: 8.990251541137695\n",
      "Epoch: 1, Batch number: 12700, Loss: 8.882987022399902\n",
      "Epoch: 1, Batch number: 12800, Loss: 9.026351928710938\n",
      "Epoch: 1, Batch number: 12900, Loss: 8.369305610656738\n",
      "Epoch: 1, Batch number: 13000, Loss: 8.263127326965332\n",
      "Epoch: 1, Batch number: 13100, Loss: 8.520886421203613\n",
      "Epoch: 1, Batch number: 13200, Loss: 8.600800514221191\n",
      "Epoch: 1, Batch number: 13300, Loss: 8.36376667022705\n",
      "Epoch: 1, Batch number: 13400, Loss: 8.422537803649902\n",
      "Epoch: 1, Batch number: 13500, Loss: 9.324028015136719\n",
      "Epoch: 1, Batch number: 13600, Loss: 8.605749130249023\n",
      "Epoch: 1, Batch number: 13700, Loss: 8.5391206741333\n",
      "Epoch: 1, Batch number: 13800, Loss: 8.349095344543457\n",
      "Epoch: 1, Batch number: 13900, Loss: 8.653334617614746\n",
      "Epoch: 1, Batch number: 14000, Loss: 9.0370512008667\n",
      "Epoch: 1, Batch number: 14100, Loss: 8.166443824768066\n",
      "Epoch: 1, Batch number: 14200, Loss: 8.052850723266602\n",
      "Epoch: 1, Batch number: 14300, Loss: 8.524357795715332\n",
      "Epoch: 1, Batch number: 14400, Loss: 8.800541877746582\n",
      "Epoch: 1, Batch number: 14500, Loss: 8.554718971252441\n",
      "Epoch: 1, Batch number: 14600, Loss: 8.691781044006348\n",
      "Epoch: 1, Batch number: 14700, Loss: 8.18565845489502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch number: 14800, Loss: 8.817760467529297\n",
      "Epoch: 1, Batch number: 14900, Loss: 9.081283569335938\n",
      "Epoch: 1, Batch number: 15000, Loss: 8.57607650756836\n",
      "Epoch: 1, Batch number: 15100, Loss: 8.477394104003906\n",
      "Epoch: 1, Batch number: 15200, Loss: 9.043216705322266\n",
      "Epoch: 1, Batch number: 15300, Loss: 8.230592727661133\n",
      "Epoch: 1, Batch number: 15400, Loss: 8.61379337310791\n",
      "Epoch: 1, Batch number: 15500, Loss: 8.701645851135254\n",
      "Epoch: 1, Batch number: 15600, Loss: 8.921229362487793\n",
      "Epoch: 1, Batch number: 15700, Loss: 8.476524353027344\n",
      "Epoch: 1, Batch number: 15800, Loss: 8.340311050415039\n",
      "Epoch: 1, Batch number: 15900, Loss: 8.283858299255371\n",
      "Epoch: 1, Batch number: 16000, Loss: 8.501845359802246\n",
      "Epoch: 1, Batch number: 16100, Loss: 8.722813606262207\n",
      "Epoch: 1, Batch number: 16200, Loss: 8.297213554382324\n",
      "Epoch: 1, Batch number: 16300, Loss: 8.431227684020996\n",
      "Epoch: 1, Batch number: 16400, Loss: 8.374164581298828\n",
      "Epoch: 1, Batch number: 16500, Loss: 8.967323303222656\n",
      "Epoch: 1, Batch number: 16600, Loss: 7.969027519226074\n",
      "Epoch: 1, Batch number: 16700, Loss: 8.982674598693848\n",
      "Epoch: 1, Batch number: 16800, Loss: 8.489716529846191\n",
      "Epoch: 1, Batch number: 16900, Loss: 8.4819917678833\n",
      "Epoch: 1, Batch number: 17000, Loss: 8.244384765625\n",
      "Epoch: 1, Batch number: 17100, Loss: 8.218795776367188\n",
      "Epoch: 1, Batch number: 17200, Loss: 8.629055976867676\n",
      "Epoch: 1, Batch number: 17300, Loss: 8.060503005981445\n",
      "Epoch: 1, Batch number: 17400, Loss: 7.946547985076904\n",
      "Epoch: 1, Batch number: 17500, Loss: 8.69387149810791\n",
      "Epoch: 1, Batch number: 17600, Loss: 8.16437816619873\n",
      "Epoch: 1, Batch number: 17700, Loss: 8.350226402282715\n",
      "Epoch: 1, Batch number: 17800, Loss: 9.12210750579834\n",
      "Epoch: 1, Batch number: 17900, Loss: 8.456140518188477\n",
      "Epoch: 1, Batch number: 18000, Loss: 8.312078475952148\n",
      "Epoch: 1, Batch number: 18100, Loss: 8.788590431213379\n",
      "Epoch: 1, Batch number: 18200, Loss: 8.039073944091797\n",
      "Epoch: 1, Batch number: 18300, Loss: 8.62910270690918\n",
      "Epoch: 1, Batch number: 18400, Loss: 8.404511451721191\n",
      "Epoch: 1, Batch number: 18500, Loss: 8.234131813049316\n",
      "Epoch: 1, Batch number: 18600, Loss: 8.996703147888184\n",
      "Epoch: 1, Batch number: 18700, Loss: 8.47688102722168\n",
      "Epoch: 1, Batch number: 18800, Loss: 8.70716667175293\n",
      "Epoch: 1, Batch number: 18900, Loss: 8.449373245239258\n",
      "Epoch: 1, Batch number: 19000, Loss: 8.228161811828613\n",
      "Epoch: 1, Batch number: 19100, Loss: 8.466649055480957\n",
      "Epoch: 1, Batch number: 19200, Loss: 8.836638450622559\n",
      "Epoch: 1, Batch number: 19300, Loss: 8.647963523864746\n",
      "Epoch: 1, Batch number: 19400, Loss: 8.400644302368164\n",
      "Epoch: 1, Batch number: 19500, Loss: 8.202880859375\n",
      "Epoch: 1, Batch number: 19600, Loss: 8.767480850219727\n",
      "Epoch: 1, Batch number: 19700, Loss: 8.636713981628418\n",
      "Epoch: 1, Batch number: 19800, Loss: 8.238252639770508\n",
      "Epoch: 1, Batch number: 19900, Loss: 8.487338066101074\n",
      "Epoch: 1, Batch number: 20000, Loss: 8.540244102478027\n",
      "Epoch: 1, Batch number: 20100, Loss: 8.107721328735352\n",
      "Epoch: 1, Batch number: 20200, Loss: 8.310893058776855\n",
      "Epoch: 1, Batch number: 20300, Loss: 8.369415283203125\n",
      "Epoch: 1, Batch number: 20400, Loss: 7.92763090133667\n",
      "Epoch: 1, Batch number: 20500, Loss: 8.580585479736328\n",
      "Epoch: 1, Batch number: 20600, Loss: 8.321053504943848\n",
      "Epoch: 1, Batch number: 20700, Loss: 8.511697769165039\n",
      "Epoch: 1, Batch number: 20800, Loss: 8.65673828125\n",
      "Epoch: 1, Batch number: 20900, Loss: 8.172806739807129\n",
      "Epoch: 1, Batch number: 21000, Loss: 8.505864143371582\n",
      "Epoch: 1, Batch number: 21100, Loss: 8.144766807556152\n",
      "Epoch: 1, Batch number: 21200, Loss: 8.126757621765137\n",
      "Epoch: 1, Batch number: 21300, Loss: 8.0535888671875\n",
      "Epoch: 1, Batch number: 21400, Loss: 8.507039070129395\n",
      "Epoch: 1, Batch number: 21500, Loss: 8.330767631530762\n",
      "Epoch: 1, Batch number: 21600, Loss: 8.599669456481934\n",
      "Epoch: 1, Batch number: 21700, Loss: 8.119806289672852\n",
      "Epoch: 1, Batch number: 21800, Loss: 8.254945755004883\n",
      "Epoch: 1, Batch number: 21900, Loss: 8.181360244750977\n",
      "Epoch: 1, Batch number: 22000, Loss: 8.446144104003906\n",
      "Epoch: 1, Batch number: 22100, Loss: 8.39444637298584\n",
      "Epoch: 1, Batch number: 22200, Loss: 8.606043815612793\n",
      "Epoch: 1, Batch number: 22300, Loss: 8.824872016906738\n",
      "Epoch: 1, Batch number: 22400, Loss: 8.612849235534668\n",
      "Epoch: 1, Batch number: 22500, Loss: 8.583780288696289\n",
      "Epoch: 1, Batch number: 22600, Loss: 8.456005096435547\n",
      "Epoch: 1, Batch number: 22700, Loss: 8.44288444519043\n",
      "Epoch: 1, Batch number: 22800, Loss: 8.36810302734375\n",
      "Epoch: 1, Batch number: 22900, Loss: 8.567468643188477\n",
      "Epoch: 1, Batch number: 23000, Loss: 7.950659275054932\n",
      "Epoch: 1, Batch number: 23100, Loss: 8.105854988098145\n",
      "Epoch: 1, Batch number: 23200, Loss: 8.269735336303711\n",
      "Epoch: 1, Batch number: 23300, Loss: 8.503283500671387\n",
      "Epoch: 1, Batch number: 23400, Loss: 8.19129467010498\n",
      "Epoch: 1, Batch number: 23500, Loss: 8.48237419128418\n",
      "Epoch: 1, Batch number: 23600, Loss: 8.0635986328125\n",
      "Epoch: 1, Batch number: 23700, Loss: 8.135196685791016\n",
      "Epoch: 1, Batch number: 23800, Loss: 8.313247680664062\n",
      "Epoch: 1, Batch number: 23900, Loss: 8.510276794433594\n",
      "Epoch: 1, Batch number: 24000, Loss: 7.857339859008789\n",
      "Epoch: 1, Batch number: 24100, Loss: 8.368619918823242\n",
      "Epoch: 1, Batch number: 24200, Loss: 8.277850151062012\n",
      "Epoch: 1, Batch number: 24300, Loss: 8.590497016906738\n",
      "Epoch: 1, Batch number: 24400, Loss: 8.280316352844238\n",
      "Epoch: 1, Batch number: 24500, Loss: 8.176901817321777\n",
      "Epoch: 1, Batch number: 24600, Loss: 8.725775718688965\n",
      "Epoch: 1, Batch number: 24700, Loss: 7.837371826171875\n",
      "Epoch: 1, Batch number: 24800, Loss: 7.793613910675049\n",
      "Epoch: 1, Batch number: 24900, Loss: 7.969602108001709\n",
      "Epoch: 1, Batch number: 25000, Loss: 8.055392265319824\n",
      "Epoch: 1, Batch number: 25100, Loss: 8.015819549560547\n",
      "Epoch: 1, Batch number: 25200, Loss: 8.391785621643066\n",
      "Epoch: 1, Batch number: 25300, Loss: 8.301331520080566\n",
      "Epoch: 1, Batch number: 25400, Loss: 8.25289249420166\n",
      "Epoch: 1, Batch number: 25500, Loss: 8.442352294921875\n",
      "Epoch: 1, Batch number: 25600, Loss: 8.247091293334961\n",
      "Epoch: 1, Batch number: 25700, Loss: 8.449838638305664\n",
      "Epoch: 1, Batch number: 25800, Loss: 7.909939765930176\n",
      "Epoch: 1, Batch number: 25900, Loss: 8.245085716247559\n",
      "Epoch: 1, Batch number: 26000, Loss: 8.554563522338867\n",
      "Epoch: 1, Batch number: 26100, Loss: 8.070596694946289\n",
      "Epoch: 1, Batch number: 26200, Loss: 8.16111946105957\n",
      "Epoch: 1, Batch number: 26300, Loss: 8.21415901184082\n",
      "Epoch: 1, Batch number: 26400, Loss: 7.899728775024414\n",
      "Epoch: 1, Batch number: 26500, Loss: 8.259991645812988\n",
      "Epoch: 1, Batch number: 26600, Loss: 8.196924209594727\n",
      "Epoch: 1, Batch number: 26700, Loss: 7.853094100952148\n",
      "Epoch: 1, Batch number: 26800, Loss: 8.671906471252441\n"
     ]
    }
   ],
   "source": [
    "# Modelo de lenguaje:\n",
    "method = 'SkipGram'\n",
    "window_size = 2\n",
    "embedding_dim = 100\n",
    "\n",
    "# Parámetros de iteración:\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learning_rate = 9e-1\n",
    "sample_loss_every = 100\n",
    "use_gpu = 1\n",
    "\n",
    "embedding_layer, vocab, loss_history = SGDTrainWordVectors(corpus,lm=method,window_size=window_size,batch_size=batch_size,embedding_dim=embedding_dim,use_gpu=use_gpu,epochs=epochs,learning_rate=learning_rate,sample_loss_every=sample_loss_every)\n",
    "ax.plot(loss_history['iter'],loss_history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['anarchism', 'marxism', 'philosophy', \n",
    "         'critique', 'idea', 'god', 'human', \n",
    "         'man', 'love', 'king', 'queen']\n",
    "\n",
    "distance = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "weights = next(iter(embedding_layer.parameters())).data\n",
    "\n",
    "for word in words:\n",
    "    idx = vocab.token_to_index(word)\n",
    "    weights[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
