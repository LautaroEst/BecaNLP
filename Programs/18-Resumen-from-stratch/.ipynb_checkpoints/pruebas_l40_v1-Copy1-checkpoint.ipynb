{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de lenguaje Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram trainer created:\n",
      "Window size: 1\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 1\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 1\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 1\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 1\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 1\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 3\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 3\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 3\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 3\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 3\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 3\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 4\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 4\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 4\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 4\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 4\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 4\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 5\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 5\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 5\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 5\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 5\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 5\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 6\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 6\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 6\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 6\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 6\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 6\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 7\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram trainer created:\n",
      "Window size: 7\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 7\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 7\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 7\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 7\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n",
      "SkipGram trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 50\n",
      "SkipGram trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 100\n",
      "SkipGram trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 150\n",
      "SkipGram trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 200\n",
      "SkipGram trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 300\n",
      "SkipGram trainer created:\n",
      "Window size: 8\n",
      "Number of samples: 38728\n",
      "Vocabulary Size: 5365\n",
      "Number of batches: 76\n",
      "Number of samples per batch: 512\n",
      "\n",
      "Dispositivo seleccionado: cuda:0\n",
      "Dimensión del espacio de los embeddings: 400\n"
     ]
    }
   ],
   "source": [
    "#corpus = [['w1', 'w2', 'w3', 'w4'], ['w1', 'w3', 'w3', 'w3'], ['w1'], ['w1', 'w2', 'w3', 'w4', 'w1', 'w2', 'w3', 'w4']]\n",
    "corpus = GetTrainCorpus('./promptsl40.train')\n",
    "cutoff_freq = 0\n",
    "window_size_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "batch_size = 512\n",
    "\n",
    "state_dict = None\n",
    "device = 'cuda:1'\n",
    "paralelize = False\n",
    "embedding_dim_list = [50, 100, 150, 200, 300, 400]\n",
    "\n",
    "sk_trainers = []\n",
    "for window_size in window_size_list:\n",
    "    embedding_dim_trainers = []\n",
    "    for embedding_dim in embedding_dim_list:\n",
    "        sk_trainer = SkipGramTrainer(corpus, cutoff_freq, window_size, batch_size)\n",
    "        sk_trainer.InitModel(state_dict=state_dict, device=device, paralelize=paralelize, embedding_dim=embedding_dim)\n",
    "        embedding_dim_trainers.append(sk_trainer)\n",
    "    sk_trainers.append(embedding_dim_trainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 8030.67626953125\n",
      "Epoch: 2, Batch number: 24, Loss: 7778.5224609375\n",
      "Epoch: 3, Batch number: 48, Loss: 7660.98681640625\n",
      "Epoch: 4, Batch number: 72, Loss: 7413.61376953125\n",
      "Epoch: 6, Batch number: 20, Loss: 7072.8935546875\n",
      "Epoch: 7, Batch number: 44, Loss: 7066.8876953125\n",
      "Epoch: 8, Batch number: 68, Loss: 6857.48095703125\n",
      "Epoch: 10, Batch number: 16, Loss: 6450.0283203125\n",
      "Epoch: 11, Batch number: 40, Loss: 6592.77001953125\n",
      "Epoch: 12, Batch number: 64, Loss: 6526.9814453125\n",
      "Epoch: 14, Batch number: 12, Loss: 6293.771484375\n",
      "Epoch: 15, Batch number: 36, Loss: 6032.1875\n",
      "Epoch: 16, Batch number: 60, Loss: 6055.12646484375\n",
      "Epoch: 18, Batch number: 8, Loss: 5971.427734375\n",
      "Epoch: 19, Batch number: 32, Loss: 5938.4287109375\n",
      "Epoch: 20, Batch number: 56, Loss: 5773.2060546875\n",
      "Epoch: 22, Batch number: 4, Loss: 5810.70849609375\n",
      "Epoch: 23, Batch number: 28, Loss: 5572.33984375\n",
      "Epoch: 24, Batch number: 52, Loss: 5541.17822265625\n",
      "Epoch: 26, Batch number: 0, Loss: 5381.125\n",
      "Epoch: 27, Batch number: 24, Loss: 5354.6337890625\n",
      "Epoch: 28, Batch number: 48, Loss: 5342.388671875\n",
      "Epoch: 29, Batch number: 72, Loss: 5145.025390625\n",
      "Epoch: 31, Batch number: 20, Loss: 5106.45166015625\n",
      "Epoch: 32, Batch number: 44, Loss: 4910.5283203125\n",
      "Epoch: 33, Batch number: 68, Loss: 4941.44970703125\n",
      "Epoch: 35, Batch number: 16, Loss: 4954.43701171875\n",
      "Epoch: 36, Batch number: 40, Loss: 4888.822265625\n",
      "Epoch: 37, Batch number: 64, Loss: 4868.10595703125\n",
      "Epoch: 39, Batch number: 12, Loss: 4790.134765625\n",
      "Epoch: 40, Batch number: 36, Loss: 4671.8330078125\n",
      "Epoch: 41, Batch number: 60, Loss: 4691.77197265625\n",
      "Epoch: 43, Batch number: 8, Loss: 4681.60888671875\n",
      "Epoch: 44, Batch number: 32, Loss: 4756.85302734375\n",
      "Epoch: 45, Batch number: 56, Loss: 4816.7080078125\n",
      "Epoch: 47, Batch number: 4, Loss: 4490.31103515625\n",
      "Epoch: 48, Batch number: 28, Loss: 4553.6455078125\n",
      "Epoch: 49, Batch number: 52, Loss: 4503.18017578125\n",
      "Epoch: 51, Batch number: 0, Loss: 4292.8984375\n",
      "Epoch: 52, Batch number: 24, Loss: 4648.8232421875\n",
      "Epoch: 53, Batch number: 48, Loss: 4507.79638671875\n",
      "Epoch: 54, Batch number: 72, Loss: 4520.5693359375\n",
      "Epoch: 56, Batch number: 20, Loss: 4455.27783203125\n",
      "Epoch: 57, Batch number: 44, Loss: 4345.00732421875\n",
      "Epoch: 58, Batch number: 68, Loss: 4274.89599609375\n",
      "Epoch: 60, Batch number: 16, Loss: 4209.162109375\n",
      "Epoch: 61, Batch number: 40, Loss: 4319.732421875\n",
      "Epoch: 62, Batch number: 64, Loss: 4383.69921875\n",
      "Epoch: 64, Batch number: 12, Loss: 4256.9677734375\n",
      "Epoch: 65, Batch number: 36, Loss: 4258.31884765625\n",
      "Epoch: 66, Batch number: 60, Loss: 4181.224609375\n",
      "Epoch: 68, Batch number: 8, Loss: 4148.50439453125\n",
      "Epoch: 69, Batch number: 32, Loss: 4187.64892578125\n",
      "Epoch: 70, Batch number: 56, Loss: 4223.9716796875\n",
      "Epoch: 72, Batch number: 4, Loss: 4081.32275390625\n",
      "Epoch: 73, Batch number: 28, Loss: 4098.20556640625\n",
      "Epoch: 74, Batch number: 52, Loss: 4155.302734375\n",
      "Epoch: 76, Batch number: 0, Loss: 3962.1572265625\n",
      "Epoch: 77, Batch number: 24, Loss: 3958.1630859375\n",
      "Epoch: 78, Batch number: 48, Loss: 3930.902587890625\n",
      "Epoch: 79, Batch number: 72, Loss: 4108.2763671875\n",
      "Epoch: 81, Batch number: 20, Loss: 4023.053466796875\n",
      "Epoch: 82, Batch number: 44, Loss: 3958.932373046875\n",
      "Epoch: 83, Batch number: 68, Loss: 3945.128173828125\n",
      "Epoch: 85, Batch number: 16, Loss: 3874.9912109375\n",
      "Epoch: 86, Batch number: 40, Loss: 3888.8828125\n",
      "Epoch: 87, Batch number: 64, Loss: 3950.6396484375\n",
      "Epoch: 89, Batch number: 12, Loss: 3785.42236328125\n",
      "Epoch: 90, Batch number: 36, Loss: 3757.749267578125\n",
      "Epoch: 91, Batch number: 60, Loss: 3913.876708984375\n",
      "Epoch: 93, Batch number: 8, Loss: 3804.538330078125\n",
      "Epoch: 94, Batch number: 32, Loss: 3892.53173828125\n",
      "Epoch: 95, Batch number: 56, Loss: 3808.99609375\n",
      "Epoch: 97, Batch number: 4, Loss: 3837.760986328125\n",
      "Epoch: 98, Batch number: 28, Loss: 3760.7685546875\n",
      "Epoch: 99, Batch number: 52, Loss: 3802.89111328125\n",
      "Epoch: 101, Batch number: 0, Loss: 3791.451416015625\n",
      "Epoch: 102, Batch number: 24, Loss: 3750.664794921875\n",
      "Epoch: 103, Batch number: 48, Loss: 3780.292724609375\n",
      "Epoch: 104, Batch number: 72, Loss: 3732.308349609375\n",
      "Epoch: 106, Batch number: 20, Loss: 3780.322509765625\n",
      "Epoch: 107, Batch number: 44, Loss: 3824.185546875\n",
      "Epoch: 108, Batch number: 68, Loss: 3712.94384765625\n",
      "Epoch: 110, Batch number: 16, Loss: 3757.26171875\n",
      "Epoch: 111, Batch number: 40, Loss: 3699.35498046875\n",
      "Epoch: 112, Batch number: 64, Loss: 3718.945556640625\n",
      "Epoch: 114, Batch number: 12, Loss: 3643.82080078125\n",
      "Epoch: 115, Batch number: 36, Loss: 3640.858154296875\n",
      "Epoch: 116, Batch number: 60, Loss: 3718.76513671875\n",
      "Epoch: 118, Batch number: 8, Loss: 3682.51708984375\n",
      "Epoch: 119, Batch number: 32, Loss: 3618.052490234375\n",
      "Epoch: 120, Batch number: 56, Loss: 3682.720458984375\n",
      "Epoch: 122, Batch number: 4, Loss: 3730.364013671875\n",
      "Epoch: 123, Batch number: 28, Loss: 3514.564208984375\n",
      "Epoch: 124, Batch number: 52, Loss: 3570.629150390625\n",
      "Epoch: 126, Batch number: 0, Loss: 3424.301025390625\n",
      "Epoch: 127, Batch number: 24, Loss: 3622.466552734375\n",
      "Epoch: 128, Batch number: 48, Loss: 3528.28076171875\n",
      "Epoch: 129, Batch number: 72, Loss: 3552.92822265625\n",
      "Epoch: 131, Batch number: 20, Loss: 3637.2099609375\n",
      "Epoch: 132, Batch number: 44, Loss: 3682.784912109375\n",
      "Epoch: 133, Batch number: 68, Loss: 3802.80908203125\n",
      "Epoch: 135, Batch number: 16, Loss: 3552.494384765625\n",
      "Epoch: 136, Batch number: 40, Loss: 3656.853759765625\n",
      "Epoch: 137, Batch number: 64, Loss: 3525.908203125\n",
      "Epoch: 139, Batch number: 12, Loss: 3601.952880859375\n",
      "Epoch: 140, Batch number: 36, Loss: 3577.9970703125\n",
      "Epoch: 141, Batch number: 60, Loss: 3442.78857421875\n",
      "Epoch: 143, Batch number: 8, Loss: 3548.272705078125\n",
      "Epoch: 144, Batch number: 32, Loss: 3472.285400390625\n",
      "Epoch: 145, Batch number: 56, Loss: 3386.102294921875\n",
      "Epoch: 147, Batch number: 4, Loss: 3534.35595703125\n",
      "Epoch: 148, Batch number: 28, Loss: 3314.48828125\n",
      "Epoch: 149, Batch number: 52, Loss: 3501.5361328125\n",
      "Epoch: 151, Batch number: 0, Loss: 3541.681640625\n",
      "Epoch: 152, Batch number: 24, Loss: 3456.283935546875\n",
      "Epoch: 153, Batch number: 48, Loss: 3423.515869140625\n",
      "Epoch: 154, Batch number: 72, Loss: 3607.013671875\n",
      "Epoch: 156, Batch number: 20, Loss: 3439.2119140625\n",
      "Epoch: 157, Batch number: 44, Loss: 3423.197509765625\n",
      "Epoch: 158, Batch number: 68, Loss: 3480.110595703125\n",
      "Epoch: 160, Batch number: 16, Loss: 3360.38525390625\n",
      "Epoch: 161, Batch number: 40, Loss: 3464.736328125\n",
      "Epoch: 162, Batch number: 64, Loss: 3548.638671875\n",
      "Epoch: 164, Batch number: 12, Loss: 3407.931396484375\n",
      "Epoch: 165, Batch number: 36, Loss: 3474.078369140625\n",
      "Epoch: 166, Batch number: 60, Loss: 3453.477294921875\n",
      "Epoch: 168, Batch number: 8, Loss: 3437.499267578125\n",
      "Epoch: 169, Batch number: 32, Loss: 3417.557861328125\n",
      "Epoch: 170, Batch number: 56, Loss: 3403.14501953125\n",
      "Epoch: 172, Batch number: 4, Loss: 3355.6923828125\n",
      "Epoch: 173, Batch number: 28, Loss: 3349.841796875\n",
      "Epoch: 174, Batch number: 52, Loss: 3368.161865234375\n",
      "Epoch: 176, Batch number: 0, Loss: 3420.617919921875\n",
      "Epoch: 177, Batch number: 24, Loss: 3422.865966796875\n",
      "Epoch: 178, Batch number: 48, Loss: 3645.498779296875\n",
      "Epoch: 179, Batch number: 72, Loss: 3403.0009765625\n",
      "Epoch: 181, Batch number: 20, Loss: 3358.42529296875\n",
      "Epoch: 182, Batch number: 44, Loss: 3372.947265625\n",
      "Epoch: 183, Batch number: 68, Loss: 3469.818603515625\n",
      "Epoch: 185, Batch number: 16, Loss: 3512.7158203125\n",
      "Epoch: 186, Batch number: 40, Loss: 3547.75927734375\n",
      "Epoch: 187, Batch number: 64, Loss: 3345.5419921875\n",
      "Epoch: 189, Batch number: 12, Loss: 3441.7158203125\n",
      "Epoch: 190, Batch number: 36, Loss: 3274.949462890625\n",
      "Epoch: 191, Batch number: 60, Loss: 3438.067138671875\n",
      "Epoch: 193, Batch number: 8, Loss: 3279.47412109375\n",
      "Epoch: 194, Batch number: 32, Loss: 3507.408447265625\n",
      "Epoch: 195, Batch number: 56, Loss: 3376.878173828125\n",
      "Epoch: 197, Batch number: 4, Loss: 3271.62451171875\n",
      "Epoch: 198, Batch number: 28, Loss: 3355.768310546875\n",
      "Epoch: 199, Batch number: 52, Loss: 3497.07763671875\n",
      "Epoch: 201, Batch number: 0, Loss: 3253.65380859375\n",
      "Epoch: 202, Batch number: 24, Loss: 3284.871337890625\n",
      "Epoch: 203, Batch number: 48, Loss: 3322.598388671875\n",
      "Epoch: 204, Batch number: 72, Loss: 3209.5654296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 206, Batch number: 20, Loss: 3356.16845703125\n",
      "Epoch: 207, Batch number: 44, Loss: 3363.999267578125\n",
      "Epoch: 208, Batch number: 68, Loss: 3426.956787109375\n",
      "Epoch: 210, Batch number: 16, Loss: 3460.0888671875\n",
      "Epoch: 211, Batch number: 40, Loss: 3441.960693359375\n",
      "Epoch: 212, Batch number: 64, Loss: 3343.122314453125\n",
      "Epoch: 214, Batch number: 12, Loss: 3322.14794921875\n",
      "Epoch: 215, Batch number: 36, Loss: 3223.489990234375\n",
      "Epoch: 216, Batch number: 60, Loss: 3274.672607421875\n",
      "Epoch: 218, Batch number: 8, Loss: 3516.762451171875\n",
      "Epoch: 219, Batch number: 32, Loss: 3451.792724609375\n",
      "Epoch: 220, Batch number: 56, Loss: 3235.129150390625\n",
      "Epoch: 222, Batch number: 4, Loss: 3147.301513671875\n",
      "Epoch: 223, Batch number: 28, Loss: 3229.815673828125\n",
      "Epoch: 224, Batch number: 52, Loss: 3402.410888671875\n",
      "Epoch: 226, Batch number: 0, Loss: 3356.888427734375\n",
      "Epoch: 227, Batch number: 24, Loss: 3182.70361328125\n",
      "Epoch: 228, Batch number: 48, Loss: 3385.10498046875\n",
      "Epoch: 229, Batch number: 72, Loss: 3416.743408203125\n",
      "Epoch: 231, Batch number: 20, Loss: 3298.094970703125\n",
      "Epoch: 232, Batch number: 44, Loss: 3319.271240234375\n",
      "Epoch: 233, Batch number: 68, Loss: 3260.841064453125\n",
      "Epoch: 235, Batch number: 16, Loss: 3300.4638671875\n",
      "Epoch: 236, Batch number: 40, Loss: 3337.07861328125\n",
      "Epoch: 237, Batch number: 64, Loss: 3435.837158203125\n",
      "Epoch: 239, Batch number: 12, Loss: 3252.559326171875\n",
      "Epoch: 240, Batch number: 36, Loss: 3363.005126953125\n",
      "Epoch: 241, Batch number: 60, Loss: 3280.227783203125\n",
      "Epoch: 243, Batch number: 8, Loss: 3287.0458984375\n",
      "Epoch: 244, Batch number: 32, Loss: 3405.844482421875\n",
      "Epoch: 245, Batch number: 56, Loss: 3358.458984375\n",
      "Epoch: 247, Batch number: 4, Loss: 3368.865966796875\n",
      "Epoch: 248, Batch number: 28, Loss: 3371.89501953125\n",
      "Epoch: 249, Batch number: 52, Loss: 3314.1298828125\n",
      "Epoch: 251, Batch number: 0, Loss: 3307.158935546875\n",
      "Epoch: 252, Batch number: 24, Loss: 3268.47998046875\n",
      "Epoch: 253, Batch number: 48, Loss: 3214.616943359375\n",
      "Epoch: 254, Batch number: 72, Loss: 3342.978515625\n",
      "Epoch: 256, Batch number: 20, Loss: 3377.984619140625\n",
      "Epoch: 257, Batch number: 44, Loss: 3196.9697265625\n",
      "Epoch: 258, Batch number: 68, Loss: 3207.750732421875\n",
      "Epoch: 260, Batch number: 16, Loss: 3207.325439453125\n",
      "Epoch: 261, Batch number: 40, Loss: 3266.526123046875\n",
      "Epoch: 262, Batch number: 64, Loss: 3286.60009765625\n",
      "Epoch: 264, Batch number: 12, Loss: 3223.1591796875\n",
      "Epoch: 265, Batch number: 36, Loss: 3291.171630859375\n",
      "Epoch: 266, Batch number: 60, Loss: 3325.60888671875\n",
      "Epoch: 268, Batch number: 8, Loss: 3142.953369140625\n",
      "Epoch: 269, Batch number: 32, Loss: 3291.135009765625\n",
      "Epoch: 270, Batch number: 56, Loss: 3260.869873046875\n",
      "Epoch: 272, Batch number: 4, Loss: 3301.208251953125\n",
      "Epoch: 273, Batch number: 28, Loss: 3301.130615234375\n",
      "Epoch: 274, Batch number: 52, Loss: 3297.0224609375\n",
      "Epoch: 276, Batch number: 0, Loss: 3330.247314453125\n",
      "Epoch: 277, Batch number: 24, Loss: 3260.109375\n",
      "Epoch: 278, Batch number: 48, Loss: 3262.712890625\n",
      "Epoch: 279, Batch number: 72, Loss: 3281.256103515625\n",
      "Epoch: 281, Batch number: 20, Loss: 3265.68896484375\n",
      "Epoch: 282, Batch number: 44, Loss: 3364.178955078125\n",
      "Epoch: 283, Batch number: 68, Loss: 3347.483154296875\n",
      "Epoch: 285, Batch number: 16, Loss: 3168.556640625\n",
      "Epoch: 286, Batch number: 40, Loss: 3349.20068359375\n",
      "Epoch: 287, Batch number: 64, Loss: 3144.81201171875\n",
      "Epoch: 289, Batch number: 12, Loss: 3171.553466796875\n",
      "Epoch: 290, Batch number: 36, Loss: 3143.82373046875\n",
      "Epoch: 291, Batch number: 60, Loss: 3434.93212890625\n",
      "Epoch: 293, Batch number: 8, Loss: 3265.727783203125\n",
      "Epoch: 294, Batch number: 32, Loss: 3245.06103515625\n",
      "Epoch: 295, Batch number: 56, Loss: 3112.270263671875\n",
      "Epoch: 297, Batch number: 4, Loss: 3173.082763671875\n",
      "Epoch: 298, Batch number: 28, Loss: 3066.451416015625\n",
      "Epoch: 299, Batch number: 52, Loss: 3163.414306640625\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 7935.23486328125\n",
      "Epoch: 2, Batch number: 24, Loss: 7697.95166015625\n",
      "Epoch: 3, Batch number: 48, Loss: 7321.8505859375\n",
      "Epoch: 4, Batch number: 72, Loss: 7151.8583984375\n",
      "Epoch: 6, Batch number: 20, Loss: 6619.37255859375\n",
      "Epoch: 7, Batch number: 44, Loss: 6667.4794921875\n",
      "Epoch: 8, Batch number: 68, Loss: 6443.060546875\n",
      "Epoch: 10, Batch number: 16, Loss: 5937.98583984375\n",
      "Epoch: 11, Batch number: 40, Loss: 5960.5966796875\n",
      "Epoch: 12, Batch number: 64, Loss: 5876.451171875\n",
      "Epoch: 14, Batch number: 12, Loss: 5642.3974609375\n",
      "Epoch: 15, Batch number: 36, Loss: 5502.20361328125\n",
      "Epoch: 16, Batch number: 60, Loss: 5385.2861328125\n",
      "Epoch: 18, Batch number: 8, Loss: 5207.33154296875\n",
      "Epoch: 19, Batch number: 32, Loss: 5161.62744140625\n",
      "Epoch: 20, Batch number: 56, Loss: 4983.78662109375\n",
      "Epoch: 22, Batch number: 4, Loss: 4797.13720703125\n",
      "Epoch: 23, Batch number: 28, Loss: 4744.37353515625\n",
      "Epoch: 24, Batch number: 52, Loss: 4717.25048828125\n",
      "Epoch: 26, Batch number: 0, Loss: 4499.7734375\n",
      "Epoch: 27, Batch number: 24, Loss: 4507.20947265625\n",
      "Epoch: 28, Batch number: 48, Loss: 4418.39013671875\n",
      "Epoch: 29, Batch number: 72, Loss: 4534.66552734375\n",
      "Epoch: 31, Batch number: 20, Loss: 4359.2080078125\n",
      "Epoch: 32, Batch number: 44, Loss: 4455.51123046875\n",
      "Epoch: 33, Batch number: 68, Loss: 4434.66650390625\n",
      "Epoch: 35, Batch number: 16, Loss: 4133.451171875\n",
      "Epoch: 36, Batch number: 40, Loss: 4226.1884765625\n",
      "Epoch: 37, Batch number: 64, Loss: 4210.04638671875\n",
      "Epoch: 39, Batch number: 12, Loss: 4061.563720703125\n",
      "Epoch: 40, Batch number: 36, Loss: 4050.912109375\n",
      "Epoch: 41, Batch number: 60, Loss: 4123.18701171875\n",
      "Epoch: 43, Batch number: 8, Loss: 3952.077392578125\n",
      "Epoch: 44, Batch number: 32, Loss: 4015.69775390625\n",
      "Epoch: 45, Batch number: 56, Loss: 3926.638427734375\n",
      "Epoch: 47, Batch number: 4, Loss: 3907.48779296875\n",
      "Epoch: 48, Batch number: 28, Loss: 3853.18994140625\n",
      "Epoch: 49, Batch number: 52, Loss: 3880.416015625\n",
      "Epoch: 51, Batch number: 0, Loss: 3698.1201171875\n",
      "Epoch: 52, Batch number: 24, Loss: 3915.55908203125\n",
      "Epoch: 53, Batch number: 48, Loss: 3773.75927734375\n",
      "Epoch: 54, Batch number: 72, Loss: 3778.03125\n",
      "Epoch: 56, Batch number: 20, Loss: 3698.108642578125\n",
      "Epoch: 57, Batch number: 44, Loss: 3783.814453125\n",
      "Epoch: 58, Batch number: 68, Loss: 3875.132080078125\n",
      "Epoch: 60, Batch number: 16, Loss: 3703.733154296875\n",
      "Epoch: 61, Batch number: 40, Loss: 3732.533203125\n",
      "Epoch: 62, Batch number: 64, Loss: 3691.53076171875\n",
      "Epoch: 64, Batch number: 12, Loss: 3591.818115234375\n",
      "Epoch: 65, Batch number: 36, Loss: 3594.033203125\n",
      "Epoch: 66, Batch number: 60, Loss: 3653.203369140625\n",
      "Epoch: 68, Batch number: 8, Loss: 3656.205810546875\n",
      "Epoch: 69, Batch number: 32, Loss: 3577.058349609375\n",
      "Epoch: 70, Batch number: 56, Loss: 3473.149169921875\n",
      "Epoch: 72, Batch number: 4, Loss: 3546.714599609375\n",
      "Epoch: 73, Batch number: 28, Loss: 3517.95361328125\n",
      "Epoch: 74, Batch number: 52, Loss: 3648.306884765625\n",
      "Epoch: 76, Batch number: 0, Loss: 3482.711669921875\n",
      "Epoch: 77, Batch number: 24, Loss: 3597.820556640625\n",
      "Epoch: 78, Batch number: 48, Loss: 3436.959228515625\n",
      "Epoch: 79, Batch number: 72, Loss: 3434.65380859375\n",
      "Epoch: 81, Batch number: 20, Loss: 3412.979736328125\n",
      "Epoch: 82, Batch number: 44, Loss: 3463.900146484375\n",
      "Epoch: 83, Batch number: 68, Loss: 3534.26806640625\n",
      "Epoch: 85, Batch number: 16, Loss: 3361.90380859375\n",
      "Epoch: 86, Batch number: 40, Loss: 3515.7001953125\n",
      "Epoch: 87, Batch number: 64, Loss: 3373.3427734375\n",
      "Epoch: 89, Batch number: 12, Loss: 3438.21044921875\n",
      "Epoch: 90, Batch number: 36, Loss: 3374.89013671875\n",
      "Epoch: 91, Batch number: 60, Loss: 3533.138427734375\n",
      "Epoch: 93, Batch number: 8, Loss: 3475.31298828125\n",
      "Epoch: 94, Batch number: 32, Loss: 3308.574951171875\n",
      "Epoch: 95, Batch number: 56, Loss: 3487.03076171875\n",
      "Epoch: 97, Batch number: 4, Loss: 3395.895751953125\n",
      "Epoch: 98, Batch number: 28, Loss: 3335.202392578125\n",
      "Epoch: 99, Batch number: 52, Loss: 3320.335205078125\n",
      "Epoch: 101, Batch number: 0, Loss: 3314.47705078125\n",
      "Epoch: 102, Batch number: 24, Loss: 3362.72509765625\n",
      "Epoch: 103, Batch number: 48, Loss: 3300.2626953125\n",
      "Epoch: 104, Batch number: 72, Loss: 3335.4560546875\n",
      "Epoch: 106, Batch number: 20, Loss: 3344.428955078125\n",
      "Epoch: 107, Batch number: 44, Loss: 3448.7470703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108, Batch number: 68, Loss: 3299.842529296875\n",
      "Epoch: 110, Batch number: 16, Loss: 3214.678955078125\n",
      "Epoch: 111, Batch number: 40, Loss: 3318.345947265625\n",
      "Epoch: 112, Batch number: 64, Loss: 3271.87744140625\n",
      "Epoch: 114, Batch number: 12, Loss: 3385.904052734375\n",
      "Epoch: 115, Batch number: 36, Loss: 3273.841796875\n",
      "Epoch: 116, Batch number: 60, Loss: 3308.88427734375\n",
      "Epoch: 118, Batch number: 8, Loss: 3283.134033203125\n",
      "Epoch: 119, Batch number: 32, Loss: 3356.568359375\n",
      "Epoch: 120, Batch number: 56, Loss: 3252.722412109375\n",
      "Epoch: 122, Batch number: 4, Loss: 3248.814453125\n",
      "Epoch: 123, Batch number: 28, Loss: 3209.739990234375\n",
      "Epoch: 124, Batch number: 52, Loss: 3468.196533203125\n",
      "Epoch: 126, Batch number: 0, Loss: 3236.68896484375\n",
      "Epoch: 127, Batch number: 24, Loss: 3304.306884765625\n",
      "Epoch: 128, Batch number: 48, Loss: 3324.38037109375\n",
      "Epoch: 129, Batch number: 72, Loss: 3349.521240234375\n",
      "Epoch: 131, Batch number: 20, Loss: 3194.63232421875\n",
      "Epoch: 132, Batch number: 44, Loss: 3453.68701171875\n",
      "Epoch: 133, Batch number: 68, Loss: 3245.23779296875\n",
      "Epoch: 135, Batch number: 16, Loss: 3297.968505859375\n",
      "Epoch: 136, Batch number: 40, Loss: 3283.601318359375\n",
      "Epoch: 137, Batch number: 64, Loss: 3202.07470703125\n",
      "Epoch: 139, Batch number: 12, Loss: 3145.741943359375\n",
      "Epoch: 140, Batch number: 36, Loss: 3218.73291015625\n",
      "Epoch: 141, Batch number: 60, Loss: 3166.71728515625\n",
      "Epoch: 143, Batch number: 8, Loss: 3172.38232421875\n",
      "Epoch: 144, Batch number: 32, Loss: 3273.6396484375\n",
      "Epoch: 145, Batch number: 56, Loss: 3321.656494140625\n",
      "Epoch: 147, Batch number: 4, Loss: 3275.49267578125\n",
      "Epoch: 148, Batch number: 28, Loss: 3205.584716796875\n",
      "Epoch: 149, Batch number: 52, Loss: 3249.046630859375\n",
      "Epoch: 151, Batch number: 0, Loss: 3234.70556640625\n",
      "Epoch: 152, Batch number: 24, Loss: 3241.29833984375\n",
      "Epoch: 153, Batch number: 48, Loss: 3241.379150390625\n",
      "Epoch: 154, Batch number: 72, Loss: 3189.127685546875\n",
      "Epoch: 156, Batch number: 20, Loss: 3183.884521484375\n",
      "Epoch: 157, Batch number: 44, Loss: 3163.961181640625\n",
      "Epoch: 158, Batch number: 68, Loss: 3353.072265625\n",
      "Epoch: 160, Batch number: 16, Loss: 3044.965576171875\n",
      "Epoch: 161, Batch number: 40, Loss: 3321.828125\n",
      "Epoch: 162, Batch number: 64, Loss: 3336.01904296875\n",
      "Epoch: 164, Batch number: 12, Loss: 3193.049560546875\n",
      "Epoch: 165, Batch number: 36, Loss: 3180.0087890625\n",
      "Epoch: 166, Batch number: 60, Loss: 3270.87939453125\n",
      "Epoch: 168, Batch number: 8, Loss: 3062.96484375\n",
      "Epoch: 169, Batch number: 32, Loss: 3050.614501953125\n",
      "Epoch: 170, Batch number: 56, Loss: 3185.401123046875\n",
      "Epoch: 172, Batch number: 4, Loss: 3359.51123046875\n",
      "Epoch: 173, Batch number: 28, Loss: 3271.334716796875\n",
      "Epoch: 174, Batch number: 52, Loss: 3155.44580078125\n",
      "Epoch: 176, Batch number: 0, Loss: 3132.4345703125\n",
      "Epoch: 177, Batch number: 24, Loss: 3256.1669921875\n",
      "Epoch: 178, Batch number: 48, Loss: 3212.94189453125\n",
      "Epoch: 179, Batch number: 72, Loss: 3220.47265625\n",
      "Epoch: 181, Batch number: 20, Loss: 3238.485107421875\n",
      "Epoch: 182, Batch number: 44, Loss: 3242.86865234375\n",
      "Epoch: 183, Batch number: 68, Loss: 3215.311279296875\n",
      "Epoch: 185, Batch number: 16, Loss: 3224.151123046875\n",
      "Epoch: 186, Batch number: 40, Loss: 3250.982421875\n",
      "Epoch: 187, Batch number: 64, Loss: 3119.877685546875\n",
      "Epoch: 189, Batch number: 12, Loss: 3299.41943359375\n",
      "Epoch: 190, Batch number: 36, Loss: 3164.181640625\n",
      "Epoch: 191, Batch number: 60, Loss: 3274.779541015625\n",
      "Epoch: 193, Batch number: 8, Loss: 3253.973388671875\n",
      "Epoch: 194, Batch number: 32, Loss: 3328.615234375\n",
      "Epoch: 195, Batch number: 56, Loss: 3344.14404296875\n",
      "Epoch: 197, Batch number: 4, Loss: 3250.8271484375\n",
      "Epoch: 198, Batch number: 28, Loss: 3324.87841796875\n",
      "Epoch: 199, Batch number: 52, Loss: 3290.03955078125\n",
      "Epoch: 201, Batch number: 0, Loss: 3178.654052734375\n",
      "Epoch: 202, Batch number: 24, Loss: 3159.56103515625\n",
      "Epoch: 203, Batch number: 48, Loss: 3264.384521484375\n",
      "Epoch: 204, Batch number: 72, Loss: 3255.5244140625\n",
      "Epoch: 206, Batch number: 20, Loss: 3183.483154296875\n",
      "Epoch: 207, Batch number: 44, Loss: 3214.7080078125\n",
      "Epoch: 208, Batch number: 68, Loss: 3336.19580078125\n",
      "Epoch: 210, Batch number: 16, Loss: 3214.754638671875\n",
      "Epoch: 211, Batch number: 40, Loss: 3287.30224609375\n",
      "Epoch: 212, Batch number: 64, Loss: 3288.2001953125\n",
      "Epoch: 214, Batch number: 12, Loss: 2997.85498046875\n",
      "Epoch: 215, Batch number: 36, Loss: 3308.14453125\n",
      "Epoch: 216, Batch number: 60, Loss: 3335.237060546875\n",
      "Epoch: 218, Batch number: 8, Loss: 3294.048583984375\n",
      "Epoch: 219, Batch number: 32, Loss: 3194.82177734375\n",
      "Epoch: 220, Batch number: 56, Loss: 3224.90673828125\n",
      "Epoch: 222, Batch number: 4, Loss: 3078.86181640625\n",
      "Epoch: 223, Batch number: 28, Loss: 3332.44384765625\n",
      "Epoch: 224, Batch number: 52, Loss: 3176.65087890625\n",
      "Epoch: 226, Batch number: 0, Loss: 3183.744873046875\n",
      "Epoch: 227, Batch number: 24, Loss: 3306.9990234375\n",
      "Epoch: 228, Batch number: 48, Loss: 3221.14990234375\n",
      "Epoch: 229, Batch number: 72, Loss: 3168.737548828125\n",
      "Epoch: 231, Batch number: 20, Loss: 3125.146484375\n",
      "Epoch: 232, Batch number: 44, Loss: 3180.807861328125\n",
      "Epoch: 233, Batch number: 68, Loss: 3078.601318359375\n",
      "Epoch: 235, Batch number: 16, Loss: 3090.782958984375\n",
      "Epoch: 236, Batch number: 40, Loss: 3250.04736328125\n",
      "Epoch: 237, Batch number: 64, Loss: 3286.241943359375\n",
      "Epoch: 239, Batch number: 12, Loss: 3122.927001953125\n",
      "Epoch: 240, Batch number: 36, Loss: 3243.1513671875\n",
      "Epoch: 241, Batch number: 60, Loss: 3274.360107421875\n",
      "Epoch: 243, Batch number: 8, Loss: 3114.280517578125\n",
      "Epoch: 244, Batch number: 32, Loss: 3252.046875\n",
      "Epoch: 245, Batch number: 56, Loss: 3249.104248046875\n",
      "Epoch: 247, Batch number: 4, Loss: 3175.718994140625\n",
      "Epoch: 248, Batch number: 28, Loss: 3146.74169921875\n",
      "Epoch: 249, Batch number: 52, Loss: 3255.111083984375\n",
      "Epoch: 251, Batch number: 0, Loss: 3085.466796875\n",
      "Epoch: 252, Batch number: 24, Loss: 3071.841796875\n",
      "Epoch: 253, Batch number: 48, Loss: 3139.41845703125\n",
      "Epoch: 254, Batch number: 72, Loss: 3283.610595703125\n",
      "Epoch: 256, Batch number: 20, Loss: 3103.71484375\n",
      "Epoch: 257, Batch number: 44, Loss: 3217.677490234375\n",
      "Epoch: 258, Batch number: 68, Loss: 3230.93994140625\n",
      "Epoch: 260, Batch number: 16, Loss: 3199.598876953125\n",
      "Epoch: 261, Batch number: 40, Loss: 3224.9638671875\n",
      "Epoch: 262, Batch number: 64, Loss: 3302.4072265625\n",
      "Epoch: 264, Batch number: 12, Loss: 3084.232421875\n",
      "Epoch: 265, Batch number: 36, Loss: 3100.908203125\n",
      "Epoch: 266, Batch number: 60, Loss: 3301.439697265625\n",
      "Epoch: 268, Batch number: 8, Loss: 3229.798095703125\n",
      "Epoch: 269, Batch number: 32, Loss: 3115.089111328125\n",
      "Epoch: 270, Batch number: 56, Loss: 3248.874755859375\n",
      "Epoch: 272, Batch number: 4, Loss: 3270.3447265625\n",
      "Epoch: 273, Batch number: 28, Loss: 3274.8916015625\n",
      "Epoch: 274, Batch number: 52, Loss: 3242.09716796875\n",
      "Epoch: 276, Batch number: 0, Loss: 3264.756103515625\n",
      "Epoch: 277, Batch number: 24, Loss: 3116.13720703125\n",
      "Epoch: 278, Batch number: 48, Loss: 3178.209716796875\n",
      "Epoch: 279, Batch number: 72, Loss: 3373.06494140625\n",
      "Epoch: 281, Batch number: 20, Loss: 3207.11376953125\n",
      "Epoch: 282, Batch number: 44, Loss: 3224.13134765625\n",
      "Epoch: 283, Batch number: 68, Loss: 3133.311279296875\n",
      "Epoch: 285, Batch number: 16, Loss: 3302.12890625\n",
      "Epoch: 286, Batch number: 40, Loss: 3262.77587890625\n",
      "Epoch: 287, Batch number: 64, Loss: 3139.38671875\n",
      "Epoch: 289, Batch number: 12, Loss: 3200.34765625\n",
      "Epoch: 290, Batch number: 36, Loss: 3024.2880859375\n",
      "Epoch: 291, Batch number: 60, Loss: 3278.79736328125\n",
      "Epoch: 293, Batch number: 8, Loss: 3001.056640625\n",
      "Epoch: 294, Batch number: 32, Loss: 3272.087890625\n",
      "Epoch: 295, Batch number: 56, Loss: 3082.103759765625\n",
      "Epoch: 297, Batch number: 4, Loss: 3140.33984375\n",
      "Epoch: 298, Batch number: 28, Loss: 3075.93701171875\n",
      "Epoch: 299, Batch number: 52, Loss: 3187.220947265625\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 8128.650390625\n",
      "Epoch: 2, Batch number: 24, Loss: 7527.32861328125\n",
      "Epoch: 3, Batch number: 48, Loss: 7087.21240234375\n",
      "Epoch: 4, Batch number: 72, Loss: 6864.61865234375\n",
      "Epoch: 6, Batch number: 20, Loss: 6492.37158203125\n",
      "Epoch: 7, Batch number: 44, Loss: 6251.666015625\n",
      "Epoch: 8, Batch number: 68, Loss: 6017.5078125\n",
      "Epoch: 10, Batch number: 16, Loss: 5677.7783203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Batch number: 40, Loss: 5643.5380859375\n",
      "Epoch: 12, Batch number: 64, Loss: 5431.52490234375\n",
      "Epoch: 14, Batch number: 12, Loss: 5230.212890625\n",
      "Epoch: 15, Batch number: 36, Loss: 5156.35546875\n",
      "Epoch: 16, Batch number: 60, Loss: 4864.15625\n",
      "Epoch: 18, Batch number: 8, Loss: 4855.09033203125\n",
      "Epoch: 19, Batch number: 32, Loss: 4614.7841796875\n",
      "Epoch: 20, Batch number: 56, Loss: 4489.01708984375\n",
      "Epoch: 22, Batch number: 4, Loss: 4454.41259765625\n",
      "Epoch: 23, Batch number: 28, Loss: 4446.154296875\n",
      "Epoch: 24, Batch number: 52, Loss: 4428.8046875\n",
      "Epoch: 26, Batch number: 0, Loss: 4182.6767578125\n",
      "Epoch: 27, Batch number: 24, Loss: 4151.59521484375\n",
      "Epoch: 28, Batch number: 48, Loss: 4122.302734375\n",
      "Epoch: 29, Batch number: 72, Loss: 4224.4453125\n",
      "Epoch: 31, Batch number: 20, Loss: 4007.268310546875\n",
      "Epoch: 32, Batch number: 44, Loss: 4108.00927734375\n",
      "Epoch: 33, Batch number: 68, Loss: 4031.45849609375\n",
      "Epoch: 35, Batch number: 16, Loss: 3892.5458984375\n",
      "Epoch: 36, Batch number: 40, Loss: 3823.143310546875\n",
      "Epoch: 37, Batch number: 64, Loss: 3932.555908203125\n",
      "Epoch: 39, Batch number: 12, Loss: 3826.19384765625\n",
      "Epoch: 40, Batch number: 36, Loss: 3730.246337890625\n",
      "Epoch: 41, Batch number: 60, Loss: 3699.539794921875\n",
      "Epoch: 43, Batch number: 8, Loss: 3559.479736328125\n",
      "Epoch: 44, Batch number: 32, Loss: 3552.59912109375\n",
      "Epoch: 45, Batch number: 56, Loss: 3541.068115234375\n",
      "Epoch: 47, Batch number: 4, Loss: 3624.177001953125\n",
      "Epoch: 48, Batch number: 28, Loss: 3691.827880859375\n",
      "Epoch: 49, Batch number: 52, Loss: 3604.542724609375\n",
      "Epoch: 51, Batch number: 0, Loss: 3479.241455078125\n",
      "Epoch: 52, Batch number: 24, Loss: 3492.29345703125\n",
      "Epoch: 53, Batch number: 48, Loss: 3473.322998046875\n",
      "Epoch: 54, Batch number: 72, Loss: 3510.06884765625\n",
      "Epoch: 56, Batch number: 20, Loss: 3442.993896484375\n",
      "Epoch: 57, Batch number: 44, Loss: 3540.03515625\n",
      "Epoch: 58, Batch number: 68, Loss: 3432.8408203125\n",
      "Epoch: 60, Batch number: 16, Loss: 3395.376708984375\n",
      "Epoch: 61, Batch number: 40, Loss: 3550.451904296875\n",
      "Epoch: 62, Batch number: 64, Loss: 3477.5390625\n",
      "Epoch: 64, Batch number: 12, Loss: 3318.9765625\n",
      "Epoch: 65, Batch number: 36, Loss: 3483.790283203125\n",
      "Epoch: 66, Batch number: 60, Loss: 3500.65625\n",
      "Epoch: 68, Batch number: 8, Loss: 3543.078857421875\n",
      "Epoch: 69, Batch number: 32, Loss: 3298.642822265625\n",
      "Epoch: 70, Batch number: 56, Loss: 3502.4580078125\n",
      "Epoch: 72, Batch number: 4, Loss: 3180.859130859375\n",
      "Epoch: 73, Batch number: 28, Loss: 3358.87451171875\n",
      "Epoch: 74, Batch number: 52, Loss: 3358.73779296875\n",
      "Epoch: 76, Batch number: 0, Loss: 3342.373779296875\n",
      "Epoch: 77, Batch number: 24, Loss: 3271.322998046875\n",
      "Epoch: 78, Batch number: 48, Loss: 3463.556396484375\n",
      "Epoch: 79, Batch number: 72, Loss: 3479.9482421875\n",
      "Epoch: 81, Batch number: 20, Loss: 3352.795654296875\n",
      "Epoch: 82, Batch number: 44, Loss: 3501.22265625\n",
      "Epoch: 83, Batch number: 68, Loss: 3330.40234375\n",
      "Epoch: 85, Batch number: 16, Loss: 3277.349853515625\n",
      "Epoch: 86, Batch number: 40, Loss: 3369.637451171875\n",
      "Epoch: 87, Batch number: 64, Loss: 3368.0712890625\n",
      "Epoch: 89, Batch number: 12, Loss: 3411.08447265625\n",
      "Epoch: 90, Batch number: 36, Loss: 3337.43896484375\n",
      "Epoch: 91, Batch number: 60, Loss: 3339.6103515625\n",
      "Epoch: 93, Batch number: 8, Loss: 3270.48291015625\n",
      "Epoch: 94, Batch number: 32, Loss: 3254.390869140625\n",
      "Epoch: 95, Batch number: 56, Loss: 3308.303466796875\n",
      "Epoch: 97, Batch number: 4, Loss: 3209.66845703125\n",
      "Epoch: 98, Batch number: 28, Loss: 3160.4658203125\n",
      "Epoch: 99, Batch number: 52, Loss: 3368.452392578125\n",
      "Epoch: 101, Batch number: 0, Loss: 3182.33154296875\n",
      "Epoch: 102, Batch number: 24, Loss: 3181.058837890625\n",
      "Epoch: 103, Batch number: 48, Loss: 3211.83837890625\n",
      "Epoch: 104, Batch number: 72, Loss: 3382.93994140625\n",
      "Epoch: 106, Batch number: 20, Loss: 3221.523193359375\n",
      "Epoch: 107, Batch number: 44, Loss: 3260.2548828125\n",
      "Epoch: 108, Batch number: 68, Loss: 3226.326171875\n",
      "Epoch: 110, Batch number: 16, Loss: 3306.7177734375\n",
      "Epoch: 111, Batch number: 40, Loss: 3165.490966796875\n",
      "Epoch: 112, Batch number: 64, Loss: 3318.970703125\n",
      "Epoch: 114, Batch number: 12, Loss: 3258.417724609375\n",
      "Epoch: 115, Batch number: 36, Loss: 3308.641357421875\n",
      "Epoch: 116, Batch number: 60, Loss: 3360.05810546875\n",
      "Epoch: 118, Batch number: 8, Loss: 3312.8505859375\n",
      "Epoch: 119, Batch number: 32, Loss: 3173.7919921875\n",
      "Epoch: 120, Batch number: 56, Loss: 3231.1357421875\n",
      "Epoch: 122, Batch number: 4, Loss: 3156.34326171875\n",
      "Epoch: 123, Batch number: 28, Loss: 3330.4013671875\n",
      "Epoch: 124, Batch number: 52, Loss: 3337.232177734375\n",
      "Epoch: 126, Batch number: 0, Loss: 3161.18701171875\n",
      "Epoch: 127, Batch number: 24, Loss: 3146.745849609375\n",
      "Epoch: 128, Batch number: 48, Loss: 3158.18310546875\n",
      "Epoch: 129, Batch number: 72, Loss: 3247.725830078125\n",
      "Epoch: 131, Batch number: 20, Loss: 3175.9169921875\n",
      "Epoch: 132, Batch number: 44, Loss: 3328.122802734375\n",
      "Epoch: 133, Batch number: 68, Loss: 3241.843017578125\n",
      "Epoch: 135, Batch number: 16, Loss: 3263.5517578125\n",
      "Epoch: 136, Batch number: 40, Loss: 3096.59375\n",
      "Epoch: 137, Batch number: 64, Loss: 3295.308837890625\n",
      "Epoch: 139, Batch number: 12, Loss: 3200.826416015625\n",
      "Epoch: 140, Batch number: 36, Loss: 3107.20166015625\n",
      "Epoch: 141, Batch number: 60, Loss: 3271.40380859375\n",
      "Epoch: 143, Batch number: 8, Loss: 3307.099365234375\n",
      "Epoch: 144, Batch number: 32, Loss: 3261.61865234375\n",
      "Epoch: 145, Batch number: 56, Loss: 3280.685546875\n",
      "Epoch: 147, Batch number: 4, Loss: 3184.39697265625\n",
      "Epoch: 148, Batch number: 28, Loss: 3191.55615234375\n",
      "Epoch: 149, Batch number: 52, Loss: 3296.080322265625\n",
      "Epoch: 151, Batch number: 0, Loss: 3214.261474609375\n",
      "Epoch: 152, Batch number: 24, Loss: 3071.274658203125\n",
      "Epoch: 153, Batch number: 48, Loss: 3229.575927734375\n",
      "Epoch: 154, Batch number: 72, Loss: 3264.89599609375\n",
      "Epoch: 156, Batch number: 20, Loss: 3192.15185546875\n",
      "Epoch: 157, Batch number: 44, Loss: 3016.1748046875\n",
      "Epoch: 158, Batch number: 68, Loss: 3252.72216796875\n",
      "Epoch: 160, Batch number: 16, Loss: 3113.684326171875\n",
      "Epoch: 161, Batch number: 40, Loss: 3330.51708984375\n",
      "Epoch: 162, Batch number: 64, Loss: 3280.4296875\n",
      "Epoch: 164, Batch number: 12, Loss: 3147.234130859375\n",
      "Epoch: 165, Batch number: 36, Loss: 3186.769775390625\n",
      "Epoch: 166, Batch number: 60, Loss: 3216.98681640625\n",
      "Epoch: 168, Batch number: 8, Loss: 3032.85107421875\n",
      "Epoch: 169, Batch number: 32, Loss: 3129.930908203125\n",
      "Epoch: 170, Batch number: 56, Loss: 3170.19921875\n",
      "Epoch: 172, Batch number: 4, Loss: 3128.82080078125\n",
      "Epoch: 173, Batch number: 28, Loss: 3151.349853515625\n",
      "Epoch: 174, Batch number: 52, Loss: 3094.08447265625\n",
      "Epoch: 176, Batch number: 0, Loss: 3127.517333984375\n",
      "Epoch: 177, Batch number: 24, Loss: 3249.38232421875\n",
      "Epoch: 178, Batch number: 48, Loss: 3200.06494140625\n",
      "Epoch: 179, Batch number: 72, Loss: 3159.7001953125\n",
      "Epoch: 181, Batch number: 20, Loss: 3033.068603515625\n",
      "Epoch: 182, Batch number: 44, Loss: 3119.00634765625\n",
      "Epoch: 183, Batch number: 68, Loss: 3176.53173828125\n",
      "Epoch: 185, Batch number: 16, Loss: 3204.0859375\n",
      "Epoch: 186, Batch number: 40, Loss: 3186.389892578125\n",
      "Epoch: 187, Batch number: 64, Loss: 3126.284423828125\n",
      "Epoch: 189, Batch number: 12, Loss: 3186.199462890625\n",
      "Epoch: 190, Batch number: 36, Loss: 3275.082275390625\n",
      "Epoch: 191, Batch number: 60, Loss: 3240.191162109375\n",
      "Epoch: 193, Batch number: 8, Loss: 3194.2119140625\n",
      "Epoch: 194, Batch number: 32, Loss: 3240.397216796875\n",
      "Epoch: 195, Batch number: 56, Loss: 3226.973876953125\n",
      "Epoch: 197, Batch number: 4, Loss: 3086.091796875\n",
      "Epoch: 198, Batch number: 28, Loss: 3091.69091796875\n",
      "Epoch: 199, Batch number: 52, Loss: 3214.09130859375\n",
      "Epoch: 201, Batch number: 0, Loss: 3253.736328125\n",
      "Epoch: 202, Batch number: 24, Loss: 3226.572021484375\n",
      "Epoch: 203, Batch number: 48, Loss: 3110.3544921875\n",
      "Epoch: 204, Batch number: 72, Loss: 3198.907958984375\n",
      "Epoch: 206, Batch number: 20, Loss: 3296.163330078125\n",
      "Epoch: 207, Batch number: 44, Loss: 3112.189453125\n",
      "Epoch: 208, Batch number: 68, Loss: 3015.277587890625\n",
      "Epoch: 210, Batch number: 16, Loss: 3126.23046875\n",
      "Epoch: 211, Batch number: 40, Loss: 3130.283447265625\n",
      "Epoch: 212, Batch number: 64, Loss: 3260.189208984375\n",
      "Epoch: 214, Batch number: 12, Loss: 3086.8251953125\n",
      "Epoch: 215, Batch number: 36, Loss: 3174.958740234375\n",
      "Epoch: 216, Batch number: 60, Loss: 3340.0703125\n",
      "Epoch: 218, Batch number: 8, Loss: 3124.25634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219, Batch number: 32, Loss: 3161.0810546875\n",
      "Epoch: 220, Batch number: 56, Loss: 3411.954345703125\n",
      "Epoch: 222, Batch number: 4, Loss: 3107.835205078125\n",
      "Epoch: 223, Batch number: 28, Loss: 3265.247314453125\n",
      "Epoch: 224, Batch number: 52, Loss: 3241.4501953125\n",
      "Epoch: 226, Batch number: 0, Loss: 3277.532470703125\n",
      "Epoch: 227, Batch number: 24, Loss: 3176.315185546875\n",
      "Epoch: 228, Batch number: 48, Loss: 3333.126708984375\n",
      "Epoch: 229, Batch number: 72, Loss: 3196.651123046875\n",
      "Epoch: 231, Batch number: 20, Loss: 3211.433349609375\n",
      "Epoch: 232, Batch number: 44, Loss: 3115.51171875\n",
      "Epoch: 233, Batch number: 68, Loss: 3181.314453125\n",
      "Epoch: 235, Batch number: 16, Loss: 3196.893310546875\n",
      "Epoch: 236, Batch number: 40, Loss: 3214.510986328125\n",
      "Epoch: 237, Batch number: 64, Loss: 3157.139404296875\n",
      "Epoch: 239, Batch number: 12, Loss: 3143.700927734375\n",
      "Epoch: 240, Batch number: 36, Loss: 3224.060791015625\n",
      "Epoch: 241, Batch number: 60, Loss: 3342.64306640625\n",
      "Epoch: 243, Batch number: 8, Loss: 3007.709228515625\n",
      "Epoch: 244, Batch number: 32, Loss: 3294.98583984375\n",
      "Epoch: 245, Batch number: 56, Loss: 3229.82373046875\n",
      "Epoch: 247, Batch number: 4, Loss: 3156.37744140625\n",
      "Epoch: 248, Batch number: 28, Loss: 3184.765380859375\n",
      "Epoch: 249, Batch number: 52, Loss: 3112.42919921875\n",
      "Epoch: 251, Batch number: 0, Loss: 3159.677001953125\n",
      "Epoch: 252, Batch number: 24, Loss: 3227.73095703125\n",
      "Epoch: 253, Batch number: 48, Loss: 3042.134033203125\n",
      "Epoch: 254, Batch number: 72, Loss: 3187.7509765625\n",
      "Epoch: 256, Batch number: 20, Loss: 3147.205078125\n",
      "Epoch: 257, Batch number: 44, Loss: 3303.095947265625\n",
      "Epoch: 258, Batch number: 68, Loss: 3263.30078125\n",
      "Epoch: 260, Batch number: 16, Loss: 3256.25146484375\n",
      "Epoch: 261, Batch number: 40, Loss: 3283.569580078125\n",
      "Epoch: 262, Batch number: 64, Loss: 3171.332275390625\n",
      "Epoch: 264, Batch number: 12, Loss: 3204.42138671875\n",
      "Epoch: 265, Batch number: 36, Loss: 3163.388427734375\n",
      "Epoch: 266, Batch number: 60, Loss: 3135.08837890625\n",
      "Epoch: 268, Batch number: 8, Loss: 3124.0927734375\n",
      "Epoch: 269, Batch number: 32, Loss: 3259.76953125\n",
      "Epoch: 270, Batch number: 56, Loss: 3026.720458984375\n",
      "Epoch: 272, Batch number: 4, Loss: 3035.9970703125\n",
      "Epoch: 273, Batch number: 28, Loss: 3265.205322265625\n",
      "Epoch: 274, Batch number: 52, Loss: 3229.828369140625\n",
      "Epoch: 276, Batch number: 0, Loss: 3097.9150390625\n",
      "Epoch: 277, Batch number: 24, Loss: 3155.228271484375\n",
      "Epoch: 278, Batch number: 48, Loss: 3174.03564453125\n",
      "Epoch: 279, Batch number: 72, Loss: 3133.757080078125\n",
      "Epoch: 281, Batch number: 20, Loss: 3183.848876953125\n",
      "Epoch: 282, Batch number: 44, Loss: 3167.875732421875\n",
      "Epoch: 283, Batch number: 68, Loss: 3337.11865234375\n",
      "Epoch: 285, Batch number: 16, Loss: 3259.124755859375\n",
      "Epoch: 286, Batch number: 40, Loss: 3090.3642578125\n",
      "Epoch: 287, Batch number: 64, Loss: 3310.41455078125\n",
      "Epoch: 289, Batch number: 12, Loss: 3189.6640625\n",
      "Epoch: 290, Batch number: 36, Loss: 3282.05859375\n",
      "Epoch: 291, Batch number: 60, Loss: 3165.685546875\n",
      "Epoch: 293, Batch number: 8, Loss: 3068.131591796875\n",
      "Epoch: 294, Batch number: 32, Loss: 3347.759765625\n",
      "Epoch: 295, Batch number: 56, Loss: 3127.73583984375\n",
      "Epoch: 297, Batch number: 4, Loss: 3173.169677734375\n",
      "Epoch: 298, Batch number: 28, Loss: 3242.314697265625\n",
      "Epoch: 299, Batch number: 52, Loss: 3106.0068359375\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 8079.47607421875\n",
      "Epoch: 2, Batch number: 24, Loss: 7293.2900390625\n",
      "Epoch: 3, Batch number: 48, Loss: 6821.86376953125\n",
      "Epoch: 4, Batch number: 72, Loss: 6674.41455078125\n",
      "Epoch: 6, Batch number: 20, Loss: 6077.27978515625\n",
      "Epoch: 7, Batch number: 44, Loss: 6041.53271484375\n",
      "Epoch: 8, Batch number: 68, Loss: 5729.47216796875\n",
      "Epoch: 10, Batch number: 16, Loss: 5501.76611328125\n",
      "Epoch: 11, Batch number: 40, Loss: 5277.08984375\n",
      "Epoch: 12, Batch number: 64, Loss: 4970.85986328125\n",
      "Epoch: 14, Batch number: 12, Loss: 4899.2294921875\n",
      "Epoch: 15, Batch number: 36, Loss: 4778.4453125\n",
      "Epoch: 16, Batch number: 60, Loss: 4418.68798828125\n",
      "Epoch: 18, Batch number: 8, Loss: 4300.1279296875\n",
      "Epoch: 19, Batch number: 32, Loss: 4248.552734375\n",
      "Epoch: 20, Batch number: 56, Loss: 4282.0703125\n",
      "Epoch: 22, Batch number: 4, Loss: 4208.0810546875\n",
      "Epoch: 23, Batch number: 28, Loss: 4225.7001953125\n",
      "Epoch: 24, Batch number: 52, Loss: 3984.70703125\n",
      "Epoch: 26, Batch number: 0, Loss: 3718.696533203125\n",
      "Epoch: 27, Batch number: 24, Loss: 3866.7578125\n",
      "Epoch: 28, Batch number: 48, Loss: 3809.509521484375\n",
      "Epoch: 29, Batch number: 72, Loss: 3866.544677734375\n",
      "Epoch: 31, Batch number: 20, Loss: 3743.528564453125\n",
      "Epoch: 32, Batch number: 44, Loss: 3747.916015625\n",
      "Epoch: 33, Batch number: 68, Loss: 3698.611083984375\n",
      "Epoch: 35, Batch number: 16, Loss: 3817.525390625\n",
      "Epoch: 36, Batch number: 40, Loss: 3565.08251953125\n",
      "Epoch: 37, Batch number: 64, Loss: 3735.38330078125\n",
      "Epoch: 39, Batch number: 12, Loss: 3518.749755859375\n",
      "Epoch: 40, Batch number: 36, Loss: 3500.171630859375\n",
      "Epoch: 41, Batch number: 60, Loss: 3810.457275390625\n",
      "Epoch: 43, Batch number: 8, Loss: 3523.8095703125\n",
      "Epoch: 44, Batch number: 32, Loss: 3608.58154296875\n",
      "Epoch: 45, Batch number: 56, Loss: 3544.763916015625\n",
      "Epoch: 47, Batch number: 4, Loss: 3652.326171875\n",
      "Epoch: 48, Batch number: 28, Loss: 3447.568359375\n",
      "Epoch: 49, Batch number: 52, Loss: 3621.3134765625\n",
      "Epoch: 51, Batch number: 0, Loss: 3377.603759765625\n",
      "Epoch: 52, Batch number: 24, Loss: 3348.27587890625\n",
      "Epoch: 53, Batch number: 48, Loss: 3364.287109375\n",
      "Epoch: 54, Batch number: 72, Loss: 3520.916748046875\n",
      "Epoch: 56, Batch number: 20, Loss: 3394.830078125\n",
      "Epoch: 57, Batch number: 44, Loss: 3384.789794921875\n",
      "Epoch: 58, Batch number: 68, Loss: 3312.347412109375\n",
      "Epoch: 60, Batch number: 16, Loss: 3253.27734375\n",
      "Epoch: 61, Batch number: 40, Loss: 3541.429443359375\n",
      "Epoch: 62, Batch number: 64, Loss: 3400.103271484375\n",
      "Epoch: 64, Batch number: 12, Loss: 3356.4091796875\n",
      "Epoch: 65, Batch number: 36, Loss: 3247.37255859375\n",
      "Epoch: 66, Batch number: 60, Loss: 3296.46044921875\n",
      "Epoch: 68, Batch number: 8, Loss: 3231.119384765625\n",
      "Epoch: 69, Batch number: 32, Loss: 3396.270751953125\n",
      "Epoch: 70, Batch number: 56, Loss: 3361.391357421875\n",
      "Epoch: 72, Batch number: 4, Loss: 3321.180419921875\n",
      "Epoch: 73, Batch number: 28, Loss: 3156.0517578125\n",
      "Epoch: 74, Batch number: 52, Loss: 3327.78125\n",
      "Epoch: 76, Batch number: 0, Loss: 3179.82861328125\n",
      "Epoch: 77, Batch number: 24, Loss: 3328.593017578125\n",
      "Epoch: 78, Batch number: 48, Loss: 3357.62158203125\n",
      "Epoch: 79, Batch number: 72, Loss: 3144.471435546875\n",
      "Epoch: 81, Batch number: 20, Loss: 3066.63720703125\n",
      "Epoch: 82, Batch number: 44, Loss: 3257.818603515625\n",
      "Epoch: 83, Batch number: 68, Loss: 3487.3994140625\n",
      "Epoch: 85, Batch number: 16, Loss: 3081.32373046875\n",
      "Epoch: 86, Batch number: 40, Loss: 3390.355224609375\n",
      "Epoch: 87, Batch number: 64, Loss: 3320.853515625\n",
      "Epoch: 89, Batch number: 12, Loss: 3232.50830078125\n",
      "Epoch: 90, Batch number: 36, Loss: 3277.765625\n",
      "Epoch: 91, Batch number: 60, Loss: 3258.289306640625\n",
      "Epoch: 93, Batch number: 8, Loss: 3228.033935546875\n",
      "Epoch: 94, Batch number: 32, Loss: 3296.134033203125\n",
      "Epoch: 95, Batch number: 56, Loss: 3305.197265625\n",
      "Epoch: 97, Batch number: 4, Loss: 3189.775390625\n",
      "Epoch: 98, Batch number: 28, Loss: 3180.261962890625\n",
      "Epoch: 99, Batch number: 52, Loss: 3232.591552734375\n",
      "Epoch: 101, Batch number: 0, Loss: 3255.162841796875\n",
      "Epoch: 102, Batch number: 24, Loss: 3222.678466796875\n",
      "Epoch: 103, Batch number: 48, Loss: 3234.738037109375\n",
      "Epoch: 104, Batch number: 72, Loss: 3304.804443359375\n",
      "Epoch: 106, Batch number: 20, Loss: 3131.21923828125\n",
      "Epoch: 107, Batch number: 44, Loss: 3197.342529296875\n",
      "Epoch: 108, Batch number: 68, Loss: 3289.9990234375\n",
      "Epoch: 110, Batch number: 16, Loss: 3035.100341796875\n",
      "Epoch: 111, Batch number: 40, Loss: 3258.271728515625\n",
      "Epoch: 112, Batch number: 64, Loss: 3291.62939453125\n",
      "Epoch: 114, Batch number: 12, Loss: 3290.1708984375\n",
      "Epoch: 115, Batch number: 36, Loss: 3155.060791015625\n",
      "Epoch: 116, Batch number: 60, Loss: 3244.26611328125\n",
      "Epoch: 118, Batch number: 8, Loss: 3175.04931640625\n",
      "Epoch: 119, Batch number: 32, Loss: 3242.9599609375\n",
      "Epoch: 120, Batch number: 56, Loss: 3193.78076171875\n",
      "Epoch: 122, Batch number: 4, Loss: 3109.991455078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123, Batch number: 28, Loss: 3143.684814453125\n",
      "Epoch: 124, Batch number: 52, Loss: 3242.466064453125\n",
      "Epoch: 126, Batch number: 0, Loss: 3144.481201171875\n",
      "Epoch: 127, Batch number: 24, Loss: 3113.004150390625\n",
      "Epoch: 128, Batch number: 48, Loss: 3226.072265625\n",
      "Epoch: 129, Batch number: 72, Loss: 3191.55615234375\n",
      "Epoch: 131, Batch number: 20, Loss: 3134.431884765625\n",
      "Epoch: 132, Batch number: 44, Loss: 3241.7861328125\n",
      "Epoch: 133, Batch number: 68, Loss: 3306.25\n",
      "Epoch: 135, Batch number: 16, Loss: 3122.4892578125\n",
      "Epoch: 136, Batch number: 40, Loss: 3262.59375\n",
      "Epoch: 137, Batch number: 64, Loss: 3340.421875\n",
      "Epoch: 139, Batch number: 12, Loss: 3175.990234375\n",
      "Epoch: 140, Batch number: 36, Loss: 3207.231689453125\n",
      "Epoch: 141, Batch number: 60, Loss: 3318.614501953125\n",
      "Epoch: 143, Batch number: 8, Loss: 3277.122314453125\n",
      "Epoch: 144, Batch number: 32, Loss: 3329.23046875\n",
      "Epoch: 145, Batch number: 56, Loss: 3266.40234375\n",
      "Epoch: 147, Batch number: 4, Loss: 3115.437255859375\n",
      "Epoch: 148, Batch number: 28, Loss: 3305.111328125\n",
      "Epoch: 149, Batch number: 52, Loss: 3406.9150390625\n",
      "Epoch: 151, Batch number: 0, Loss: 3157.234130859375\n",
      "Epoch: 152, Batch number: 24, Loss: 3162.2529296875\n",
      "Epoch: 153, Batch number: 48, Loss: 3288.369140625\n",
      "Epoch: 154, Batch number: 72, Loss: 3313.357177734375\n",
      "Epoch: 156, Batch number: 20, Loss: 3171.0556640625\n",
      "Epoch: 157, Batch number: 44, Loss: 3210.92919921875\n",
      "Epoch: 158, Batch number: 68, Loss: 3345.44091796875\n",
      "Epoch: 160, Batch number: 16, Loss: 3170.120361328125\n",
      "Epoch: 161, Batch number: 40, Loss: 3267.134765625\n",
      "Epoch: 162, Batch number: 64, Loss: 3207.571533203125\n",
      "Epoch: 164, Batch number: 12, Loss: 3091.4912109375\n",
      "Epoch: 165, Batch number: 36, Loss: 3143.79150390625\n",
      "Epoch: 166, Batch number: 60, Loss: 3270.575439453125\n",
      "Epoch: 168, Batch number: 8, Loss: 3266.905517578125\n",
      "Epoch: 169, Batch number: 32, Loss: 3207.40966796875\n",
      "Epoch: 170, Batch number: 56, Loss: 3186.370361328125\n",
      "Epoch: 172, Batch number: 4, Loss: 3189.141357421875\n",
      "Epoch: 173, Batch number: 28, Loss: 3261.574951171875\n",
      "Epoch: 174, Batch number: 52, Loss: 3076.64697265625\n",
      "Epoch: 176, Batch number: 0, Loss: 3161.37255859375\n",
      "Epoch: 177, Batch number: 24, Loss: 3250.4697265625\n",
      "Epoch: 178, Batch number: 48, Loss: 3213.660888671875\n",
      "Epoch: 179, Batch number: 72, Loss: 3267.9208984375\n",
      "Epoch: 181, Batch number: 20, Loss: 3291.20947265625\n",
      "Epoch: 182, Batch number: 44, Loss: 3259.92333984375\n",
      "Epoch: 183, Batch number: 68, Loss: 3195.5302734375\n",
      "Epoch: 185, Batch number: 16, Loss: 3068.3916015625\n",
      "Epoch: 186, Batch number: 40, Loss: 3204.03564453125\n",
      "Epoch: 187, Batch number: 64, Loss: 3241.497802734375\n",
      "Epoch: 189, Batch number: 12, Loss: 3168.06005859375\n",
      "Epoch: 190, Batch number: 36, Loss: 3140.371826171875\n",
      "Epoch: 191, Batch number: 60, Loss: 3306.28662109375\n",
      "Epoch: 193, Batch number: 8, Loss: 3148.241943359375\n",
      "Epoch: 194, Batch number: 32, Loss: 3218.5205078125\n",
      "Epoch: 195, Batch number: 56, Loss: 3272.90625\n",
      "Epoch: 197, Batch number: 4, Loss: 3111.2998046875\n",
      "Epoch: 198, Batch number: 28, Loss: 3208.218994140625\n",
      "Epoch: 199, Batch number: 52, Loss: 3284.135498046875\n",
      "Epoch: 201, Batch number: 0, Loss: 3264.00830078125\n",
      "Epoch: 202, Batch number: 24, Loss: 3234.991455078125\n",
      "Epoch: 203, Batch number: 48, Loss: 3221.91748046875\n",
      "Epoch: 204, Batch number: 72, Loss: 3288.3935546875\n",
      "Epoch: 206, Batch number: 20, Loss: 3247.17431640625\n",
      "Epoch: 207, Batch number: 44, Loss: 3242.9287109375\n",
      "Epoch: 208, Batch number: 68, Loss: 3281.41015625\n",
      "Epoch: 210, Batch number: 16, Loss: 3258.903564453125\n",
      "Epoch: 211, Batch number: 40, Loss: 3205.483642578125\n",
      "Epoch: 212, Batch number: 64, Loss: 3298.62109375\n",
      "Epoch: 214, Batch number: 12, Loss: 3140.147705078125\n",
      "Epoch: 215, Batch number: 36, Loss: 3226.38720703125\n",
      "Epoch: 216, Batch number: 60, Loss: 3223.821533203125\n",
      "Epoch: 218, Batch number: 8, Loss: 3125.61279296875\n",
      "Epoch: 219, Batch number: 32, Loss: 3264.2783203125\n",
      "Epoch: 220, Batch number: 56, Loss: 3194.08154296875\n",
      "Epoch: 222, Batch number: 4, Loss: 3102.859375\n",
      "Epoch: 223, Batch number: 28, Loss: 3106.823974609375\n",
      "Epoch: 224, Batch number: 52, Loss: 3294.7392578125\n",
      "Epoch: 226, Batch number: 0, Loss: 3239.013427734375\n",
      "Epoch: 227, Batch number: 24, Loss: 3225.548583984375\n",
      "Epoch: 228, Batch number: 48, Loss: 3237.93994140625\n",
      "Epoch: 229, Batch number: 72, Loss: 3208.590087890625\n",
      "Epoch: 231, Batch number: 20, Loss: 3350.888916015625\n",
      "Epoch: 232, Batch number: 44, Loss: 3127.43505859375\n",
      "Epoch: 233, Batch number: 68, Loss: 3002.163330078125\n",
      "Epoch: 235, Batch number: 16, Loss: 3256.582763671875\n",
      "Epoch: 236, Batch number: 40, Loss: 3195.52001953125\n",
      "Epoch: 237, Batch number: 64, Loss: 3256.207763671875\n",
      "Epoch: 239, Batch number: 12, Loss: 3151.941650390625\n",
      "Epoch: 240, Batch number: 36, Loss: 3218.5966796875\n",
      "Epoch: 241, Batch number: 60, Loss: 3375.3115234375\n",
      "Epoch: 243, Batch number: 8, Loss: 3121.578857421875\n",
      "Epoch: 244, Batch number: 32, Loss: 3137.021728515625\n",
      "Epoch: 245, Batch number: 56, Loss: 3188.7744140625\n",
      "Epoch: 247, Batch number: 4, Loss: 3111.82958984375\n",
      "Epoch: 248, Batch number: 28, Loss: 3151.126953125\n",
      "Epoch: 249, Batch number: 52, Loss: 3363.94091796875\n",
      "Epoch: 251, Batch number: 0, Loss: 3230.049560546875\n",
      "Epoch: 252, Batch number: 24, Loss: 3043.709716796875\n",
      "Epoch: 253, Batch number: 48, Loss: 3186.083251953125\n",
      "Epoch: 254, Batch number: 72, Loss: 3235.571044921875\n",
      "Epoch: 256, Batch number: 20, Loss: 3262.986083984375\n",
      "Epoch: 257, Batch number: 44, Loss: 3232.322265625\n",
      "Epoch: 258, Batch number: 68, Loss: 3395.041259765625\n",
      "Epoch: 260, Batch number: 16, Loss: 3107.511474609375\n",
      "Epoch: 261, Batch number: 40, Loss: 3284.45556640625\n",
      "Epoch: 262, Batch number: 64, Loss: 3271.912109375\n",
      "Epoch: 264, Batch number: 12, Loss: 2967.059814453125\n",
      "Epoch: 265, Batch number: 36, Loss: 3327.69287109375\n",
      "Epoch: 266, Batch number: 60, Loss: 3342.76806640625\n",
      "Epoch: 268, Batch number: 8, Loss: 3151.038818359375\n",
      "Epoch: 269, Batch number: 32, Loss: 3340.76318359375\n",
      "Epoch: 270, Batch number: 56, Loss: 3225.71630859375\n",
      "Epoch: 272, Batch number: 4, Loss: 3065.00439453125\n",
      "Epoch: 273, Batch number: 28, Loss: 3310.92724609375\n",
      "Epoch: 274, Batch number: 52, Loss: 3351.45263671875\n",
      "Epoch: 276, Batch number: 0, Loss: 3125.291015625\n",
      "Epoch: 277, Batch number: 24, Loss: 3189.96630859375\n",
      "Epoch: 278, Batch number: 48, Loss: 3318.037109375\n",
      "Epoch: 279, Batch number: 72, Loss: 3298.318359375\n",
      "Epoch: 281, Batch number: 20, Loss: 3338.927001953125\n",
      "Epoch: 282, Batch number: 44, Loss: 3345.089111328125\n",
      "Epoch: 283, Batch number: 68, Loss: 3347.03125\n",
      "Epoch: 285, Batch number: 16, Loss: 3044.37646484375\n",
      "Epoch: 286, Batch number: 40, Loss: 3156.171875\n",
      "Epoch: 287, Batch number: 64, Loss: 3201.007080078125\n",
      "Epoch: 289, Batch number: 12, Loss: 3176.15380859375\n",
      "Epoch: 290, Batch number: 36, Loss: 3185.810546875\n",
      "Epoch: 291, Batch number: 60, Loss: 3313.756591796875\n",
      "Epoch: 293, Batch number: 8, Loss: 3066.50048828125\n",
      "Epoch: 294, Batch number: 32, Loss: 3283.603759765625\n",
      "Epoch: 295, Batch number: 56, Loss: 3323.64404296875\n",
      "Epoch: 297, Batch number: 4, Loss: 3257.7900390625\n",
      "Epoch: 298, Batch number: 28, Loss: 3083.593994140625\n",
      "Epoch: 299, Batch number: 52, Loss: 3230.6337890625\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 8174.78564453125\n",
      "Epoch: 2, Batch number: 24, Loss: 7324.8974609375\n",
      "Epoch: 3, Batch number: 48, Loss: 6648.30859375\n",
      "Epoch: 4, Batch number: 72, Loss: 6320.22265625\n",
      "Epoch: 6, Batch number: 20, Loss: 5872.005859375\n",
      "Epoch: 7, Batch number: 44, Loss: 5625.7958984375\n",
      "Epoch: 8, Batch number: 68, Loss: 5404.63818359375\n",
      "Epoch: 10, Batch number: 16, Loss: 4995.0869140625\n",
      "Epoch: 11, Batch number: 40, Loss: 4775.279296875\n",
      "Epoch: 12, Batch number: 64, Loss: 4801.029296875\n",
      "Epoch: 14, Batch number: 12, Loss: 4352.00146484375\n",
      "Epoch: 15, Batch number: 36, Loss: 4287.4541015625\n",
      "Epoch: 16, Batch number: 60, Loss: 4432.95556640625\n",
      "Epoch: 18, Batch number: 8, Loss: 4105.8330078125\n",
      "Epoch: 19, Batch number: 32, Loss: 4107.009765625\n",
      "Epoch: 20, Batch number: 56, Loss: 3983.679931640625\n",
      "Epoch: 22, Batch number: 4, Loss: 3845.5263671875\n",
      "Epoch: 23, Batch number: 28, Loss: 3812.69482421875\n",
      "Epoch: 24, Batch number: 52, Loss: 3780.5537109375\n",
      "Epoch: 26, Batch number: 0, Loss: 3569.552001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Batch number: 24, Loss: 3652.281982421875\n",
      "Epoch: 28, Batch number: 48, Loss: 3608.4140625\n",
      "Epoch: 29, Batch number: 72, Loss: 3796.44921875\n",
      "Epoch: 31, Batch number: 20, Loss: 3445.804931640625\n",
      "Epoch: 32, Batch number: 44, Loss: 3445.406982421875\n",
      "Epoch: 33, Batch number: 68, Loss: 3570.217041015625\n",
      "Epoch: 35, Batch number: 16, Loss: 3487.60107421875\n",
      "Epoch: 36, Batch number: 40, Loss: 3443.462646484375\n",
      "Epoch: 37, Batch number: 64, Loss: 3486.103515625\n",
      "Epoch: 39, Batch number: 12, Loss: 3360.364990234375\n",
      "Epoch: 40, Batch number: 36, Loss: 3443.603271484375\n",
      "Epoch: 41, Batch number: 60, Loss: 3572.364501953125\n",
      "Epoch: 43, Batch number: 8, Loss: 3299.8330078125\n",
      "Epoch: 44, Batch number: 32, Loss: 3376.684814453125\n",
      "Epoch: 45, Batch number: 56, Loss: 3358.038818359375\n",
      "Epoch: 47, Batch number: 4, Loss: 3258.233642578125\n",
      "Epoch: 48, Batch number: 28, Loss: 3376.373291015625\n",
      "Epoch: 49, Batch number: 52, Loss: 3240.24169921875\n",
      "Epoch: 51, Batch number: 0, Loss: 3270.810546875\n",
      "Epoch: 52, Batch number: 24, Loss: 3217.797119140625\n",
      "Epoch: 53, Batch number: 48, Loss: 3312.1435546875\n",
      "Epoch: 54, Batch number: 72, Loss: 3369.02099609375\n",
      "Epoch: 56, Batch number: 20, Loss: 3228.855712890625\n",
      "Epoch: 57, Batch number: 44, Loss: 3224.0302734375\n",
      "Epoch: 58, Batch number: 68, Loss: 3251.679931640625\n",
      "Epoch: 60, Batch number: 16, Loss: 3222.708251953125\n",
      "Epoch: 61, Batch number: 40, Loss: 3331.829345703125\n",
      "Epoch: 62, Batch number: 64, Loss: 3342.136474609375\n",
      "Epoch: 64, Batch number: 12, Loss: 3136.332763671875\n",
      "Epoch: 65, Batch number: 36, Loss: 3335.092529296875\n",
      "Epoch: 66, Batch number: 60, Loss: 3301.76171875\n",
      "Epoch: 68, Batch number: 8, Loss: 3151.42431640625\n",
      "Epoch: 69, Batch number: 32, Loss: 3149.314208984375\n",
      "Epoch: 70, Batch number: 56, Loss: 3160.619873046875\n",
      "Epoch: 72, Batch number: 4, Loss: 3158.00927734375\n",
      "Epoch: 73, Batch number: 28, Loss: 3181.219482421875\n",
      "Epoch: 74, Batch number: 52, Loss: 3385.476806640625\n",
      "Epoch: 76, Batch number: 0, Loss: 3157.06591796875\n",
      "Epoch: 77, Batch number: 24, Loss: 3174.403564453125\n",
      "Epoch: 78, Batch number: 48, Loss: 3341.77978515625\n",
      "Epoch: 79, Batch number: 72, Loss: 3413.6669921875\n",
      "Epoch: 81, Batch number: 20, Loss: 3195.4677734375\n",
      "Epoch: 82, Batch number: 44, Loss: 3293.27099609375\n",
      "Epoch: 83, Batch number: 68, Loss: 3345.851318359375\n",
      "Epoch: 85, Batch number: 16, Loss: 3286.18310546875\n",
      "Epoch: 86, Batch number: 40, Loss: 3388.968505859375\n",
      "Epoch: 87, Batch number: 64, Loss: 3144.8466796875\n",
      "Epoch: 89, Batch number: 12, Loss: 3132.63037109375\n",
      "Epoch: 90, Batch number: 36, Loss: 3303.39501953125\n",
      "Epoch: 91, Batch number: 60, Loss: 3297.337646484375\n",
      "Epoch: 93, Batch number: 8, Loss: 3198.168701171875\n",
      "Epoch: 94, Batch number: 32, Loss: 3234.54248046875\n",
      "Epoch: 95, Batch number: 56, Loss: 3174.13671875\n",
      "Epoch: 97, Batch number: 4, Loss: 3094.60693359375\n",
      "Epoch: 98, Batch number: 28, Loss: 3076.522216796875\n",
      "Epoch: 99, Batch number: 52, Loss: 3302.925537109375\n",
      "Epoch: 101, Batch number: 0, Loss: 3078.10791015625\n",
      "Epoch: 102, Batch number: 24, Loss: 3386.142822265625\n",
      "Epoch: 103, Batch number: 48, Loss: 3305.928955078125\n",
      "Epoch: 104, Batch number: 72, Loss: 3252.269775390625\n",
      "Epoch: 106, Batch number: 20, Loss: 3102.21728515625\n",
      "Epoch: 107, Batch number: 44, Loss: 3315.221435546875\n",
      "Epoch: 108, Batch number: 68, Loss: 3197.03125\n",
      "Epoch: 110, Batch number: 16, Loss: 3230.175537109375\n",
      "Epoch: 111, Batch number: 40, Loss: 3196.543701171875\n",
      "Epoch: 112, Batch number: 64, Loss: 3299.101318359375\n",
      "Epoch: 114, Batch number: 12, Loss: 3176.608154296875\n",
      "Epoch: 115, Batch number: 36, Loss: 3337.112060546875\n",
      "Epoch: 116, Batch number: 60, Loss: 3213.55712890625\n",
      "Epoch: 118, Batch number: 8, Loss: 3089.735595703125\n",
      "Epoch: 119, Batch number: 32, Loss: 3212.4306640625\n",
      "Epoch: 120, Batch number: 56, Loss: 3265.13232421875\n",
      "Epoch: 122, Batch number: 4, Loss: 3224.87841796875\n",
      "Epoch: 123, Batch number: 28, Loss: 3300.727783203125\n",
      "Epoch: 124, Batch number: 52, Loss: 3270.26513671875\n",
      "Epoch: 126, Batch number: 0, Loss: 3225.98779296875\n",
      "Epoch: 127, Batch number: 24, Loss: 3360.15771484375\n",
      "Epoch: 128, Batch number: 48, Loss: 3192.9775390625\n",
      "Epoch: 129, Batch number: 72, Loss: 3287.282958984375\n",
      "Epoch: 131, Batch number: 20, Loss: 3208.025390625\n",
      "Epoch: 132, Batch number: 44, Loss: 3302.4091796875\n",
      "Epoch: 133, Batch number: 68, Loss: 3413.385986328125\n",
      "Epoch: 135, Batch number: 16, Loss: 3099.58447265625\n",
      "Epoch: 136, Batch number: 40, Loss: 3165.645263671875\n",
      "Epoch: 137, Batch number: 64, Loss: 3200.6708984375\n",
      "Epoch: 139, Batch number: 12, Loss: 2998.247802734375\n",
      "Epoch: 140, Batch number: 36, Loss: 3158.7138671875\n",
      "Epoch: 141, Batch number: 60, Loss: 3315.515869140625\n",
      "Epoch: 143, Batch number: 8, Loss: 3080.911865234375\n",
      "Epoch: 144, Batch number: 32, Loss: 3272.53759765625\n",
      "Epoch: 145, Batch number: 56, Loss: 3200.06494140625\n",
      "Epoch: 147, Batch number: 4, Loss: 3233.468017578125\n",
      "Epoch: 148, Batch number: 28, Loss: 3184.544189453125\n",
      "Epoch: 149, Batch number: 52, Loss: 3345.025146484375\n",
      "Epoch: 151, Batch number: 0, Loss: 3221.482177734375\n",
      "Epoch: 152, Batch number: 24, Loss: 3241.336669921875\n",
      "Epoch: 153, Batch number: 48, Loss: 3231.27294921875\n",
      "Epoch: 154, Batch number: 72, Loss: 3283.978515625\n",
      "Epoch: 156, Batch number: 20, Loss: 3308.286865234375\n",
      "Epoch: 157, Batch number: 44, Loss: 3210.821044921875\n",
      "Epoch: 158, Batch number: 68, Loss: 3406.607421875\n",
      "Epoch: 160, Batch number: 16, Loss: 3156.352294921875\n",
      "Epoch: 161, Batch number: 40, Loss: 3175.55224609375\n",
      "Epoch: 162, Batch number: 64, Loss: 3270.203857421875\n",
      "Epoch: 164, Batch number: 12, Loss: 3317.565673828125\n",
      "Epoch: 165, Batch number: 36, Loss: 3113.137939453125\n",
      "Epoch: 166, Batch number: 60, Loss: 3306.39111328125\n",
      "Epoch: 168, Batch number: 8, Loss: 3118.837890625\n",
      "Epoch: 169, Batch number: 32, Loss: 3172.160400390625\n",
      "Epoch: 170, Batch number: 56, Loss: 3435.050048828125\n",
      "Epoch: 172, Batch number: 4, Loss: 3070.507080078125\n",
      "Epoch: 173, Batch number: 28, Loss: 3232.395263671875\n",
      "Epoch: 174, Batch number: 52, Loss: 3306.7763671875\n",
      "Epoch: 176, Batch number: 0, Loss: 3121.91259765625\n",
      "Epoch: 177, Batch number: 24, Loss: 3297.424560546875\n",
      "Epoch: 178, Batch number: 48, Loss: 3223.1865234375\n",
      "Epoch: 179, Batch number: 72, Loss: 3190.7060546875\n",
      "Epoch: 181, Batch number: 20, Loss: 3238.123779296875\n",
      "Epoch: 182, Batch number: 44, Loss: 3143.16455078125\n",
      "Epoch: 183, Batch number: 68, Loss: 3261.041748046875\n",
      "Epoch: 185, Batch number: 16, Loss: 3111.504150390625\n",
      "Epoch: 186, Batch number: 40, Loss: 3252.282958984375\n",
      "Epoch: 187, Batch number: 64, Loss: 3250.689208984375\n",
      "Epoch: 189, Batch number: 12, Loss: 3225.619384765625\n",
      "Epoch: 190, Batch number: 36, Loss: 3134.69482421875\n",
      "Epoch: 191, Batch number: 60, Loss: 3068.45654296875\n",
      "Epoch: 193, Batch number: 8, Loss: 3198.53466796875\n",
      "Epoch: 194, Batch number: 32, Loss: 3345.333984375\n",
      "Epoch: 195, Batch number: 56, Loss: 3221.26171875\n",
      "Epoch: 197, Batch number: 4, Loss: 3213.802001953125\n",
      "Epoch: 198, Batch number: 28, Loss: 3171.697021484375\n",
      "Epoch: 199, Batch number: 52, Loss: 3267.10888671875\n",
      "Epoch: 201, Batch number: 0, Loss: 3125.972900390625\n",
      "Epoch: 202, Batch number: 24, Loss: 3146.70458984375\n",
      "Epoch: 203, Batch number: 48, Loss: 3280.25244140625\n",
      "Epoch: 204, Batch number: 72, Loss: 3280.427001953125\n",
      "Epoch: 206, Batch number: 20, Loss: 3185.2880859375\n",
      "Epoch: 207, Batch number: 44, Loss: 3279.7265625\n",
      "Epoch: 208, Batch number: 68, Loss: 3198.501708984375\n",
      "Epoch: 210, Batch number: 16, Loss: 3048.72216796875\n",
      "Epoch: 211, Batch number: 40, Loss: 3175.25537109375\n",
      "Epoch: 212, Batch number: 64, Loss: 3192.171875\n",
      "Epoch: 214, Batch number: 12, Loss: 3188.298095703125\n",
      "Epoch: 215, Batch number: 36, Loss: 3191.647216796875\n",
      "Epoch: 216, Batch number: 60, Loss: 3172.81591796875\n",
      "Epoch: 218, Batch number: 8, Loss: 3023.651123046875\n",
      "Epoch: 219, Batch number: 32, Loss: 3240.572509765625\n",
      "Epoch: 220, Batch number: 56, Loss: 3201.843505859375\n",
      "Epoch: 222, Batch number: 4, Loss: 3181.6708984375\n",
      "Epoch: 223, Batch number: 28, Loss: 3126.25390625\n",
      "Epoch: 224, Batch number: 52, Loss: 3005.505126953125\n",
      "Epoch: 226, Batch number: 0, Loss: 3087.36376953125\n",
      "Epoch: 227, Batch number: 24, Loss: 3326.9287109375\n",
      "Epoch: 228, Batch number: 48, Loss: 3128.9482421875\n",
      "Epoch: 229, Batch number: 72, Loss: 3344.961181640625\n",
      "Epoch: 231, Batch number: 20, Loss: 3220.503173828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232, Batch number: 44, Loss: 3238.28955078125\n",
      "Epoch: 233, Batch number: 68, Loss: 3207.71728515625\n",
      "Epoch: 235, Batch number: 16, Loss: 3133.994873046875\n",
      "Epoch: 236, Batch number: 40, Loss: 3215.276123046875\n",
      "Epoch: 237, Batch number: 64, Loss: 3363.839111328125\n",
      "Epoch: 239, Batch number: 12, Loss: 3158.80224609375\n",
      "Epoch: 240, Batch number: 36, Loss: 3205.058349609375\n",
      "Epoch: 241, Batch number: 60, Loss: 3321.3828125\n",
      "Epoch: 243, Batch number: 8, Loss: 2993.469970703125\n",
      "Epoch: 244, Batch number: 32, Loss: 3267.669189453125\n",
      "Epoch: 245, Batch number: 56, Loss: 3237.753173828125\n",
      "Epoch: 247, Batch number: 4, Loss: 3154.94482421875\n",
      "Epoch: 248, Batch number: 28, Loss: 3248.494873046875\n",
      "Epoch: 249, Batch number: 52, Loss: 3178.606689453125\n",
      "Epoch: 251, Batch number: 0, Loss: 3178.366943359375\n",
      "Epoch: 252, Batch number: 24, Loss: 3240.0927734375\n",
      "Epoch: 253, Batch number: 48, Loss: 3249.34814453125\n",
      "Epoch: 254, Batch number: 72, Loss: 3187.884033203125\n",
      "Epoch: 256, Batch number: 20, Loss: 3158.229736328125\n",
      "Epoch: 257, Batch number: 44, Loss: 3175.50048828125\n",
      "Epoch: 258, Batch number: 68, Loss: 3374.90185546875\n",
      "Epoch: 260, Batch number: 16, Loss: 3166.540283203125\n",
      "Epoch: 261, Batch number: 40, Loss: 3045.752197265625\n",
      "Epoch: 262, Batch number: 64, Loss: 3237.9931640625\n",
      "Epoch: 264, Batch number: 12, Loss: 3319.262451171875\n",
      "Epoch: 265, Batch number: 36, Loss: 3301.47265625\n",
      "Epoch: 266, Batch number: 60, Loss: 3350.478759765625\n",
      "Epoch: 268, Batch number: 8, Loss: 3100.221923828125\n",
      "Epoch: 269, Batch number: 32, Loss: 3190.564453125\n",
      "Epoch: 270, Batch number: 56, Loss: 3308.36083984375\n",
      "Epoch: 272, Batch number: 4, Loss: 3248.23193359375\n",
      "Epoch: 273, Batch number: 28, Loss: 3147.514404296875\n",
      "Epoch: 274, Batch number: 52, Loss: 3169.32275390625\n",
      "Epoch: 276, Batch number: 0, Loss: 3138.8544921875\n",
      "Epoch: 277, Batch number: 24, Loss: 3165.85205078125\n",
      "Epoch: 278, Batch number: 48, Loss: 3156.988525390625\n",
      "Epoch: 279, Batch number: 72, Loss: 3261.47705078125\n",
      "Epoch: 281, Batch number: 20, Loss: 3441.585693359375\n",
      "Epoch: 282, Batch number: 44, Loss: 3236.77783203125\n",
      "Epoch: 283, Batch number: 68, Loss: 3477.765869140625\n",
      "Epoch: 285, Batch number: 16, Loss: 3137.071533203125\n",
      "Epoch: 286, Batch number: 40, Loss: 3186.312744140625\n",
      "Epoch: 287, Batch number: 64, Loss: 3191.51318359375\n",
      "Epoch: 289, Batch number: 12, Loss: 3230.37890625\n",
      "Epoch: 290, Batch number: 36, Loss: 3195.881103515625\n",
      "Epoch: 291, Batch number: 60, Loss: 3382.72314453125\n",
      "Epoch: 293, Batch number: 8, Loss: 2957.813232421875\n",
      "Epoch: 294, Batch number: 32, Loss: 3090.07373046875\n",
      "Epoch: 295, Batch number: 56, Loss: 3246.536865234375\n",
      "Epoch: 297, Batch number: 4, Loss: 3136.210205078125\n",
      "Epoch: 298, Batch number: 28, Loss: 3265.353515625\n",
      "Epoch: 299, Batch number: 52, Loss: 3182.66845703125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 8041.49951171875\n",
      "Epoch: 2, Batch number: 24, Loss: 7034.95166015625\n",
      "Epoch: 3, Batch number: 48, Loss: 6569.00830078125\n",
      "Epoch: 4, Batch number: 72, Loss: 5999.16650390625\n",
      "Epoch: 6, Batch number: 20, Loss: 5572.048828125\n",
      "Epoch: 7, Batch number: 44, Loss: 5212.9453125\n",
      "Epoch: 8, Batch number: 68, Loss: 5019.1708984375\n",
      "Epoch: 10, Batch number: 16, Loss: 4749.90869140625\n",
      "Epoch: 11, Batch number: 40, Loss: 4743.0908203125\n",
      "Epoch: 12, Batch number: 64, Loss: 4292.06787109375\n",
      "Epoch: 14, Batch number: 12, Loss: 4158.3984375\n",
      "Epoch: 15, Batch number: 36, Loss: 4235.89501953125\n",
      "Epoch: 16, Batch number: 60, Loss: 4000.36328125\n",
      "Epoch: 18, Batch number: 8, Loss: 3847.059814453125\n",
      "Epoch: 19, Batch number: 32, Loss: 3847.01318359375\n",
      "Epoch: 20, Batch number: 56, Loss: 3822.660400390625\n",
      "Epoch: 22, Batch number: 4, Loss: 3570.1103515625\n",
      "Epoch: 23, Batch number: 28, Loss: 3514.341796875\n",
      "Epoch: 24, Batch number: 52, Loss: 3653.136962890625\n",
      "Epoch: 26, Batch number: 0, Loss: 3594.429443359375\n",
      "Epoch: 27, Batch number: 24, Loss: 3308.660400390625\n",
      "Epoch: 28, Batch number: 48, Loss: 3472.130615234375\n",
      "Epoch: 29, Batch number: 72, Loss: 3484.02099609375\n",
      "Epoch: 31, Batch number: 20, Loss: 3345.483154296875\n",
      "Epoch: 32, Batch number: 44, Loss: 3504.5234375\n",
      "Epoch: 33, Batch number: 68, Loss: 3377.780517578125\n",
      "Epoch: 35, Batch number: 16, Loss: 3372.750244140625\n",
      "Epoch: 36, Batch number: 40, Loss: 3362.181396484375\n",
      "Epoch: 37, Batch number: 64, Loss: 3407.166015625\n",
      "Epoch: 39, Batch number: 12, Loss: 3310.094482421875\n",
      "Epoch: 40, Batch number: 36, Loss: 3322.636474609375\n",
      "Epoch: 41, Batch number: 60, Loss: 3441.916748046875\n",
      "Epoch: 43, Batch number: 8, Loss: 3331.4248046875\n",
      "Epoch: 44, Batch number: 32, Loss: 3411.093505859375\n",
      "Epoch: 45, Batch number: 56, Loss: 3424.03076171875\n",
      "Epoch: 47, Batch number: 4, Loss: 3299.056884765625\n",
      "Epoch: 48, Batch number: 28, Loss: 3100.77880859375\n",
      "Epoch: 49, Batch number: 52, Loss: 3382.640625\n",
      "Epoch: 51, Batch number: 0, Loss: 3202.8154296875\n",
      "Epoch: 52, Batch number: 24, Loss: 3336.327392578125\n",
      "Epoch: 53, Batch number: 48, Loss: 3397.78955078125\n",
      "Epoch: 54, Batch number: 72, Loss: 3445.66943359375\n",
      "Epoch: 56, Batch number: 20, Loss: 3278.989990234375\n",
      "Epoch: 57, Batch number: 44, Loss: 3402.329833984375\n",
      "Epoch: 58, Batch number: 68, Loss: 3309.887451171875\n",
      "Epoch: 60, Batch number: 16, Loss: 3197.49072265625\n",
      "Epoch: 61, Batch number: 40, Loss: 3257.564208984375\n",
      "Epoch: 62, Batch number: 64, Loss: 3497.664306640625\n",
      "Epoch: 64, Batch number: 12, Loss: 3326.1484375\n",
      "Epoch: 65, Batch number: 36, Loss: 3250.0283203125\n",
      "Epoch: 66, Batch number: 60, Loss: 3284.391357421875\n",
      "Epoch: 68, Batch number: 8, Loss: 3043.779052734375\n",
      "Epoch: 69, Batch number: 32, Loss: 3184.358154296875\n",
      "Epoch: 70, Batch number: 56, Loss: 3182.732177734375\n",
      "Epoch: 72, Batch number: 4, Loss: 3096.437255859375\n",
      "Epoch: 73, Batch number: 28, Loss: 3180.348876953125\n",
      "Epoch: 74, Batch number: 52, Loss: 3262.507080078125\n",
      "Epoch: 76, Batch number: 0, Loss: 3222.946044921875\n",
      "Epoch: 77, Batch number: 24, Loss: 3287.380126953125\n",
      "Epoch: 78, Batch number: 48, Loss: 3239.176513671875\n",
      "Epoch: 79, Batch number: 72, Loss: 3252.53466796875\n",
      "Epoch: 81, Batch number: 20, Loss: 3324.507568359375\n",
      "Epoch: 82, Batch number: 44, Loss: 3221.65087890625\n",
      "Epoch: 83, Batch number: 68, Loss: 3252.757080078125\n",
      "Epoch: 85, Batch number: 16, Loss: 3100.511474609375\n",
      "Epoch: 86, Batch number: 40, Loss: 3146.498291015625\n",
      "Epoch: 87, Batch number: 64, Loss: 3329.327880859375\n",
      "Epoch: 89, Batch number: 12, Loss: 3083.2939453125\n",
      "Epoch: 90, Batch number: 36, Loss: 3322.828857421875\n",
      "Epoch: 91, Batch number: 60, Loss: 3397.055419921875\n",
      "Epoch: 93, Batch number: 8, Loss: 3233.36328125\n",
      "Epoch: 94, Batch number: 32, Loss: 3217.322021484375\n",
      "Epoch: 95, Batch number: 56, Loss: 3222.2275390625\n",
      "Epoch: 97, Batch number: 4, Loss: 3171.18896484375\n",
      "Epoch: 98, Batch number: 28, Loss: 3039.24755859375\n",
      "Epoch: 99, Batch number: 52, Loss: 3276.562744140625\n",
      "Epoch: 101, Batch number: 0, Loss: 3140.39794921875\n",
      "Epoch: 102, Batch number: 24, Loss: 3139.364013671875\n",
      "Epoch: 103, Batch number: 48, Loss: 3158.352783203125\n",
      "Epoch: 104, Batch number: 72, Loss: 3344.20703125\n",
      "Epoch: 106, Batch number: 20, Loss: 3273.6298828125\n",
      "Epoch: 107, Batch number: 44, Loss: 3333.307373046875\n",
      "Epoch: 108, Batch number: 68, Loss: 3373.9404296875\n",
      "Epoch: 110, Batch number: 16, Loss: 3127.793701171875\n",
      "Epoch: 111, Batch number: 40, Loss: 3352.440673828125\n",
      "Epoch: 112, Batch number: 64, Loss: 3324.876953125\n",
      "Epoch: 114, Batch number: 12, Loss: 3152.473876953125\n",
      "Epoch: 115, Batch number: 36, Loss: 3379.959716796875\n",
      "Epoch: 116, Batch number: 60, Loss: 3111.8759765625\n",
      "Epoch: 118, Batch number: 8, Loss: 3189.049072265625\n",
      "Epoch: 119, Batch number: 32, Loss: 3256.93896484375\n",
      "Epoch: 120, Batch number: 56, Loss: 3326.918701171875\n",
      "Epoch: 122, Batch number: 4, Loss: 3180.21142578125\n",
      "Epoch: 123, Batch number: 28, Loss: 3058.328857421875\n",
      "Epoch: 124, Batch number: 52, Loss: 3342.812744140625\n",
      "Epoch: 126, Batch number: 0, Loss: 3172.57421875\n",
      "Epoch: 127, Batch number: 24, Loss: 3174.682861328125\n",
      "Epoch: 128, Batch number: 48, Loss: 3206.63330078125\n",
      "Epoch: 129, Batch number: 72, Loss: 3301.72265625\n",
      "Epoch: 131, Batch number: 20, Loss: 3264.4482421875\n",
      "Epoch: 132, Batch number: 44, Loss: 3310.6689453125\n",
      "Epoch: 133, Batch number: 68, Loss: 3291.834716796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135, Batch number: 16, Loss: 3124.93310546875\n",
      "Epoch: 136, Batch number: 40, Loss: 3237.58984375\n",
      "Epoch: 137, Batch number: 64, Loss: 3339.762939453125\n",
      "Epoch: 139, Batch number: 12, Loss: 3202.705810546875\n",
      "Epoch: 140, Batch number: 36, Loss: 3198.29931640625\n",
      "Epoch: 141, Batch number: 60, Loss: 3182.290771484375\n",
      "Epoch: 143, Batch number: 8, Loss: 3047.377197265625\n",
      "Epoch: 144, Batch number: 32, Loss: 3148.760009765625\n",
      "Epoch: 145, Batch number: 56, Loss: 3467.434814453125\n",
      "Epoch: 147, Batch number: 4, Loss: 3173.24853515625\n",
      "Epoch: 148, Batch number: 28, Loss: 3391.103759765625\n",
      "Epoch: 149, Batch number: 52, Loss: 3420.861572265625\n",
      "Epoch: 151, Batch number: 0, Loss: 3155.928955078125\n",
      "Epoch: 152, Batch number: 24, Loss: 3201.55517578125\n",
      "Epoch: 153, Batch number: 48, Loss: 3229.9248046875\n",
      "Epoch: 154, Batch number: 72, Loss: 3463.863037109375\n",
      "Epoch: 156, Batch number: 20, Loss: 3263.87548828125\n",
      "Epoch: 157, Batch number: 44, Loss: 3245.53759765625\n",
      "Epoch: 158, Batch number: 68, Loss: 3280.0\n",
      "Epoch: 160, Batch number: 16, Loss: 3291.312744140625\n",
      "Epoch: 161, Batch number: 40, Loss: 3310.742431640625\n",
      "Epoch: 162, Batch number: 64, Loss: 3337.697998046875\n",
      "Epoch: 164, Batch number: 12, Loss: 3178.259765625\n",
      "Epoch: 165, Batch number: 36, Loss: 3276.8701171875\n",
      "Epoch: 166, Batch number: 60, Loss: 3350.2080078125\n",
      "Epoch: 168, Batch number: 8, Loss: 3120.01318359375\n",
      "Epoch: 169, Batch number: 32, Loss: 3134.255126953125\n",
      "Epoch: 170, Batch number: 56, Loss: 3400.463134765625\n",
      "Epoch: 172, Batch number: 4, Loss: 3214.107421875\n",
      "Epoch: 173, Batch number: 28, Loss: 3179.705078125\n",
      "Epoch: 174, Batch number: 52, Loss: 3331.503662109375\n",
      "Epoch: 176, Batch number: 0, Loss: 3228.8486328125\n",
      "Epoch: 177, Batch number: 24, Loss: 3209.691162109375\n",
      "Epoch: 178, Batch number: 48, Loss: 3382.158447265625\n",
      "Epoch: 179, Batch number: 72, Loss: 3305.450439453125\n",
      "Epoch: 181, Batch number: 20, Loss: 3149.876220703125\n",
      "Epoch: 182, Batch number: 44, Loss: 3143.055419921875\n",
      "Epoch: 183, Batch number: 68, Loss: 3398.4150390625\n",
      "Epoch: 185, Batch number: 16, Loss: 3241.055908203125\n",
      "Epoch: 186, Batch number: 40, Loss: 3178.662841796875\n",
      "Epoch: 187, Batch number: 64, Loss: 3406.123046875\n",
      "Epoch: 189, Batch number: 12, Loss: 3011.2216796875\n",
      "Epoch: 190, Batch number: 36, Loss: 3164.489501953125\n",
      "Epoch: 191, Batch number: 60, Loss: 3363.2548828125\n",
      "Epoch: 193, Batch number: 8, Loss: 3191.65771484375\n",
      "Epoch: 194, Batch number: 32, Loss: 3264.2265625\n",
      "Epoch: 195, Batch number: 56, Loss: 3312.480712890625\n",
      "Epoch: 197, Batch number: 4, Loss: 3143.41455078125\n",
      "Epoch: 198, Batch number: 28, Loss: 3294.989990234375\n",
      "Epoch: 199, Batch number: 52, Loss: 3200.576904296875\n",
      "Epoch: 201, Batch number: 0, Loss: 3276.043701171875\n",
      "Epoch: 202, Batch number: 24, Loss: 3159.46484375\n",
      "Epoch: 203, Batch number: 48, Loss: 3318.79736328125\n",
      "Epoch: 204, Batch number: 72, Loss: 3335.203369140625\n",
      "Epoch: 206, Batch number: 20, Loss: 3133.63037109375\n",
      "Epoch: 207, Batch number: 44, Loss: 3264.195068359375\n",
      "Epoch: 208, Batch number: 68, Loss: 3321.395751953125\n",
      "Epoch: 210, Batch number: 16, Loss: 3225.5390625\n",
      "Epoch: 211, Batch number: 40, Loss: 3342.110107421875\n",
      "Epoch: 212, Batch number: 64, Loss: 3417.11181640625\n",
      "Epoch: 214, Batch number: 12, Loss: 3074.412109375\n",
      "Epoch: 215, Batch number: 36, Loss: 3442.914794921875\n",
      "Epoch: 216, Batch number: 60, Loss: 3460.400146484375\n",
      "Epoch: 218, Batch number: 8, Loss: 3194.311767578125\n",
      "Epoch: 219, Batch number: 32, Loss: 3104.816162109375\n",
      "Epoch: 220, Batch number: 56, Loss: 3312.619384765625\n",
      "Epoch: 222, Batch number: 4, Loss: 3179.57421875\n",
      "Epoch: 223, Batch number: 28, Loss: 3172.8671875\n",
      "Epoch: 224, Batch number: 52, Loss: 3313.66650390625\n",
      "Epoch: 226, Batch number: 0, Loss: 3161.5341796875\n",
      "Epoch: 227, Batch number: 24, Loss: 3251.2939453125\n",
      "Epoch: 228, Batch number: 48, Loss: 3308.192138671875\n",
      "Epoch: 229, Batch number: 72, Loss: 3232.06591796875\n",
      "Epoch: 231, Batch number: 20, Loss: 3063.14013671875\n",
      "Epoch: 232, Batch number: 44, Loss: 3381.33935546875\n",
      "Epoch: 233, Batch number: 68, Loss: 3243.04345703125\n",
      "Epoch: 235, Batch number: 16, Loss: 3094.515625\n",
      "Epoch: 236, Batch number: 40, Loss: 3300.34375\n",
      "Epoch: 237, Batch number: 64, Loss: 3254.844482421875\n",
      "Epoch: 239, Batch number: 12, Loss: 3214.15380859375\n",
      "Epoch: 240, Batch number: 36, Loss: 3393.46923828125\n",
      "Epoch: 241, Batch number: 60, Loss: 3307.72314453125\n",
      "Epoch: 243, Batch number: 8, Loss: 3080.381103515625\n",
      "Epoch: 244, Batch number: 32, Loss: 3353.120361328125\n",
      "Epoch: 245, Batch number: 56, Loss: 3204.087890625\n",
      "Epoch: 247, Batch number: 4, Loss: 3196.8828125\n",
      "Epoch: 248, Batch number: 28, Loss: 3181.95556640625\n",
      "Epoch: 249, Batch number: 52, Loss: 3491.53173828125\n",
      "Epoch: 251, Batch number: 0, Loss: 3214.7080078125\n",
      "Epoch: 252, Batch number: 24, Loss: 3059.124267578125\n",
      "Epoch: 253, Batch number: 48, Loss: 3180.45654296875\n",
      "Epoch: 254, Batch number: 72, Loss: 3149.68017578125\n",
      "Epoch: 256, Batch number: 20, Loss: 3179.989990234375\n",
      "Epoch: 257, Batch number: 44, Loss: 3232.177001953125\n",
      "Epoch: 258, Batch number: 68, Loss: 3369.74072265625\n",
      "Epoch: 260, Batch number: 16, Loss: 3066.838134765625\n",
      "Epoch: 261, Batch number: 40, Loss: 3310.6298828125\n",
      "Epoch: 262, Batch number: 64, Loss: 3154.379638671875\n",
      "Epoch: 264, Batch number: 12, Loss: 3033.27978515625\n",
      "Epoch: 265, Batch number: 36, Loss: 3184.300537109375\n",
      "Epoch: 266, Batch number: 60, Loss: 3256.5380859375\n",
      "Epoch: 268, Batch number: 8, Loss: 3235.080810546875\n",
      "Epoch: 269, Batch number: 32, Loss: 3099.213134765625\n",
      "Epoch: 270, Batch number: 56, Loss: 3209.0595703125\n",
      "Epoch: 272, Batch number: 4, Loss: 3100.5458984375\n",
      "Epoch: 273, Batch number: 28, Loss: 3235.958740234375\n",
      "Epoch: 274, Batch number: 52, Loss: 3338.15185546875\n",
      "Epoch: 276, Batch number: 0, Loss: 3111.594970703125\n",
      "Epoch: 277, Batch number: 24, Loss: 3476.105712890625\n",
      "Epoch: 278, Batch number: 48, Loss: 3221.00048828125\n",
      "Epoch: 279, Batch number: 72, Loss: 3318.1318359375\n",
      "Epoch: 281, Batch number: 20, Loss: 3271.489013671875\n",
      "Epoch: 282, Batch number: 44, Loss: 3229.880126953125\n",
      "Epoch: 283, Batch number: 68, Loss: 3358.0673828125\n",
      "Epoch: 285, Batch number: 16, Loss: 3208.08544921875\n",
      "Epoch: 286, Batch number: 40, Loss: 3109.450439453125\n",
      "Epoch: 287, Batch number: 64, Loss: 3326.61767578125\n",
      "Epoch: 289, Batch number: 12, Loss: 3258.887939453125\n",
      "Epoch: 290, Batch number: 36, Loss: 3285.286376953125\n",
      "Epoch: 291, Batch number: 60, Loss: 3310.115478515625\n",
      "Epoch: 293, Batch number: 8, Loss: 3238.716796875\n",
      "Epoch: 294, Batch number: 32, Loss: 3184.926025390625\n",
      "Epoch: 295, Batch number: 56, Loss: 3379.8330078125\n",
      "Epoch: 297, Batch number: 4, Loss: 3188.593505859375\n",
      "Epoch: 298, Batch number: 28, Loss: 3140.13037109375\n",
      "Epoch: 299, Batch number: 52, Loss: 3310.767333984375\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15144.94921875\n",
      "Epoch: 2, Batch number: 24, Loss: 14699.0859375\n",
      "Epoch: 3, Batch number: 48, Loss: 14769.03125\n",
      "Epoch: 4, Batch number: 72, Loss: 13855.490234375\n",
      "Epoch: 6, Batch number: 20, Loss: 13644.9892578125\n",
      "Epoch: 7, Batch number: 44, Loss: 13241.255859375\n",
      "Epoch: 8, Batch number: 68, Loss: 12901.18359375\n",
      "Epoch: 10, Batch number: 16, Loss: 12645.69140625\n",
      "Epoch: 11, Batch number: 40, Loss: 12270.5849609375\n",
      "Epoch: 12, Batch number: 64, Loss: 12242.5\n",
      "Epoch: 14, Batch number: 12, Loss: 11833.8076171875\n",
      "Epoch: 15, Batch number: 36, Loss: 11793.4765625\n",
      "Epoch: 16, Batch number: 60, Loss: 11743.828125\n",
      "Epoch: 18, Batch number: 8, Loss: 11371.947265625\n",
      "Epoch: 19, Batch number: 32, Loss: 11282.1298828125\n",
      "Epoch: 20, Batch number: 56, Loss: 11034.958984375\n",
      "Epoch: 22, Batch number: 4, Loss: 10979.388671875\n",
      "Epoch: 23, Batch number: 28, Loss: 10931.9228515625\n",
      "Epoch: 24, Batch number: 52, Loss: 10646.90625\n",
      "Epoch: 26, Batch number: 0, Loss: 10669.1064453125\n",
      "Epoch: 27, Batch number: 24, Loss: 10538.1474609375\n",
      "Epoch: 28, Batch number: 48, Loss: 10557.87890625\n",
      "Epoch: 29, Batch number: 72, Loss: 10770.9609375\n",
      "Epoch: 31, Batch number: 20, Loss: 10162.7685546875\n",
      "Epoch: 32, Batch number: 44, Loss: 10159.92578125\n",
      "Epoch: 33, Batch number: 68, Loss: 10113.4384765625\n",
      "Epoch: 35, Batch number: 16, Loss: 9966.5498046875\n",
      "Epoch: 36, Batch number: 40, Loss: 10204.650390625\n",
      "Epoch: 37, Batch number: 64, Loss: 10231.1953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Batch number: 12, Loss: 9672.78125\n",
      "Epoch: 40, Batch number: 36, Loss: 9806.703125\n",
      "Epoch: 41, Batch number: 60, Loss: 9967.07421875\n",
      "Epoch: 43, Batch number: 8, Loss: 9735.8447265625\n",
      "Epoch: 44, Batch number: 32, Loss: 9873.1845703125\n",
      "Epoch: 45, Batch number: 56, Loss: 9593.8974609375\n",
      "Epoch: 47, Batch number: 4, Loss: 9590.404296875\n",
      "Epoch: 48, Batch number: 28, Loss: 9518.486328125\n",
      "Epoch: 49, Batch number: 52, Loss: 9552.1318359375\n",
      "Epoch: 51, Batch number: 0, Loss: 9626.1787109375\n",
      "Epoch: 52, Batch number: 24, Loss: 9564.9580078125\n",
      "Epoch: 53, Batch number: 48, Loss: 9436.1474609375\n",
      "Epoch: 54, Batch number: 72, Loss: 9489.7333984375\n",
      "Epoch: 56, Batch number: 20, Loss: 9174.2373046875\n",
      "Epoch: 57, Batch number: 44, Loss: 9075.7021484375\n",
      "Epoch: 58, Batch number: 68, Loss: 9274.322265625\n",
      "Epoch: 60, Batch number: 16, Loss: 9212.1533203125\n",
      "Epoch: 61, Batch number: 40, Loss: 9349.97265625\n",
      "Epoch: 62, Batch number: 64, Loss: 9362.6181640625\n",
      "Epoch: 64, Batch number: 12, Loss: 9253.3701171875\n",
      "Epoch: 65, Batch number: 36, Loss: 9322.203125\n",
      "Epoch: 66, Batch number: 60, Loss: 9124.69921875\n",
      "Epoch: 68, Batch number: 8, Loss: 8989.9296875\n",
      "Epoch: 69, Batch number: 32, Loss: 9075.0966796875\n",
      "Epoch: 70, Batch number: 56, Loss: 9277.193359375\n",
      "Epoch: 72, Batch number: 4, Loss: 8928.1982421875\n",
      "Epoch: 73, Batch number: 28, Loss: 9160.419921875\n",
      "Epoch: 74, Batch number: 52, Loss: 9010.7880859375\n",
      "Epoch: 76, Batch number: 0, Loss: 8998.029296875\n",
      "Epoch: 77, Batch number: 24, Loss: 8958.7578125\n",
      "Epoch: 78, Batch number: 48, Loss: 8763.185546875\n",
      "Epoch: 79, Batch number: 72, Loss: 8786.435546875\n",
      "Epoch: 81, Batch number: 20, Loss: 8928.73046875\n",
      "Epoch: 82, Batch number: 44, Loss: 8806.0341796875\n",
      "Epoch: 83, Batch number: 68, Loss: 9095.453125\n",
      "Epoch: 85, Batch number: 16, Loss: 8683.9345703125\n",
      "Epoch: 86, Batch number: 40, Loss: 8742.2021484375\n",
      "Epoch: 87, Batch number: 64, Loss: 8766.6826171875\n",
      "Epoch: 89, Batch number: 12, Loss: 8802.80859375\n",
      "Epoch: 90, Batch number: 36, Loss: 8859.357421875\n",
      "Epoch: 91, Batch number: 60, Loss: 8695.08203125\n",
      "Epoch: 93, Batch number: 8, Loss: 8641.7470703125\n",
      "Epoch: 94, Batch number: 32, Loss: 8812.0126953125\n",
      "Epoch: 95, Batch number: 56, Loss: 8636.6552734375\n",
      "Epoch: 97, Batch number: 4, Loss: 8769.6279296875\n",
      "Epoch: 98, Batch number: 28, Loss: 8604.697265625\n",
      "Epoch: 99, Batch number: 52, Loss: 8540.4326171875\n",
      "Epoch: 101, Batch number: 0, Loss: 8522.5810546875\n",
      "Epoch: 102, Batch number: 24, Loss: 8579.2890625\n",
      "Epoch: 103, Batch number: 48, Loss: 8555.451171875\n",
      "Epoch: 104, Batch number: 72, Loss: 8721.1357421875\n",
      "Epoch: 106, Batch number: 20, Loss: 8576.75390625\n",
      "Epoch: 107, Batch number: 44, Loss: 8515.7841796875\n",
      "Epoch: 108, Batch number: 68, Loss: 8747.6484375\n",
      "Epoch: 110, Batch number: 16, Loss: 8379.5224609375\n",
      "Epoch: 111, Batch number: 40, Loss: 8401.2744140625\n",
      "Epoch: 112, Batch number: 64, Loss: 8516.3984375\n",
      "Epoch: 114, Batch number: 12, Loss: 8215.69921875\n",
      "Epoch: 115, Batch number: 36, Loss: 8406.599609375\n",
      "Epoch: 116, Batch number: 60, Loss: 8501.31640625\n",
      "Epoch: 118, Batch number: 8, Loss: 8255.892578125\n",
      "Epoch: 119, Batch number: 32, Loss: 8200.869140625\n",
      "Epoch: 120, Batch number: 56, Loss: 8267.2421875\n",
      "Epoch: 122, Batch number: 4, Loss: 8302.029296875\n",
      "Epoch: 123, Batch number: 28, Loss: 8400.64453125\n",
      "Epoch: 124, Batch number: 52, Loss: 8390.0390625\n",
      "Epoch: 126, Batch number: 0, Loss: 8243.400390625\n",
      "Epoch: 127, Batch number: 24, Loss: 8038.38818359375\n",
      "Epoch: 128, Batch number: 48, Loss: 8035.43017578125\n",
      "Epoch: 129, Batch number: 72, Loss: 8235.5771484375\n",
      "Epoch: 131, Batch number: 20, Loss: 8320.3056640625\n",
      "Epoch: 132, Batch number: 44, Loss: 8261.6572265625\n",
      "Epoch: 133, Batch number: 68, Loss: 8096.36669921875\n",
      "Epoch: 135, Batch number: 16, Loss: 8395.201171875\n",
      "Epoch: 136, Batch number: 40, Loss: 8081.97314453125\n",
      "Epoch: 137, Batch number: 64, Loss: 8412.802734375\n",
      "Epoch: 139, Batch number: 12, Loss: 8140.9599609375\n",
      "Epoch: 140, Batch number: 36, Loss: 8029.16650390625\n",
      "Epoch: 141, Batch number: 60, Loss: 8098.947265625\n",
      "Epoch: 143, Batch number: 8, Loss: 8113.8779296875\n",
      "Epoch: 144, Batch number: 32, Loss: 7924.74072265625\n",
      "Epoch: 145, Batch number: 56, Loss: 8151.017578125\n",
      "Epoch: 147, Batch number: 4, Loss: 8334.66015625\n",
      "Epoch: 148, Batch number: 28, Loss: 8219.966796875\n",
      "Epoch: 149, Batch number: 52, Loss: 8135.33447265625\n",
      "Epoch: 151, Batch number: 0, Loss: 7890.703125\n",
      "Epoch: 152, Batch number: 24, Loss: 8381.1650390625\n",
      "Epoch: 153, Batch number: 48, Loss: 7625.06396484375\n",
      "Epoch: 154, Batch number: 72, Loss: 8047.47900390625\n",
      "Epoch: 156, Batch number: 20, Loss: 8041.658203125\n",
      "Epoch: 157, Batch number: 44, Loss: 7894.72021484375\n",
      "Epoch: 158, Batch number: 68, Loss: 8167.6123046875\n",
      "Epoch: 160, Batch number: 16, Loss: 8159.15673828125\n",
      "Epoch: 161, Batch number: 40, Loss: 8127.35009765625\n",
      "Epoch: 162, Batch number: 64, Loss: 8057.611328125\n",
      "Epoch: 164, Batch number: 12, Loss: 8082.2080078125\n",
      "Epoch: 165, Batch number: 36, Loss: 8241.6767578125\n",
      "Epoch: 166, Batch number: 60, Loss: 8036.72900390625\n",
      "Epoch: 168, Batch number: 8, Loss: 7973.09130859375\n",
      "Epoch: 169, Batch number: 32, Loss: 7753.91748046875\n",
      "Epoch: 170, Batch number: 56, Loss: 7942.39404296875\n",
      "Epoch: 172, Batch number: 4, Loss: 8075.1796875\n",
      "Epoch: 173, Batch number: 28, Loss: 7867.9453125\n",
      "Epoch: 174, Batch number: 52, Loss: 7778.55126953125\n",
      "Epoch: 176, Batch number: 0, Loss: 7981.46533203125\n",
      "Epoch: 177, Batch number: 24, Loss: 8048.62744140625\n",
      "Epoch: 178, Batch number: 48, Loss: 7744.24609375\n",
      "Epoch: 179, Batch number: 72, Loss: 7807.30859375\n",
      "Epoch: 181, Batch number: 20, Loss: 7979.1982421875\n",
      "Epoch: 182, Batch number: 44, Loss: 7680.787109375\n",
      "Epoch: 183, Batch number: 68, Loss: 7883.58642578125\n",
      "Epoch: 185, Batch number: 16, Loss: 7748.8515625\n",
      "Epoch: 186, Batch number: 40, Loss: 7761.44970703125\n",
      "Epoch: 187, Batch number: 64, Loss: 8011.35009765625\n",
      "Epoch: 189, Batch number: 12, Loss: 7840.8115234375\n",
      "Epoch: 190, Batch number: 36, Loss: 7928.70751953125\n",
      "Epoch: 191, Batch number: 60, Loss: 7805.79443359375\n",
      "Epoch: 193, Batch number: 8, Loss: 7637.541015625\n",
      "Epoch: 194, Batch number: 32, Loss: 7896.44287109375\n",
      "Epoch: 195, Batch number: 56, Loss: 7857.36181640625\n",
      "Epoch: 197, Batch number: 4, Loss: 7740.0234375\n",
      "Epoch: 198, Batch number: 28, Loss: 7925.7001953125\n",
      "Epoch: 199, Batch number: 52, Loss: 7687.83349609375\n",
      "Epoch: 201, Batch number: 0, Loss: 7673.08447265625\n",
      "Epoch: 202, Batch number: 24, Loss: 7667.6923828125\n",
      "Epoch: 203, Batch number: 48, Loss: 7636.71044921875\n",
      "Epoch: 204, Batch number: 72, Loss: 7800.3837890625\n",
      "Epoch: 206, Batch number: 20, Loss: 7576.70703125\n",
      "Epoch: 207, Batch number: 44, Loss: 7783.3056640625\n",
      "Epoch: 208, Batch number: 68, Loss: 7954.45751953125\n",
      "Epoch: 210, Batch number: 16, Loss: 7839.57275390625\n",
      "Epoch: 211, Batch number: 40, Loss: 7698.525390625\n",
      "Epoch: 212, Batch number: 64, Loss: 7602.08544921875\n",
      "Epoch: 214, Batch number: 12, Loss: 7770.5341796875\n",
      "Epoch: 215, Batch number: 36, Loss: 7659.72265625\n",
      "Epoch: 216, Batch number: 60, Loss: 7985.5205078125\n",
      "Epoch: 218, Batch number: 8, Loss: 7518.40380859375\n",
      "Epoch: 219, Batch number: 32, Loss: 7544.337890625\n",
      "Epoch: 220, Batch number: 56, Loss: 7683.83251953125\n",
      "Epoch: 222, Batch number: 4, Loss: 7858.41552734375\n",
      "Epoch: 223, Batch number: 28, Loss: 7634.6767578125\n",
      "Epoch: 224, Batch number: 52, Loss: 7395.380859375\n",
      "Epoch: 226, Batch number: 0, Loss: 7544.689453125\n",
      "Epoch: 227, Batch number: 24, Loss: 7783.8173828125\n",
      "Epoch: 228, Batch number: 48, Loss: 7817.9638671875\n",
      "Epoch: 229, Batch number: 72, Loss: 7534.1923828125\n",
      "Epoch: 231, Batch number: 20, Loss: 7802.94677734375\n",
      "Epoch: 232, Batch number: 44, Loss: 7739.13232421875\n",
      "Epoch: 233, Batch number: 68, Loss: 7760.00830078125\n",
      "Epoch: 235, Batch number: 16, Loss: 7450.1875\n",
      "Epoch: 236, Batch number: 40, Loss: 7790.2939453125\n",
      "Epoch: 237, Batch number: 64, Loss: 7644.88818359375\n",
      "Epoch: 239, Batch number: 12, Loss: 7656.09765625\n",
      "Epoch: 240, Batch number: 36, Loss: 7891.47265625\n",
      "Epoch: 241, Batch number: 60, Loss: 7590.205078125\n",
      "Epoch: 243, Batch number: 8, Loss: 7297.8291015625\n",
      "Epoch: 244, Batch number: 32, Loss: 7729.54296875\n",
      "Epoch: 245, Batch number: 56, Loss: 7501.748046875\n",
      "Epoch: 247, Batch number: 4, Loss: 7494.60595703125\n",
      "Epoch: 248, Batch number: 28, Loss: 7518.1181640625\n",
      "Epoch: 249, Batch number: 52, Loss: 7438.06640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 251, Batch number: 0, Loss: 7667.75390625\n",
      "Epoch: 252, Batch number: 24, Loss: 7475.31689453125\n",
      "Epoch: 253, Batch number: 48, Loss: 7643.39306640625\n",
      "Epoch: 254, Batch number: 72, Loss: 7316.578125\n",
      "Epoch: 256, Batch number: 20, Loss: 7373.359375\n",
      "Epoch: 257, Batch number: 44, Loss: 7400.08837890625\n",
      "Epoch: 258, Batch number: 68, Loss: 7447.3408203125\n",
      "Epoch: 260, Batch number: 16, Loss: 7266.2568359375\n",
      "Epoch: 261, Batch number: 40, Loss: 7576.00634765625\n",
      "Epoch: 262, Batch number: 64, Loss: 7575.17236328125\n",
      "Epoch: 264, Batch number: 12, Loss: 7555.60791015625\n",
      "Epoch: 265, Batch number: 36, Loss: 7527.015625\n",
      "Epoch: 266, Batch number: 60, Loss: 7690.62451171875\n",
      "Epoch: 268, Batch number: 8, Loss: 7654.890625\n",
      "Epoch: 269, Batch number: 32, Loss: 7517.98779296875\n",
      "Epoch: 270, Batch number: 56, Loss: 7711.6630859375\n",
      "Epoch: 272, Batch number: 4, Loss: 7332.67822265625\n",
      "Epoch: 273, Batch number: 28, Loss: 7586.30712890625\n",
      "Epoch: 274, Batch number: 52, Loss: 7560.482421875\n",
      "Epoch: 276, Batch number: 0, Loss: 7468.6044921875\n",
      "Epoch: 277, Batch number: 24, Loss: 7534.38525390625\n",
      "Epoch: 278, Batch number: 48, Loss: 7686.41015625\n",
      "Epoch: 279, Batch number: 72, Loss: 7480.83056640625\n",
      "Epoch: 281, Batch number: 20, Loss: 7474.51953125\n",
      "Epoch: 282, Batch number: 44, Loss: 7385.9287109375\n",
      "Epoch: 283, Batch number: 68, Loss: 7681.2998046875\n",
      "Epoch: 285, Batch number: 16, Loss: 7590.59375\n",
      "Epoch: 286, Batch number: 40, Loss: 7616.49560546875\n",
      "Epoch: 287, Batch number: 64, Loss: 7544.666015625\n",
      "Epoch: 289, Batch number: 12, Loss: 7574.76123046875\n",
      "Epoch: 290, Batch number: 36, Loss: 7501.24853515625\n",
      "Epoch: 291, Batch number: 60, Loss: 7483.44873046875\n",
      "Epoch: 293, Batch number: 8, Loss: 7593.740234375\n",
      "Epoch: 294, Batch number: 32, Loss: 7420.37060546875\n",
      "Epoch: 295, Batch number: 56, Loss: 7408.3525390625\n",
      "Epoch: 297, Batch number: 4, Loss: 7181.626953125\n",
      "Epoch: 298, Batch number: 28, Loss: 7229.3447265625\n",
      "Epoch: 299, Batch number: 52, Loss: 7410.8720703125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15150.455078125\n",
      "Epoch: 2, Batch number: 24, Loss: 14790.7353515625\n",
      "Epoch: 3, Batch number: 48, Loss: 14150.857421875\n",
      "Epoch: 4, Batch number: 72, Loss: 13473.8828125\n",
      "Epoch: 6, Batch number: 20, Loss: 12710.6171875\n",
      "Epoch: 7, Batch number: 44, Loss: 12573.4208984375\n",
      "Epoch: 8, Batch number: 68, Loss: 12216.845703125\n",
      "Epoch: 10, Batch number: 16, Loss: 11987.365234375\n",
      "Epoch: 11, Batch number: 40, Loss: 11296.0986328125\n",
      "Epoch: 12, Batch number: 64, Loss: 11381.2578125\n",
      "Epoch: 14, Batch number: 12, Loss: 11082.3662109375\n",
      "Epoch: 15, Batch number: 36, Loss: 10571.3837890625\n",
      "Epoch: 16, Batch number: 60, Loss: 10722.810546875\n",
      "Epoch: 18, Batch number: 8, Loss: 10150.0078125\n",
      "Epoch: 19, Batch number: 32, Loss: 10026.5966796875\n",
      "Epoch: 20, Batch number: 56, Loss: 10069.6171875\n",
      "Epoch: 22, Batch number: 4, Loss: 10035.853515625\n",
      "Epoch: 23, Batch number: 28, Loss: 9875.5888671875\n",
      "Epoch: 24, Batch number: 52, Loss: 9826.8828125\n",
      "Epoch: 26, Batch number: 0, Loss: 9582.3818359375\n",
      "Epoch: 27, Batch number: 24, Loss: 9453.49609375\n",
      "Epoch: 28, Batch number: 48, Loss: 9426.59375\n",
      "Epoch: 29, Batch number: 72, Loss: 9250.158203125\n",
      "Epoch: 31, Batch number: 20, Loss: 9389.2978515625\n",
      "Epoch: 32, Batch number: 44, Loss: 8887.841796875\n",
      "Epoch: 33, Batch number: 68, Loss: 8960.92578125\n",
      "Epoch: 35, Batch number: 16, Loss: 8884.0625\n",
      "Epoch: 36, Batch number: 40, Loss: 9056.38671875\n",
      "Epoch: 37, Batch number: 64, Loss: 9062.1650390625\n",
      "Epoch: 39, Batch number: 12, Loss: 8974.662109375\n",
      "Epoch: 40, Batch number: 36, Loss: 8719.5693359375\n",
      "Epoch: 41, Batch number: 60, Loss: 8791.6474609375\n",
      "Epoch: 43, Batch number: 8, Loss: 8583.697265625\n",
      "Epoch: 44, Batch number: 32, Loss: 8650.66796875\n",
      "Epoch: 45, Batch number: 56, Loss: 8623.8095703125\n",
      "Epoch: 47, Batch number: 4, Loss: 8521.396484375\n",
      "Epoch: 48, Batch number: 28, Loss: 8515.6181640625\n",
      "Epoch: 49, Batch number: 52, Loss: 8674.4189453125\n",
      "Epoch: 51, Batch number: 0, Loss: 8514.6484375\n",
      "Epoch: 52, Batch number: 24, Loss: 8447.41796875\n",
      "Epoch: 53, Batch number: 48, Loss: 8445.626953125\n",
      "Epoch: 54, Batch number: 72, Loss: 8510.4365234375\n",
      "Epoch: 56, Batch number: 20, Loss: 8461.6064453125\n",
      "Epoch: 57, Batch number: 44, Loss: 8335.5986328125\n",
      "Epoch: 58, Batch number: 68, Loss: 8264.4853515625\n",
      "Epoch: 60, Batch number: 16, Loss: 8045.466796875\n",
      "Epoch: 61, Batch number: 40, Loss: 8216.537109375\n",
      "Epoch: 62, Batch number: 64, Loss: 8118.0498046875\n",
      "Epoch: 64, Batch number: 12, Loss: 8339.8251953125\n",
      "Epoch: 65, Batch number: 36, Loss: 8140.08154296875\n",
      "Epoch: 66, Batch number: 60, Loss: 8259.65234375\n",
      "Epoch: 68, Batch number: 8, Loss: 8030.92138671875\n",
      "Epoch: 69, Batch number: 32, Loss: 8074.4677734375\n",
      "Epoch: 70, Batch number: 56, Loss: 8012.6064453125\n",
      "Epoch: 72, Batch number: 4, Loss: 8226.7158203125\n",
      "Epoch: 73, Batch number: 28, Loss: 8102.109375\n",
      "Epoch: 74, Batch number: 52, Loss: 8015.51318359375\n",
      "Epoch: 76, Batch number: 0, Loss: 7854.0673828125\n",
      "Epoch: 77, Batch number: 24, Loss: 8072.65087890625\n",
      "Epoch: 78, Batch number: 48, Loss: 8145.19287109375\n",
      "Epoch: 79, Batch number: 72, Loss: 8064.5546875\n",
      "Epoch: 81, Batch number: 20, Loss: 7712.9775390625\n",
      "Epoch: 82, Batch number: 44, Loss: 7737.232421875\n",
      "Epoch: 83, Batch number: 68, Loss: 8038.65380859375\n",
      "Epoch: 85, Batch number: 16, Loss: 7640.72265625\n",
      "Epoch: 86, Batch number: 40, Loss: 7850.99267578125\n",
      "Epoch: 87, Batch number: 64, Loss: 7876.13720703125\n",
      "Epoch: 89, Batch number: 12, Loss: 7762.1572265625\n",
      "Epoch: 90, Batch number: 36, Loss: 7792.5810546875\n",
      "Epoch: 91, Batch number: 60, Loss: 8004.90625\n",
      "Epoch: 93, Batch number: 8, Loss: 7849.4697265625\n",
      "Epoch: 94, Batch number: 32, Loss: 7772.23779296875\n",
      "Epoch: 95, Batch number: 56, Loss: 7731.1162109375\n",
      "Epoch: 97, Batch number: 4, Loss: 7568.13134765625\n",
      "Epoch: 98, Batch number: 28, Loss: 7814.37939453125\n",
      "Epoch: 99, Batch number: 52, Loss: 7864.4140625\n",
      "Epoch: 101, Batch number: 0, Loss: 7877.15478515625\n",
      "Epoch: 102, Batch number: 24, Loss: 7528.76025390625\n",
      "Epoch: 103, Batch number: 48, Loss: 7883.71044921875\n",
      "Epoch: 104, Batch number: 72, Loss: 7681.4033203125\n",
      "Epoch: 106, Batch number: 20, Loss: 7822.10888671875\n",
      "Epoch: 107, Batch number: 44, Loss: 7681.96533203125\n",
      "Epoch: 108, Batch number: 68, Loss: 7519.01416015625\n",
      "Epoch: 110, Batch number: 16, Loss: 7529.505859375\n",
      "Epoch: 111, Batch number: 40, Loss: 7705.732421875\n",
      "Epoch: 112, Batch number: 64, Loss: 7459.0234375\n",
      "Epoch: 114, Batch number: 12, Loss: 7694.52392578125\n",
      "Epoch: 115, Batch number: 36, Loss: 7592.79345703125\n",
      "Epoch: 116, Batch number: 60, Loss: 7624.6640625\n",
      "Epoch: 118, Batch number: 8, Loss: 7224.04638671875\n",
      "Epoch: 119, Batch number: 32, Loss: 7202.72509765625\n",
      "Epoch: 120, Batch number: 56, Loss: 7523.1650390625\n",
      "Epoch: 122, Batch number: 4, Loss: 7591.0703125\n",
      "Epoch: 123, Batch number: 28, Loss: 7440.47705078125\n",
      "Epoch: 124, Batch number: 52, Loss: 7512.64599609375\n",
      "Epoch: 126, Batch number: 0, Loss: 7520.4912109375\n",
      "Epoch: 127, Batch number: 24, Loss: 7607.85595703125\n",
      "Epoch: 128, Batch number: 48, Loss: 7675.3564453125\n",
      "Epoch: 129, Batch number: 72, Loss: 7594.380859375\n",
      "Epoch: 131, Batch number: 20, Loss: 7265.22607421875\n",
      "Epoch: 132, Batch number: 44, Loss: 7578.78515625\n",
      "Epoch: 133, Batch number: 68, Loss: 7286.833984375\n",
      "Epoch: 135, Batch number: 16, Loss: 7348.31982421875\n",
      "Epoch: 136, Batch number: 40, Loss: 7597.07373046875\n",
      "Epoch: 137, Batch number: 64, Loss: 7281.16455078125\n",
      "Epoch: 139, Batch number: 12, Loss: 7350.607421875\n",
      "Epoch: 140, Batch number: 36, Loss: 7230.81298828125\n",
      "Epoch: 141, Batch number: 60, Loss: 7288.47705078125\n",
      "Epoch: 143, Batch number: 8, Loss: 7303.216796875\n",
      "Epoch: 144, Batch number: 32, Loss: 7374.69287109375\n",
      "Epoch: 145, Batch number: 56, Loss: 7298.81201171875\n",
      "Epoch: 147, Batch number: 4, Loss: 7202.27099609375\n",
      "Epoch: 148, Batch number: 28, Loss: 7357.5625\n",
      "Epoch: 149, Batch number: 52, Loss: 7271.25146484375\n",
      "Epoch: 151, Batch number: 0, Loss: 7375.078125\n",
      "Epoch: 152, Batch number: 24, Loss: 7449.765625\n",
      "Epoch: 153, Batch number: 48, Loss: 7136.65185546875\n",
      "Epoch: 154, Batch number: 72, Loss: 7356.7255859375\n",
      "Epoch: 156, Batch number: 20, Loss: 7395.6435546875\n",
      "Epoch: 157, Batch number: 44, Loss: 7303.072265625\n",
      "Epoch: 158, Batch number: 68, Loss: 7406.509765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Batch number: 16, Loss: 7234.552734375\n",
      "Epoch: 161, Batch number: 40, Loss: 7137.5341796875\n",
      "Epoch: 162, Batch number: 64, Loss: 7187.59765625\n",
      "Epoch: 164, Batch number: 12, Loss: 7301.94921875\n",
      "Epoch: 165, Batch number: 36, Loss: 7045.6708984375\n",
      "Epoch: 166, Batch number: 60, Loss: 7203.33544921875\n",
      "Epoch: 168, Batch number: 8, Loss: 7141.78173828125\n",
      "Epoch: 169, Batch number: 32, Loss: 7021.232421875\n",
      "Epoch: 170, Batch number: 56, Loss: 7038.04833984375\n",
      "Epoch: 172, Batch number: 4, Loss: 7080.3095703125\n",
      "Epoch: 173, Batch number: 28, Loss: 7116.34716796875\n",
      "Epoch: 174, Batch number: 52, Loss: 7178.11474609375\n",
      "Epoch: 176, Batch number: 0, Loss: 7043.26171875\n",
      "Epoch: 177, Batch number: 24, Loss: 7387.05859375\n",
      "Epoch: 178, Batch number: 48, Loss: 7417.2021484375\n",
      "Epoch: 179, Batch number: 72, Loss: 7106.48486328125\n",
      "Epoch: 181, Batch number: 20, Loss: 7039.306640625\n",
      "Epoch: 182, Batch number: 44, Loss: 7051.4287109375\n",
      "Epoch: 183, Batch number: 68, Loss: 7147.07470703125\n",
      "Epoch: 185, Batch number: 16, Loss: 7088.892578125\n",
      "Epoch: 186, Batch number: 40, Loss: 7338.7041015625\n",
      "Epoch: 187, Batch number: 64, Loss: 7456.1865234375\n",
      "Epoch: 189, Batch number: 12, Loss: 7031.23486328125\n",
      "Epoch: 190, Batch number: 36, Loss: 7284.697265625\n",
      "Epoch: 191, Batch number: 60, Loss: 7288.01806640625\n",
      "Epoch: 193, Batch number: 8, Loss: 7079.02783203125\n",
      "Epoch: 194, Batch number: 32, Loss: 7160.126953125\n",
      "Epoch: 195, Batch number: 56, Loss: 7286.2666015625\n",
      "Epoch: 197, Batch number: 4, Loss: 7294.056640625\n",
      "Epoch: 198, Batch number: 28, Loss: 7272.28759765625\n",
      "Epoch: 199, Batch number: 52, Loss: 7223.87353515625\n",
      "Epoch: 201, Batch number: 0, Loss: 7087.00927734375\n",
      "Epoch: 202, Batch number: 24, Loss: 6981.9208984375\n",
      "Epoch: 203, Batch number: 48, Loss: 7091.09912109375\n",
      "Epoch: 204, Batch number: 72, Loss: 6825.568359375\n",
      "Epoch: 206, Batch number: 20, Loss: 7460.28076171875\n",
      "Epoch: 207, Batch number: 44, Loss: 7228.59912109375\n",
      "Epoch: 208, Batch number: 68, Loss: 7199.73095703125\n",
      "Epoch: 210, Batch number: 16, Loss: 7176.3701171875\n",
      "Epoch: 211, Batch number: 40, Loss: 7252.37060546875\n",
      "Epoch: 212, Batch number: 64, Loss: 7230.537109375\n",
      "Epoch: 214, Batch number: 12, Loss: 7216.17626953125\n",
      "Epoch: 215, Batch number: 36, Loss: 7065.43017578125\n",
      "Epoch: 216, Batch number: 60, Loss: 7317.6845703125\n",
      "Epoch: 218, Batch number: 8, Loss: 6989.17724609375\n",
      "Epoch: 219, Batch number: 32, Loss: 6978.5009765625\n",
      "Epoch: 220, Batch number: 56, Loss: 7202.8203125\n",
      "Epoch: 222, Batch number: 4, Loss: 6861.3115234375\n",
      "Epoch: 223, Batch number: 28, Loss: 7182.19287109375\n",
      "Epoch: 224, Batch number: 52, Loss: 7267.2568359375\n",
      "Epoch: 226, Batch number: 0, Loss: 7050.884765625\n",
      "Epoch: 227, Batch number: 24, Loss: 6825.01513671875\n",
      "Epoch: 228, Batch number: 48, Loss: 7141.63427734375\n",
      "Epoch: 229, Batch number: 72, Loss: 7324.85693359375\n",
      "Epoch: 231, Batch number: 20, Loss: 7108.98388671875\n",
      "Epoch: 232, Batch number: 44, Loss: 7188.435546875\n",
      "Epoch: 233, Batch number: 68, Loss: 7053.66650390625\n",
      "Epoch: 235, Batch number: 16, Loss: 6770.86865234375\n",
      "Epoch: 236, Batch number: 40, Loss: 7142.7080078125\n",
      "Epoch: 237, Batch number: 64, Loss: 7034.87890625\n",
      "Epoch: 239, Batch number: 12, Loss: 7002.2109375\n",
      "Epoch: 240, Batch number: 36, Loss: 7292.28955078125\n",
      "Epoch: 241, Batch number: 60, Loss: 7431.4501953125\n",
      "Epoch: 243, Batch number: 8, Loss: 6974.0751953125\n",
      "Epoch: 244, Batch number: 32, Loss: 7085.83251953125\n",
      "Epoch: 245, Batch number: 56, Loss: 7082.59912109375\n",
      "Epoch: 247, Batch number: 4, Loss: 7252.6904296875\n",
      "Epoch: 248, Batch number: 28, Loss: 6892.1630859375\n",
      "Epoch: 249, Batch number: 52, Loss: 6891.23828125\n",
      "Epoch: 251, Batch number: 0, Loss: 6953.4375\n",
      "Epoch: 252, Batch number: 24, Loss: 6976.92138671875\n",
      "Epoch: 253, Batch number: 48, Loss: 7094.46044921875\n",
      "Epoch: 254, Batch number: 72, Loss: 7216.97509765625\n",
      "Epoch: 256, Batch number: 20, Loss: 6952.44091796875\n",
      "Epoch: 257, Batch number: 44, Loss: 6918.39794921875\n",
      "Epoch: 258, Batch number: 68, Loss: 6931.26611328125\n",
      "Epoch: 260, Batch number: 16, Loss: 6873.580078125\n",
      "Epoch: 261, Batch number: 40, Loss: 7119.30615234375\n",
      "Epoch: 262, Batch number: 64, Loss: 6903.6435546875\n",
      "Epoch: 264, Batch number: 12, Loss: 7022.8916015625\n",
      "Epoch: 265, Batch number: 36, Loss: 6956.865234375\n",
      "Epoch: 266, Batch number: 60, Loss: 7179.4013671875\n",
      "Epoch: 268, Batch number: 8, Loss: 6964.79345703125\n",
      "Epoch: 269, Batch number: 32, Loss: 6815.13818359375\n",
      "Epoch: 270, Batch number: 56, Loss: 7405.26513671875\n",
      "Epoch: 272, Batch number: 4, Loss: 6977.7900390625\n",
      "Epoch: 273, Batch number: 28, Loss: 7080.27392578125\n",
      "Epoch: 274, Batch number: 52, Loss: 7214.0625\n",
      "Epoch: 276, Batch number: 0, Loss: 6761.28369140625\n",
      "Epoch: 277, Batch number: 24, Loss: 7177.99609375\n",
      "Epoch: 278, Batch number: 48, Loss: 7102.5439453125\n",
      "Epoch: 279, Batch number: 72, Loss: 7073.810546875\n",
      "Epoch: 281, Batch number: 20, Loss: 6990.56884765625\n",
      "Epoch: 282, Batch number: 44, Loss: 6941.05908203125\n",
      "Epoch: 283, Batch number: 68, Loss: 7002.0302734375\n",
      "Epoch: 285, Batch number: 16, Loss: 7163.37939453125\n",
      "Epoch: 286, Batch number: 40, Loss: 7262.74609375\n",
      "Epoch: 287, Batch number: 64, Loss: 6793.88671875\n",
      "Epoch: 289, Batch number: 12, Loss: 6856.046875\n",
      "Epoch: 290, Batch number: 36, Loss: 7169.23095703125\n",
      "Epoch: 291, Batch number: 60, Loss: 7255.67431640625\n",
      "Epoch: 293, Batch number: 8, Loss: 7277.56494140625\n",
      "Epoch: 294, Batch number: 32, Loss: 7035.71826171875\n",
      "Epoch: 295, Batch number: 56, Loss: 6710.28125\n",
      "Epoch: 297, Batch number: 4, Loss: 7050.32763671875\n",
      "Epoch: 298, Batch number: 28, Loss: 6924.43115234375\n",
      "Epoch: 299, Batch number: 52, Loss: 6849.49560546875\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15584.326171875\n",
      "Epoch: 2, Batch number: 24, Loss: 14276.2841796875\n",
      "Epoch: 3, Batch number: 48, Loss: 13654.390625\n",
      "Epoch: 4, Batch number: 72, Loss: 13362.9873046875\n",
      "Epoch: 6, Batch number: 20, Loss: 12443.0\n",
      "Epoch: 7, Batch number: 44, Loss: 12078.25\n",
      "Epoch: 8, Batch number: 68, Loss: 11613.5859375\n",
      "Epoch: 10, Batch number: 16, Loss: 11249.6318359375\n",
      "Epoch: 11, Batch number: 40, Loss: 11156.7685546875\n",
      "Epoch: 12, Batch number: 64, Loss: 10530.1787109375\n",
      "Epoch: 14, Batch number: 12, Loss: 10236.439453125\n",
      "Epoch: 15, Batch number: 36, Loss: 10218.2744140625\n",
      "Epoch: 16, Batch number: 60, Loss: 10166.9404296875\n",
      "Epoch: 18, Batch number: 8, Loss: 9729.3564453125\n",
      "Epoch: 19, Batch number: 32, Loss: 9481.767578125\n",
      "Epoch: 20, Batch number: 56, Loss: 9544.5654296875\n",
      "Epoch: 22, Batch number: 4, Loss: 9276.25390625\n",
      "Epoch: 23, Batch number: 28, Loss: 9010.9013671875\n",
      "Epoch: 24, Batch number: 52, Loss: 9186.6416015625\n",
      "Epoch: 26, Batch number: 0, Loss: 8767.0087890625\n",
      "Epoch: 27, Batch number: 24, Loss: 8779.22265625\n",
      "Epoch: 28, Batch number: 48, Loss: 8634.818359375\n",
      "Epoch: 29, Batch number: 72, Loss: 8858.4111328125\n",
      "Epoch: 31, Batch number: 20, Loss: 8619.0361328125\n",
      "Epoch: 32, Batch number: 44, Loss: 8589.283203125\n",
      "Epoch: 33, Batch number: 68, Loss: 8628.6865234375\n",
      "Epoch: 35, Batch number: 16, Loss: 8643.001953125\n",
      "Epoch: 36, Batch number: 40, Loss: 8277.0244140625\n",
      "Epoch: 37, Batch number: 64, Loss: 8302.4111328125\n",
      "Epoch: 39, Batch number: 12, Loss: 8191.98095703125\n",
      "Epoch: 40, Batch number: 36, Loss: 8378.5126953125\n",
      "Epoch: 41, Batch number: 60, Loss: 8217.0478515625\n",
      "Epoch: 43, Batch number: 8, Loss: 8068.88525390625\n",
      "Epoch: 44, Batch number: 32, Loss: 7835.61474609375\n",
      "Epoch: 45, Batch number: 56, Loss: 8223.05078125\n",
      "Epoch: 47, Batch number: 4, Loss: 8008.6826171875\n",
      "Epoch: 48, Batch number: 28, Loss: 8134.98974609375\n",
      "Epoch: 49, Batch number: 52, Loss: 8064.125\n",
      "Epoch: 51, Batch number: 0, Loss: 7948.94775390625\n",
      "Epoch: 52, Batch number: 24, Loss: 7796.30126953125\n",
      "Epoch: 53, Batch number: 48, Loss: 7762.40185546875\n",
      "Epoch: 54, Batch number: 72, Loss: 7872.77978515625\n",
      "Epoch: 56, Batch number: 20, Loss: 7953.283203125\n",
      "Epoch: 57, Batch number: 44, Loss: 7850.68798828125\n",
      "Epoch: 58, Batch number: 68, Loss: 7932.376953125\n",
      "Epoch: 60, Batch number: 16, Loss: 7442.15283203125\n",
      "Epoch: 61, Batch number: 40, Loss: 7814.47705078125\n",
      "Epoch: 62, Batch number: 64, Loss: 7604.16162109375\n",
      "Epoch: 64, Batch number: 12, Loss: 7669.47705078125\n",
      "Epoch: 65, Batch number: 36, Loss: 7679.65771484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Batch number: 60, Loss: 7700.294921875\n",
      "Epoch: 68, Batch number: 8, Loss: 7569.92333984375\n",
      "Epoch: 69, Batch number: 32, Loss: 7584.60205078125\n",
      "Epoch: 70, Batch number: 56, Loss: 7579.87548828125\n",
      "Epoch: 72, Batch number: 4, Loss: 7596.10986328125\n",
      "Epoch: 73, Batch number: 28, Loss: 7509.80078125\n",
      "Epoch: 74, Batch number: 52, Loss: 7691.623046875\n",
      "Epoch: 76, Batch number: 0, Loss: 7496.4560546875\n",
      "Epoch: 77, Batch number: 24, Loss: 7507.11865234375\n",
      "Epoch: 78, Batch number: 48, Loss: 7533.1455078125\n",
      "Epoch: 79, Batch number: 72, Loss: 7751.4697265625\n",
      "Epoch: 81, Batch number: 20, Loss: 7339.3798828125\n",
      "Epoch: 82, Batch number: 44, Loss: 7493.5029296875\n",
      "Epoch: 83, Batch number: 68, Loss: 7622.107421875\n",
      "Epoch: 85, Batch number: 16, Loss: 7327.44775390625\n",
      "Epoch: 86, Batch number: 40, Loss: 7442.8115234375\n",
      "Epoch: 87, Batch number: 64, Loss: 7323.52197265625\n",
      "Epoch: 89, Batch number: 12, Loss: 7304.50634765625\n",
      "Epoch: 90, Batch number: 36, Loss: 7329.35595703125\n",
      "Epoch: 91, Batch number: 60, Loss: 7409.60400390625\n",
      "Epoch: 93, Batch number: 8, Loss: 7512.6875\n",
      "Epoch: 94, Batch number: 32, Loss: 7432.89990234375\n",
      "Epoch: 95, Batch number: 56, Loss: 7394.0224609375\n",
      "Epoch: 97, Batch number: 4, Loss: 7151.78173828125\n",
      "Epoch: 98, Batch number: 28, Loss: 7458.4970703125\n",
      "Epoch: 99, Batch number: 52, Loss: 7589.99169921875\n",
      "Epoch: 101, Batch number: 0, Loss: 7220.8046875\n",
      "Epoch: 102, Batch number: 24, Loss: 7355.064453125\n",
      "Epoch: 103, Batch number: 48, Loss: 7443.14306640625\n",
      "Epoch: 104, Batch number: 72, Loss: 7355.91455078125\n",
      "Epoch: 106, Batch number: 20, Loss: 7136.833984375\n",
      "Epoch: 107, Batch number: 44, Loss: 7182.9091796875\n",
      "Epoch: 108, Batch number: 68, Loss: 7615.4794921875\n",
      "Epoch: 110, Batch number: 16, Loss: 7213.6611328125\n",
      "Epoch: 111, Batch number: 40, Loss: 7082.56787109375\n",
      "Epoch: 112, Batch number: 64, Loss: 7473.8759765625\n",
      "Epoch: 114, Batch number: 12, Loss: 7310.93505859375\n",
      "Epoch: 115, Batch number: 36, Loss: 7263.72607421875\n",
      "Epoch: 116, Batch number: 60, Loss: 7479.14013671875\n",
      "Epoch: 118, Batch number: 8, Loss: 6916.30078125\n",
      "Epoch: 119, Batch number: 32, Loss: 7428.521484375\n",
      "Epoch: 120, Batch number: 56, Loss: 7056.97216796875\n",
      "Epoch: 122, Batch number: 4, Loss: 7176.12939453125\n",
      "Epoch: 123, Batch number: 28, Loss: 7111.31982421875\n",
      "Epoch: 124, Batch number: 52, Loss: 7249.14013671875\n",
      "Epoch: 126, Batch number: 0, Loss: 6903.65771484375\n",
      "Epoch: 127, Batch number: 24, Loss: 7005.75439453125\n",
      "Epoch: 128, Batch number: 48, Loss: 7215.1962890625\n",
      "Epoch: 129, Batch number: 72, Loss: 7206.32421875\n",
      "Epoch: 131, Batch number: 20, Loss: 7120.0361328125\n",
      "Epoch: 132, Batch number: 44, Loss: 7029.11376953125\n",
      "Epoch: 133, Batch number: 68, Loss: 7184.5830078125\n",
      "Epoch: 135, Batch number: 16, Loss: 7468.16455078125\n",
      "Epoch: 136, Batch number: 40, Loss: 7184.81982421875\n",
      "Epoch: 137, Batch number: 64, Loss: 7041.626953125\n",
      "Epoch: 139, Batch number: 12, Loss: 7109.431640625\n",
      "Epoch: 140, Batch number: 36, Loss: 7058.15478515625\n",
      "Epoch: 141, Batch number: 60, Loss: 7012.53515625\n",
      "Epoch: 143, Batch number: 8, Loss: 6940.259765625\n",
      "Epoch: 144, Batch number: 32, Loss: 6804.33544921875\n",
      "Epoch: 145, Batch number: 56, Loss: 7021.05419921875\n",
      "Epoch: 147, Batch number: 4, Loss: 7030.77978515625\n",
      "Epoch: 148, Batch number: 28, Loss: 6857.798828125\n",
      "Epoch: 149, Batch number: 52, Loss: 7296.33447265625\n",
      "Epoch: 151, Batch number: 0, Loss: 6988.41748046875\n",
      "Epoch: 152, Batch number: 24, Loss: 6682.20556640625\n",
      "Epoch: 153, Batch number: 48, Loss: 6865.8388671875\n",
      "Epoch: 154, Batch number: 72, Loss: 7058.91064453125\n",
      "Epoch: 156, Batch number: 20, Loss: 7449.4130859375\n",
      "Epoch: 157, Batch number: 44, Loss: 6983.2373046875\n",
      "Epoch: 158, Batch number: 68, Loss: 6960.41650390625\n",
      "Epoch: 160, Batch number: 16, Loss: 7039.36376953125\n",
      "Epoch: 161, Batch number: 40, Loss: 7121.37548828125\n",
      "Epoch: 162, Batch number: 64, Loss: 7197.22900390625\n",
      "Epoch: 164, Batch number: 12, Loss: 6877.7548828125\n",
      "Epoch: 165, Batch number: 36, Loss: 7239.91357421875\n",
      "Epoch: 166, Batch number: 60, Loss: 7240.11767578125\n",
      "Epoch: 168, Batch number: 8, Loss: 6843.0400390625\n",
      "Epoch: 169, Batch number: 32, Loss: 7002.890625\n",
      "Epoch: 170, Batch number: 56, Loss: 7131.80810546875\n",
      "Epoch: 172, Batch number: 4, Loss: 7032.1142578125\n",
      "Epoch: 173, Batch number: 28, Loss: 6981.21240234375\n",
      "Epoch: 174, Batch number: 52, Loss: 7080.88671875\n",
      "Epoch: 176, Batch number: 0, Loss: 6955.388671875\n",
      "Epoch: 177, Batch number: 24, Loss: 6897.0732421875\n",
      "Epoch: 178, Batch number: 48, Loss: 6846.1806640625\n",
      "Epoch: 179, Batch number: 72, Loss: 6906.2255859375\n",
      "Epoch: 181, Batch number: 20, Loss: 7183.2158203125\n",
      "Epoch: 182, Batch number: 44, Loss: 6974.22607421875\n",
      "Epoch: 183, Batch number: 68, Loss: 7178.13037109375\n",
      "Epoch: 185, Batch number: 16, Loss: 6819.23046875\n",
      "Epoch: 186, Batch number: 40, Loss: 7188.220703125\n",
      "Epoch: 187, Batch number: 64, Loss: 7083.05322265625\n",
      "Epoch: 189, Batch number: 12, Loss: 6909.71630859375\n",
      "Epoch: 190, Batch number: 36, Loss: 7034.4375\n",
      "Epoch: 191, Batch number: 60, Loss: 6970.3046875\n",
      "Epoch: 193, Batch number: 8, Loss: 6881.2412109375\n",
      "Epoch: 194, Batch number: 32, Loss: 6927.916015625\n",
      "Epoch: 195, Batch number: 56, Loss: 7308.12060546875\n",
      "Epoch: 197, Batch number: 4, Loss: 7070.97705078125\n",
      "Epoch: 198, Batch number: 28, Loss: 6980.6005859375\n",
      "Epoch: 199, Batch number: 52, Loss: 7149.56005859375\n",
      "Epoch: 201, Batch number: 0, Loss: 6865.7109375\n",
      "Epoch: 202, Batch number: 24, Loss: 6931.23876953125\n",
      "Epoch: 203, Batch number: 48, Loss: 6728.84619140625\n",
      "Epoch: 204, Batch number: 72, Loss: 7115.99267578125\n",
      "Epoch: 206, Batch number: 20, Loss: 6861.6201171875\n",
      "Epoch: 207, Batch number: 44, Loss: 6791.17822265625\n",
      "Epoch: 208, Batch number: 68, Loss: 6962.16064453125\n",
      "Epoch: 210, Batch number: 16, Loss: 6735.478515625\n",
      "Epoch: 211, Batch number: 40, Loss: 6979.9091796875\n",
      "Epoch: 212, Batch number: 64, Loss: 7116.13134765625\n",
      "Epoch: 214, Batch number: 12, Loss: 6790.021484375\n",
      "Epoch: 215, Batch number: 36, Loss: 7180.099609375\n",
      "Epoch: 216, Batch number: 60, Loss: 6998.87744140625\n",
      "Epoch: 218, Batch number: 8, Loss: 6784.84619140625\n",
      "Epoch: 219, Batch number: 32, Loss: 7186.91259765625\n",
      "Epoch: 220, Batch number: 56, Loss: 7140.7919921875\n",
      "Epoch: 222, Batch number: 4, Loss: 7188.138671875\n",
      "Epoch: 223, Batch number: 28, Loss: 6952.408203125\n",
      "Epoch: 224, Batch number: 52, Loss: 6859.2412109375\n",
      "Epoch: 226, Batch number: 0, Loss: 6879.52734375\n",
      "Epoch: 227, Batch number: 24, Loss: 6967.4091796875\n",
      "Epoch: 228, Batch number: 48, Loss: 7108.1005859375\n",
      "Epoch: 229, Batch number: 72, Loss: 7091.6259765625\n",
      "Epoch: 231, Batch number: 20, Loss: 6986.376953125\n",
      "Epoch: 232, Batch number: 44, Loss: 6955.779296875\n",
      "Epoch: 233, Batch number: 68, Loss: 7102.5673828125\n",
      "Epoch: 235, Batch number: 16, Loss: 6966.01708984375\n",
      "Epoch: 236, Batch number: 40, Loss: 6916.62353515625\n",
      "Epoch: 237, Batch number: 64, Loss: 7262.48681640625\n",
      "Epoch: 239, Batch number: 12, Loss: 7195.046875\n",
      "Epoch: 240, Batch number: 36, Loss: 7097.29052734375\n",
      "Epoch: 241, Batch number: 60, Loss: 7221.19189453125\n",
      "Epoch: 243, Batch number: 8, Loss: 7255.34619140625\n",
      "Epoch: 244, Batch number: 32, Loss: 7056.72119140625\n",
      "Epoch: 245, Batch number: 56, Loss: 7121.51025390625\n",
      "Epoch: 247, Batch number: 4, Loss: 6785.20263671875\n",
      "Epoch: 248, Batch number: 28, Loss: 6601.3388671875\n",
      "Epoch: 249, Batch number: 52, Loss: 6967.72119140625\n",
      "Epoch: 251, Batch number: 0, Loss: 6849.37060546875\n",
      "Epoch: 252, Batch number: 24, Loss: 6904.27783203125\n",
      "Epoch: 253, Batch number: 48, Loss: 7082.5830078125\n",
      "Epoch: 254, Batch number: 72, Loss: 6727.80908203125\n",
      "Epoch: 256, Batch number: 20, Loss: 7187.7705078125\n",
      "Epoch: 257, Batch number: 44, Loss: 6819.56640625\n",
      "Epoch: 258, Batch number: 68, Loss: 7094.923828125\n",
      "Epoch: 260, Batch number: 16, Loss: 7274.57080078125\n",
      "Epoch: 261, Batch number: 40, Loss: 7050.81494140625\n",
      "Epoch: 262, Batch number: 64, Loss: 7047.224609375\n",
      "Epoch: 264, Batch number: 12, Loss: 6682.7822265625\n",
      "Epoch: 265, Batch number: 36, Loss: 7063.88427734375\n",
      "Epoch: 266, Batch number: 60, Loss: 6941.2900390625\n",
      "Epoch: 268, Batch number: 8, Loss: 6897.1171875\n",
      "Epoch: 269, Batch number: 32, Loss: 7004.96875\n",
      "Epoch: 270, Batch number: 56, Loss: 7167.39892578125\n",
      "Epoch: 272, Batch number: 4, Loss: 6956.60498046875\n",
      "Epoch: 273, Batch number: 28, Loss: 6812.7275390625\n",
      "Epoch: 274, Batch number: 52, Loss: 7001.61376953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276, Batch number: 0, Loss: 6843.5341796875\n",
      "Epoch: 277, Batch number: 24, Loss: 6801.7060546875\n",
      "Epoch: 278, Batch number: 48, Loss: 7001.89306640625\n",
      "Epoch: 279, Batch number: 72, Loss: 7091.8828125\n",
      "Epoch: 281, Batch number: 20, Loss: 6917.146484375\n",
      "Epoch: 282, Batch number: 44, Loss: 7173.71728515625\n",
      "Epoch: 283, Batch number: 68, Loss: 7088.16259765625\n",
      "Epoch: 285, Batch number: 16, Loss: 6858.61181640625\n",
      "Epoch: 286, Batch number: 40, Loss: 7223.5244140625\n",
      "Epoch: 287, Batch number: 64, Loss: 7222.2998046875\n",
      "Epoch: 289, Batch number: 12, Loss: 6936.51416015625\n",
      "Epoch: 290, Batch number: 36, Loss: 7045.87255859375\n",
      "Epoch: 291, Batch number: 60, Loss: 6911.296875\n",
      "Epoch: 293, Batch number: 8, Loss: 6986.26708984375\n",
      "Epoch: 294, Batch number: 32, Loss: 7214.3828125\n",
      "Epoch: 295, Batch number: 56, Loss: 7081.7099609375\n",
      "Epoch: 297, Batch number: 4, Loss: 6805.55126953125\n",
      "Epoch: 298, Batch number: 28, Loss: 6870.29150390625\n",
      "Epoch: 299, Batch number: 52, Loss: 6661.97607421875\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15344.548828125\n",
      "Epoch: 2, Batch number: 24, Loss: 13992.619140625\n",
      "Epoch: 3, Batch number: 48, Loss: 13403.416015625\n",
      "Epoch: 4, Batch number: 72, Loss: 12480.70703125\n",
      "Epoch: 6, Batch number: 20, Loss: 12078.6181640625\n",
      "Epoch: 7, Batch number: 44, Loss: 11631.734375\n",
      "Epoch: 8, Batch number: 68, Loss: 11143.7666015625\n",
      "Epoch: 10, Batch number: 16, Loss: 10778.3310546875\n",
      "Epoch: 11, Batch number: 40, Loss: 10559.048828125\n",
      "Epoch: 12, Batch number: 64, Loss: 10066.865234375\n",
      "Epoch: 14, Batch number: 12, Loss: 9903.9169921875\n",
      "Epoch: 15, Batch number: 36, Loss: 9804.064453125\n",
      "Epoch: 16, Batch number: 60, Loss: 9612.6259765625\n",
      "Epoch: 18, Batch number: 8, Loss: 9120.8515625\n",
      "Epoch: 19, Batch number: 32, Loss: 9201.6572265625\n",
      "Epoch: 20, Batch number: 56, Loss: 9089.900390625\n",
      "Epoch: 22, Batch number: 4, Loss: 8748.43359375\n",
      "Epoch: 23, Batch number: 28, Loss: 8763.8046875\n",
      "Epoch: 24, Batch number: 52, Loss: 8763.431640625\n",
      "Epoch: 26, Batch number: 0, Loss: 8396.6357421875\n",
      "Epoch: 27, Batch number: 24, Loss: 8449.5205078125\n",
      "Epoch: 28, Batch number: 48, Loss: 8332.0205078125\n",
      "Epoch: 29, Batch number: 72, Loss: 8173.99267578125\n",
      "Epoch: 31, Batch number: 20, Loss: 8172.7265625\n",
      "Epoch: 32, Batch number: 44, Loss: 8081.14111328125\n",
      "Epoch: 33, Batch number: 68, Loss: 8158.2568359375\n",
      "Epoch: 35, Batch number: 16, Loss: 8237.7275390625\n",
      "Epoch: 36, Batch number: 40, Loss: 7752.74755859375\n",
      "Epoch: 37, Batch number: 64, Loss: 8117.697265625\n",
      "Epoch: 39, Batch number: 12, Loss: 7960.884765625\n",
      "Epoch: 40, Batch number: 36, Loss: 7992.74267578125\n",
      "Epoch: 41, Batch number: 60, Loss: 7984.26513671875\n",
      "Epoch: 43, Batch number: 8, Loss: 7747.333984375\n",
      "Epoch: 44, Batch number: 32, Loss: 7820.87841796875\n",
      "Epoch: 45, Batch number: 56, Loss: 8003.443359375\n",
      "Epoch: 47, Batch number: 4, Loss: 7855.228515625\n",
      "Epoch: 48, Batch number: 28, Loss: 7845.119140625\n",
      "Epoch: 49, Batch number: 52, Loss: 7605.2236328125\n",
      "Epoch: 51, Batch number: 0, Loss: 7998.18505859375\n",
      "Epoch: 52, Batch number: 24, Loss: 7642.29248046875\n",
      "Epoch: 53, Batch number: 48, Loss: 7475.4462890625\n",
      "Epoch: 54, Batch number: 72, Loss: 7671.86376953125\n",
      "Epoch: 56, Batch number: 20, Loss: 7510.49755859375\n",
      "Epoch: 57, Batch number: 44, Loss: 7564.37158203125\n",
      "Epoch: 58, Batch number: 68, Loss: 7654.83740234375\n",
      "Epoch: 60, Batch number: 16, Loss: 7714.162109375\n",
      "Epoch: 61, Batch number: 40, Loss: 7572.3212890625\n",
      "Epoch: 62, Batch number: 64, Loss: 7440.08642578125\n",
      "Epoch: 64, Batch number: 12, Loss: 7363.23974609375\n",
      "Epoch: 65, Batch number: 36, Loss: 7743.0732421875\n",
      "Epoch: 66, Batch number: 60, Loss: 7602.24267578125\n",
      "Epoch: 68, Batch number: 8, Loss: 7405.73828125\n",
      "Epoch: 69, Batch number: 32, Loss: 7295.169921875\n",
      "Epoch: 70, Batch number: 56, Loss: 7340.2333984375\n",
      "Epoch: 72, Batch number: 4, Loss: 7233.5751953125\n",
      "Epoch: 73, Batch number: 28, Loss: 7389.4951171875\n",
      "Epoch: 74, Batch number: 52, Loss: 7323.02001953125\n",
      "Epoch: 76, Batch number: 0, Loss: 7105.65283203125\n",
      "Epoch: 77, Batch number: 24, Loss: 7217.328125\n",
      "Epoch: 78, Batch number: 48, Loss: 7247.9150390625\n",
      "Epoch: 79, Batch number: 72, Loss: 7389.6982421875\n",
      "Epoch: 81, Batch number: 20, Loss: 7158.03857421875\n",
      "Epoch: 82, Batch number: 44, Loss: 7214.79443359375\n",
      "Epoch: 83, Batch number: 68, Loss: 7562.92041015625\n",
      "Epoch: 85, Batch number: 16, Loss: 7176.9541015625\n",
      "Epoch: 86, Batch number: 40, Loss: 7294.48193359375\n",
      "Epoch: 87, Batch number: 64, Loss: 7471.6533203125\n",
      "Epoch: 89, Batch number: 12, Loss: 7163.8916015625\n",
      "Epoch: 90, Batch number: 36, Loss: 7340.45751953125\n",
      "Epoch: 91, Batch number: 60, Loss: 7505.72607421875\n",
      "Epoch: 93, Batch number: 8, Loss: 7313.7216796875\n",
      "Epoch: 94, Batch number: 32, Loss: 7187.9609375\n",
      "Epoch: 95, Batch number: 56, Loss: 7232.3115234375\n",
      "Epoch: 97, Batch number: 4, Loss: 7235.60595703125\n",
      "Epoch: 98, Batch number: 28, Loss: 6936.22265625\n",
      "Epoch: 99, Batch number: 52, Loss: 7156.3818359375\n",
      "Epoch: 101, Batch number: 0, Loss: 6898.5390625\n",
      "Epoch: 102, Batch number: 24, Loss: 7302.83642578125\n",
      "Epoch: 103, Batch number: 48, Loss: 7405.177734375\n",
      "Epoch: 104, Batch number: 72, Loss: 7035.9765625\n",
      "Epoch: 106, Batch number: 20, Loss: 7147.25244140625\n",
      "Epoch: 107, Batch number: 44, Loss: 7476.27392578125\n",
      "Epoch: 108, Batch number: 68, Loss: 6920.1181640625\n",
      "Epoch: 110, Batch number: 16, Loss: 7245.341796875\n",
      "Epoch: 111, Batch number: 40, Loss: 7194.552734375\n",
      "Epoch: 112, Batch number: 64, Loss: 7166.15966796875\n",
      "Epoch: 114, Batch number: 12, Loss: 7100.7275390625\n",
      "Epoch: 115, Batch number: 36, Loss: 7307.43212890625\n",
      "Epoch: 116, Batch number: 60, Loss: 7143.1494140625\n",
      "Epoch: 118, Batch number: 8, Loss: 6934.38818359375\n",
      "Epoch: 119, Batch number: 32, Loss: 6830.25048828125\n",
      "Epoch: 120, Batch number: 56, Loss: 7254.998046875\n",
      "Epoch: 122, Batch number: 4, Loss: 7196.76904296875\n",
      "Epoch: 123, Batch number: 28, Loss: 7006.9384765625\n",
      "Epoch: 124, Batch number: 52, Loss: 7104.04833984375\n",
      "Epoch: 126, Batch number: 0, Loss: 6951.48974609375\n",
      "Epoch: 127, Batch number: 24, Loss: 7131.11767578125\n",
      "Epoch: 128, Batch number: 48, Loss: 7204.37939453125\n",
      "Epoch: 129, Batch number: 72, Loss: 7002.92041015625\n",
      "Epoch: 131, Batch number: 20, Loss: 7002.04052734375\n",
      "Epoch: 132, Batch number: 44, Loss: 7184.42333984375\n",
      "Epoch: 133, Batch number: 68, Loss: 6997.8671875\n",
      "Epoch: 135, Batch number: 16, Loss: 7156.4033203125\n",
      "Epoch: 136, Batch number: 40, Loss: 7152.59912109375\n",
      "Epoch: 137, Batch number: 64, Loss: 7078.2822265625\n",
      "Epoch: 139, Batch number: 12, Loss: 6982.03271484375\n",
      "Epoch: 140, Batch number: 36, Loss: 7112.18701171875\n",
      "Epoch: 141, Batch number: 60, Loss: 7219.34423828125\n",
      "Epoch: 143, Batch number: 8, Loss: 6832.0234375\n",
      "Epoch: 144, Batch number: 32, Loss: 6870.0283203125\n",
      "Epoch: 145, Batch number: 56, Loss: 7223.01953125\n",
      "Epoch: 147, Batch number: 4, Loss: 6801.58154296875\n",
      "Epoch: 148, Batch number: 28, Loss: 7089.4775390625\n",
      "Epoch: 149, Batch number: 52, Loss: 6844.78515625\n",
      "Epoch: 151, Batch number: 0, Loss: 6818.2138671875\n",
      "Epoch: 152, Batch number: 24, Loss: 7085.0263671875\n",
      "Epoch: 153, Batch number: 48, Loss: 7294.9345703125\n",
      "Epoch: 154, Batch number: 72, Loss: 7172.25927734375\n",
      "Epoch: 156, Batch number: 20, Loss: 7021.6513671875\n",
      "Epoch: 157, Batch number: 44, Loss: 7072.990234375\n",
      "Epoch: 158, Batch number: 68, Loss: 6982.8154296875\n",
      "Epoch: 160, Batch number: 16, Loss: 7105.57177734375\n",
      "Epoch: 161, Batch number: 40, Loss: 6994.927734375\n",
      "Epoch: 162, Batch number: 64, Loss: 7179.60986328125\n",
      "Epoch: 164, Batch number: 12, Loss: 7115.46240234375\n",
      "Epoch: 165, Batch number: 36, Loss: 7125.029296875\n",
      "Epoch: 166, Batch number: 60, Loss: 7032.0888671875\n",
      "Epoch: 168, Batch number: 8, Loss: 6982.15771484375\n",
      "Epoch: 169, Batch number: 32, Loss: 7102.490234375\n",
      "Epoch: 170, Batch number: 56, Loss: 7111.6337890625\n",
      "Epoch: 172, Batch number: 4, Loss: 7004.95166015625\n",
      "Epoch: 173, Batch number: 28, Loss: 7150.0146484375\n",
      "Epoch: 174, Batch number: 52, Loss: 6991.49853515625\n",
      "Epoch: 176, Batch number: 0, Loss: 6735.7841796875\n",
      "Epoch: 177, Batch number: 24, Loss: 6894.89111328125\n",
      "Epoch: 178, Batch number: 48, Loss: 7006.5068359375\n",
      "Epoch: 179, Batch number: 72, Loss: 7022.76025390625\n",
      "Epoch: 181, Batch number: 20, Loss: 7121.8701171875\n",
      "Epoch: 182, Batch number: 44, Loss: 7202.0439453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183, Batch number: 68, Loss: 7142.2861328125\n",
      "Epoch: 185, Batch number: 16, Loss: 7011.26318359375\n",
      "Epoch: 186, Batch number: 40, Loss: 6785.9326171875\n",
      "Epoch: 187, Batch number: 64, Loss: 7020.30712890625\n",
      "Epoch: 189, Batch number: 12, Loss: 6884.27490234375\n",
      "Epoch: 190, Batch number: 36, Loss: 7115.30615234375\n",
      "Epoch: 191, Batch number: 60, Loss: 7137.5166015625\n",
      "Epoch: 193, Batch number: 8, Loss: 7072.4970703125\n",
      "Epoch: 194, Batch number: 32, Loss: 6832.15673828125\n",
      "Epoch: 195, Batch number: 56, Loss: 7135.1123046875\n",
      "Epoch: 197, Batch number: 4, Loss: 6816.06884765625\n",
      "Epoch: 198, Batch number: 28, Loss: 7113.265625\n",
      "Epoch: 199, Batch number: 52, Loss: 7098.56787109375\n",
      "Epoch: 201, Batch number: 0, Loss: 6886.84130859375\n",
      "Epoch: 202, Batch number: 24, Loss: 6669.67041015625\n",
      "Epoch: 203, Batch number: 48, Loss: 7064.98095703125\n",
      "Epoch: 204, Batch number: 72, Loss: 6915.13330078125\n",
      "Epoch: 206, Batch number: 20, Loss: 6973.14794921875\n",
      "Epoch: 207, Batch number: 44, Loss: 7116.51708984375\n",
      "Epoch: 208, Batch number: 68, Loss: 7105.90380859375\n",
      "Epoch: 210, Batch number: 16, Loss: 7001.49853515625\n",
      "Epoch: 211, Batch number: 40, Loss: 6893.37109375\n",
      "Epoch: 212, Batch number: 64, Loss: 7185.22802734375\n",
      "Epoch: 214, Batch number: 12, Loss: 6981.7978515625\n",
      "Epoch: 215, Batch number: 36, Loss: 7184.99169921875\n",
      "Epoch: 216, Batch number: 60, Loss: 7014.96337890625\n",
      "Epoch: 218, Batch number: 8, Loss: 7070.70068359375\n",
      "Epoch: 219, Batch number: 32, Loss: 6836.91943359375\n",
      "Epoch: 220, Batch number: 56, Loss: 6875.830078125\n",
      "Epoch: 222, Batch number: 4, Loss: 6813.22900390625\n",
      "Epoch: 223, Batch number: 28, Loss: 7196.63818359375\n",
      "Epoch: 224, Batch number: 52, Loss: 7144.5576171875\n",
      "Epoch: 226, Batch number: 0, Loss: 6752.99658203125\n",
      "Epoch: 227, Batch number: 24, Loss: 7032.91552734375\n",
      "Epoch: 228, Batch number: 48, Loss: 6990.39013671875\n",
      "Epoch: 229, Batch number: 72, Loss: 7107.12548828125\n",
      "Epoch: 231, Batch number: 20, Loss: 7247.50390625\n",
      "Epoch: 232, Batch number: 44, Loss: 6775.353515625\n",
      "Epoch: 233, Batch number: 68, Loss: 7017.94677734375\n",
      "Epoch: 235, Batch number: 16, Loss: 7087.5283203125\n",
      "Epoch: 236, Batch number: 40, Loss: 7001.3251953125\n",
      "Epoch: 237, Batch number: 64, Loss: 7116.33251953125\n",
      "Epoch: 239, Batch number: 12, Loss: 6788.66650390625\n",
      "Epoch: 240, Batch number: 36, Loss: 6791.80224609375\n",
      "Epoch: 241, Batch number: 60, Loss: 7177.98779296875\n",
      "Epoch: 243, Batch number: 8, Loss: 7172.77490234375\n",
      "Epoch: 244, Batch number: 32, Loss: 6757.4033203125\n",
      "Epoch: 245, Batch number: 56, Loss: 6781.28564453125\n",
      "Epoch: 247, Batch number: 4, Loss: 7241.9462890625\n",
      "Epoch: 248, Batch number: 28, Loss: 7112.47265625\n",
      "Epoch: 249, Batch number: 52, Loss: 7402.78564453125\n",
      "Epoch: 251, Batch number: 0, Loss: 6995.466796875\n",
      "Epoch: 252, Batch number: 24, Loss: 6875.70849609375\n",
      "Epoch: 253, Batch number: 48, Loss: 7108.12158203125\n",
      "Epoch: 254, Batch number: 72, Loss: 6972.04443359375\n",
      "Epoch: 256, Batch number: 20, Loss: 6855.046875\n",
      "Epoch: 257, Batch number: 44, Loss: 7087.44873046875\n",
      "Epoch: 258, Batch number: 68, Loss: 6857.169921875\n",
      "Epoch: 260, Batch number: 16, Loss: 6924.60791015625\n",
      "Epoch: 261, Batch number: 40, Loss: 6971.15380859375\n",
      "Epoch: 262, Batch number: 64, Loss: 7154.9228515625\n",
      "Epoch: 264, Batch number: 12, Loss: 6905.5126953125\n",
      "Epoch: 265, Batch number: 36, Loss: 7264.40087890625\n",
      "Epoch: 266, Batch number: 60, Loss: 7298.03466796875\n",
      "Epoch: 268, Batch number: 8, Loss: 6899.79150390625\n",
      "Epoch: 269, Batch number: 32, Loss: 7236.3408203125\n",
      "Epoch: 270, Batch number: 56, Loss: 7274.8798828125\n",
      "Epoch: 272, Batch number: 4, Loss: 7016.19775390625\n",
      "Epoch: 273, Batch number: 28, Loss: 6712.466796875\n",
      "Epoch: 274, Batch number: 52, Loss: 6886.91796875\n",
      "Epoch: 276, Batch number: 0, Loss: 6833.49072265625\n",
      "Epoch: 277, Batch number: 24, Loss: 7137.20263671875\n",
      "Epoch: 278, Batch number: 48, Loss: 6906.474609375\n",
      "Epoch: 279, Batch number: 72, Loss: 7003.02099609375\n",
      "Epoch: 281, Batch number: 20, Loss: 7029.0966796875\n",
      "Epoch: 282, Batch number: 44, Loss: 6978.27734375\n",
      "Epoch: 283, Batch number: 68, Loss: 6778.89404296875\n",
      "Epoch: 285, Batch number: 16, Loss: 6956.23779296875\n",
      "Epoch: 286, Batch number: 40, Loss: 6993.1220703125\n",
      "Epoch: 287, Batch number: 64, Loss: 7219.45849609375\n",
      "Epoch: 289, Batch number: 12, Loss: 7129.212890625\n",
      "Epoch: 290, Batch number: 36, Loss: 6948.13916015625\n",
      "Epoch: 291, Batch number: 60, Loss: 6660.2216796875\n",
      "Epoch: 293, Batch number: 8, Loss: 7030.3798828125\n",
      "Epoch: 294, Batch number: 32, Loss: 6988.65869140625\n",
      "Epoch: 295, Batch number: 56, Loss: 7131.49609375\n",
      "Epoch: 297, Batch number: 4, Loss: 6985.36669921875\n",
      "Epoch: 298, Batch number: 28, Loss: 6966.9453125\n",
      "Epoch: 299, Batch number: 52, Loss: 6942.48486328125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15262.2412109375\n",
      "Epoch: 2, Batch number: 24, Loss: 13576.564453125\n",
      "Epoch: 3, Batch number: 48, Loss: 12797.5693359375\n",
      "Epoch: 4, Batch number: 72, Loss: 12342.791015625\n",
      "Epoch: 6, Batch number: 20, Loss: 11330.2666015625\n",
      "Epoch: 7, Batch number: 44, Loss: 10801.7333984375\n",
      "Epoch: 8, Batch number: 68, Loss: 10542.4814453125\n",
      "Epoch: 10, Batch number: 16, Loss: 10107.923828125\n",
      "Epoch: 11, Batch number: 40, Loss: 9655.5693359375\n",
      "Epoch: 12, Batch number: 64, Loss: 9676.39453125\n",
      "Epoch: 14, Batch number: 12, Loss: 9039.0087890625\n",
      "Epoch: 15, Batch number: 36, Loss: 8898.693359375\n",
      "Epoch: 16, Batch number: 60, Loss: 8811.146484375\n",
      "Epoch: 18, Batch number: 8, Loss: 8535.2421875\n",
      "Epoch: 19, Batch number: 32, Loss: 8363.3583984375\n",
      "Epoch: 20, Batch number: 56, Loss: 8317.552734375\n",
      "Epoch: 22, Batch number: 4, Loss: 8018.880859375\n",
      "Epoch: 23, Batch number: 28, Loss: 8110.0859375\n",
      "Epoch: 24, Batch number: 52, Loss: 8184.85546875\n",
      "Epoch: 26, Batch number: 0, Loss: 8036.37744140625\n",
      "Epoch: 27, Batch number: 24, Loss: 7814.07275390625\n",
      "Epoch: 28, Batch number: 48, Loss: 7966.76708984375\n",
      "Epoch: 29, Batch number: 72, Loss: 7867.95654296875\n",
      "Epoch: 31, Batch number: 20, Loss: 7620.2919921875\n",
      "Epoch: 32, Batch number: 44, Loss: 7956.91357421875\n",
      "Epoch: 33, Batch number: 68, Loss: 7825.95068359375\n",
      "Epoch: 35, Batch number: 16, Loss: 7616.04443359375\n",
      "Epoch: 36, Batch number: 40, Loss: 7972.2900390625\n",
      "Epoch: 37, Batch number: 64, Loss: 7569.89599609375\n",
      "Epoch: 39, Batch number: 12, Loss: 7714.0185546875\n",
      "Epoch: 40, Batch number: 36, Loss: 7434.49365234375\n",
      "Epoch: 41, Batch number: 60, Loss: 7535.9697265625\n",
      "Epoch: 43, Batch number: 8, Loss: 7598.38916015625\n",
      "Epoch: 44, Batch number: 32, Loss: 7368.36572265625\n",
      "Epoch: 45, Batch number: 56, Loss: 7650.82861328125\n",
      "Epoch: 47, Batch number: 4, Loss: 7227.01904296875\n",
      "Epoch: 48, Batch number: 28, Loss: 7546.59765625\n",
      "Epoch: 49, Batch number: 52, Loss: 7475.69580078125\n",
      "Epoch: 51, Batch number: 0, Loss: 7350.96142578125\n",
      "Epoch: 52, Batch number: 24, Loss: 7216.966796875\n",
      "Epoch: 53, Batch number: 48, Loss: 7178.98095703125\n",
      "Epoch: 54, Batch number: 72, Loss: 7424.666015625\n",
      "Epoch: 56, Batch number: 20, Loss: 7386.58642578125\n",
      "Epoch: 57, Batch number: 44, Loss: 7273.431640625\n",
      "Epoch: 58, Batch number: 68, Loss: 7431.65625\n",
      "Epoch: 60, Batch number: 16, Loss: 7480.9775390625\n",
      "Epoch: 61, Batch number: 40, Loss: 7217.67041015625\n",
      "Epoch: 62, Batch number: 64, Loss: 7224.484375\n",
      "Epoch: 64, Batch number: 12, Loss: 7182.62060546875\n",
      "Epoch: 65, Batch number: 36, Loss: 7281.1943359375\n",
      "Epoch: 66, Batch number: 60, Loss: 7672.59619140625\n",
      "Epoch: 68, Batch number: 8, Loss: 7204.8994140625\n",
      "Epoch: 69, Batch number: 32, Loss: 7220.076171875\n",
      "Epoch: 70, Batch number: 56, Loss: 7272.3583984375\n",
      "Epoch: 72, Batch number: 4, Loss: 7331.7138671875\n",
      "Epoch: 73, Batch number: 28, Loss: 7085.20947265625\n",
      "Epoch: 74, Batch number: 52, Loss: 7353.64404296875\n",
      "Epoch: 76, Batch number: 0, Loss: 7056.45947265625\n",
      "Epoch: 77, Batch number: 24, Loss: 7106.3837890625\n",
      "Epoch: 78, Batch number: 48, Loss: 7303.11181640625\n",
      "Epoch: 79, Batch number: 72, Loss: 7124.3994140625\n",
      "Epoch: 81, Batch number: 20, Loss: 6898.814453125\n",
      "Epoch: 82, Batch number: 44, Loss: 7236.326171875\n",
      "Epoch: 83, Batch number: 68, Loss: 6993.3154296875\n",
      "Epoch: 85, Batch number: 16, Loss: 7067.3935546875\n",
      "Epoch: 86, Batch number: 40, Loss: 7141.46826171875\n",
      "Epoch: 87, Batch number: 64, Loss: 7231.177734375\n",
      "Epoch: 89, Batch number: 12, Loss: 7090.25048828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Batch number: 36, Loss: 7016.4599609375\n",
      "Epoch: 91, Batch number: 60, Loss: 7320.8369140625\n",
      "Epoch: 93, Batch number: 8, Loss: 7249.98876953125\n",
      "Epoch: 94, Batch number: 32, Loss: 7048.48779296875\n",
      "Epoch: 95, Batch number: 56, Loss: 7055.91015625\n",
      "Epoch: 97, Batch number: 4, Loss: 7195.0341796875\n",
      "Epoch: 98, Batch number: 28, Loss: 7190.025390625\n",
      "Epoch: 99, Batch number: 52, Loss: 6984.09130859375\n",
      "Epoch: 101, Batch number: 0, Loss: 6914.20947265625\n",
      "Epoch: 102, Batch number: 24, Loss: 6915.57958984375\n",
      "Epoch: 103, Batch number: 48, Loss: 7036.16455078125\n",
      "Epoch: 104, Batch number: 72, Loss: 7215.63330078125\n",
      "Epoch: 106, Batch number: 20, Loss: 7191.662109375\n",
      "Epoch: 107, Batch number: 44, Loss: 7342.54736328125\n",
      "Epoch: 108, Batch number: 68, Loss: 7316.33837890625\n",
      "Epoch: 110, Batch number: 16, Loss: 6922.43115234375\n",
      "Epoch: 111, Batch number: 40, Loss: 7077.66748046875\n",
      "Epoch: 112, Batch number: 64, Loss: 7088.05810546875\n",
      "Epoch: 114, Batch number: 12, Loss: 7066.47265625\n",
      "Epoch: 115, Batch number: 36, Loss: 6900.97509765625\n",
      "Epoch: 116, Batch number: 60, Loss: 7112.43994140625\n",
      "Epoch: 118, Batch number: 8, Loss: 7049.13818359375\n",
      "Epoch: 119, Batch number: 32, Loss: 7021.7265625\n",
      "Epoch: 120, Batch number: 56, Loss: 7204.70458984375\n",
      "Epoch: 122, Batch number: 4, Loss: 7065.02587890625\n",
      "Epoch: 123, Batch number: 28, Loss: 7027.611328125\n",
      "Epoch: 124, Batch number: 52, Loss: 6954.57568359375\n",
      "Epoch: 126, Batch number: 0, Loss: 6824.71435546875\n",
      "Epoch: 127, Batch number: 24, Loss: 7163.404296875\n",
      "Epoch: 128, Batch number: 48, Loss: 7373.388671875\n",
      "Epoch: 129, Batch number: 72, Loss: 6945.9931640625\n",
      "Epoch: 131, Batch number: 20, Loss: 7014.568359375\n",
      "Epoch: 132, Batch number: 44, Loss: 6834.8720703125\n",
      "Epoch: 133, Batch number: 68, Loss: 6820.81298828125\n",
      "Epoch: 135, Batch number: 16, Loss: 7002.6083984375\n",
      "Epoch: 136, Batch number: 40, Loss: 7071.6611328125\n",
      "Epoch: 137, Batch number: 64, Loss: 7138.771484375\n",
      "Epoch: 139, Batch number: 12, Loss: 6754.1640625\n",
      "Epoch: 140, Batch number: 36, Loss: 7190.15966796875\n",
      "Epoch: 141, Batch number: 60, Loss: 7249.25439453125\n",
      "Epoch: 143, Batch number: 8, Loss: 6981.34814453125\n",
      "Epoch: 144, Batch number: 32, Loss: 7182.77734375\n",
      "Epoch: 145, Batch number: 56, Loss: 7078.9609375\n",
      "Epoch: 147, Batch number: 4, Loss: 7142.30126953125\n",
      "Epoch: 148, Batch number: 28, Loss: 7197.03076171875\n",
      "Epoch: 149, Batch number: 52, Loss: 6994.76318359375\n",
      "Epoch: 151, Batch number: 0, Loss: 6780.30615234375\n",
      "Epoch: 152, Batch number: 24, Loss: 7254.61572265625\n",
      "Epoch: 153, Batch number: 48, Loss: 6932.5908203125\n",
      "Epoch: 154, Batch number: 72, Loss: 7190.4677734375\n",
      "Epoch: 156, Batch number: 20, Loss: 6974.4111328125\n",
      "Epoch: 157, Batch number: 44, Loss: 7149.0361328125\n",
      "Epoch: 158, Batch number: 68, Loss: 7287.4208984375\n",
      "Epoch: 160, Batch number: 16, Loss: 7028.453125\n",
      "Epoch: 161, Batch number: 40, Loss: 7237.07568359375\n",
      "Epoch: 162, Batch number: 64, Loss: 7047.31396484375\n",
      "Epoch: 164, Batch number: 12, Loss: 7049.28369140625\n",
      "Epoch: 165, Batch number: 36, Loss: 7097.76806640625\n",
      "Epoch: 166, Batch number: 60, Loss: 7060.16845703125\n",
      "Epoch: 168, Batch number: 8, Loss: 6853.37939453125\n",
      "Epoch: 169, Batch number: 32, Loss: 7213.2939453125\n",
      "Epoch: 170, Batch number: 56, Loss: 6889.84765625\n",
      "Epoch: 172, Batch number: 4, Loss: 6790.93017578125\n",
      "Epoch: 173, Batch number: 28, Loss: 6962.44140625\n",
      "Epoch: 174, Batch number: 52, Loss: 7105.02587890625\n",
      "Epoch: 176, Batch number: 0, Loss: 7001.83740234375\n",
      "Epoch: 177, Batch number: 24, Loss: 6934.17578125\n",
      "Epoch: 178, Batch number: 48, Loss: 7099.96240234375\n",
      "Epoch: 179, Batch number: 72, Loss: 7093.0322265625\n",
      "Epoch: 181, Batch number: 20, Loss: 6780.19091796875\n",
      "Epoch: 182, Batch number: 44, Loss: 6985.6474609375\n",
      "Epoch: 183, Batch number: 68, Loss: 7117.18359375\n",
      "Epoch: 185, Batch number: 16, Loss: 7072.29833984375\n",
      "Epoch: 186, Batch number: 40, Loss: 7001.60595703125\n",
      "Epoch: 187, Batch number: 64, Loss: 6951.52734375\n",
      "Epoch: 189, Batch number: 12, Loss: 7180.505859375\n",
      "Epoch: 190, Batch number: 36, Loss: 7075.3291015625\n",
      "Epoch: 191, Batch number: 60, Loss: 7071.365234375\n",
      "Epoch: 193, Batch number: 8, Loss: 6795.30029296875\n",
      "Epoch: 194, Batch number: 32, Loss: 6937.01513671875\n",
      "Epoch: 195, Batch number: 56, Loss: 7056.80908203125\n",
      "Epoch: 197, Batch number: 4, Loss: 7021.4189453125\n",
      "Epoch: 198, Batch number: 28, Loss: 6946.0029296875\n",
      "Epoch: 199, Batch number: 52, Loss: 6977.93798828125\n",
      "Epoch: 201, Batch number: 0, Loss: 7200.11083984375\n",
      "Epoch: 202, Batch number: 24, Loss: 6830.49951171875\n",
      "Epoch: 203, Batch number: 48, Loss: 7039.0009765625\n",
      "Epoch: 204, Batch number: 72, Loss: 7165.169921875\n",
      "Epoch: 206, Batch number: 20, Loss: 6918.30419921875\n",
      "Epoch: 207, Batch number: 44, Loss: 7140.9248046875\n",
      "Epoch: 208, Batch number: 68, Loss: 7210.3994140625\n",
      "Epoch: 210, Batch number: 16, Loss: 6677.79150390625\n",
      "Epoch: 211, Batch number: 40, Loss: 6964.32666015625\n",
      "Epoch: 212, Batch number: 64, Loss: 7362.13427734375\n",
      "Epoch: 214, Batch number: 12, Loss: 6835.82080078125\n",
      "Epoch: 215, Batch number: 36, Loss: 7093.2890625\n",
      "Epoch: 216, Batch number: 60, Loss: 7012.3955078125\n",
      "Epoch: 218, Batch number: 8, Loss: 6988.0\n",
      "Epoch: 219, Batch number: 32, Loss: 6977.01171875\n",
      "Epoch: 220, Batch number: 56, Loss: 7355.1982421875\n",
      "Epoch: 222, Batch number: 4, Loss: 6778.39501953125\n",
      "Epoch: 223, Batch number: 28, Loss: 7082.33740234375\n",
      "Epoch: 224, Batch number: 52, Loss: 7112.8671875\n",
      "Epoch: 226, Batch number: 0, Loss: 7226.77685546875\n",
      "Epoch: 227, Batch number: 24, Loss: 6907.93505859375\n",
      "Epoch: 228, Batch number: 48, Loss: 7067.8369140625\n",
      "Epoch: 229, Batch number: 72, Loss: 7050.13232421875\n",
      "Epoch: 231, Batch number: 20, Loss: 7261.10595703125\n",
      "Epoch: 232, Batch number: 44, Loss: 6876.77783203125\n",
      "Epoch: 233, Batch number: 68, Loss: 7314.54345703125\n",
      "Epoch: 235, Batch number: 16, Loss: 6937.41259765625\n",
      "Epoch: 236, Batch number: 40, Loss: 6932.3115234375\n",
      "Epoch: 237, Batch number: 64, Loss: 7167.10693359375\n",
      "Epoch: 239, Batch number: 12, Loss: 7100.25634765625\n",
      "Epoch: 240, Batch number: 36, Loss: 6940.4443359375\n",
      "Epoch: 241, Batch number: 60, Loss: 7022.86669921875\n",
      "Epoch: 243, Batch number: 8, Loss: 7086.87939453125\n",
      "Epoch: 244, Batch number: 32, Loss: 6830.07861328125\n",
      "Epoch: 245, Batch number: 56, Loss: 7015.73828125\n",
      "Epoch: 247, Batch number: 4, Loss: 6846.37841796875\n",
      "Epoch: 248, Batch number: 28, Loss: 7002.146484375\n",
      "Epoch: 249, Batch number: 52, Loss: 7288.470703125\n",
      "Epoch: 251, Batch number: 0, Loss: 7105.38818359375\n",
      "Epoch: 252, Batch number: 24, Loss: 6843.22216796875\n",
      "Epoch: 253, Batch number: 48, Loss: 7384.333984375\n",
      "Epoch: 254, Batch number: 72, Loss: 7090.96826171875\n",
      "Epoch: 256, Batch number: 20, Loss: 6839.3701171875\n",
      "Epoch: 257, Batch number: 44, Loss: 7416.20556640625\n",
      "Epoch: 258, Batch number: 68, Loss: 6940.56640625\n",
      "Epoch: 260, Batch number: 16, Loss: 6754.37890625\n",
      "Epoch: 261, Batch number: 40, Loss: 7025.66552734375\n",
      "Epoch: 262, Batch number: 64, Loss: 6976.87158203125\n",
      "Epoch: 264, Batch number: 12, Loss: 6822.923828125\n",
      "Epoch: 265, Batch number: 36, Loss: 7218.3935546875\n",
      "Epoch: 266, Batch number: 60, Loss: 7052.89990234375\n",
      "Epoch: 268, Batch number: 8, Loss: 6902.08154296875\n",
      "Epoch: 269, Batch number: 32, Loss: 6832.87548828125\n",
      "Epoch: 270, Batch number: 56, Loss: 7076.13916015625\n",
      "Epoch: 272, Batch number: 4, Loss: 6767.50732421875\n",
      "Epoch: 273, Batch number: 28, Loss: 7088.3466796875\n",
      "Epoch: 274, Batch number: 52, Loss: 7018.5\n",
      "Epoch: 276, Batch number: 0, Loss: 6983.10791015625\n",
      "Epoch: 277, Batch number: 24, Loss: 7085.9208984375\n",
      "Epoch: 278, Batch number: 48, Loss: 7070.017578125\n",
      "Epoch: 279, Batch number: 72, Loss: 7333.3330078125\n",
      "Epoch: 281, Batch number: 20, Loss: 7084.24560546875\n",
      "Epoch: 282, Batch number: 44, Loss: 7160.154296875\n",
      "Epoch: 283, Batch number: 68, Loss: 7130.85693359375\n",
      "Epoch: 285, Batch number: 16, Loss: 6911.03125\n",
      "Epoch: 286, Batch number: 40, Loss: 7274.4755859375\n",
      "Epoch: 287, Batch number: 64, Loss: 6988.71044921875\n",
      "Epoch: 289, Batch number: 12, Loss: 7069.39111328125\n",
      "Epoch: 290, Batch number: 36, Loss: 7253.150390625\n",
      "Epoch: 291, Batch number: 60, Loss: 6818.22021484375\n",
      "Epoch: 293, Batch number: 8, Loss: 7066.55126953125\n",
      "Epoch: 294, Batch number: 32, Loss: 7099.84716796875\n",
      "Epoch: 295, Batch number: 56, Loss: 6733.76611328125\n",
      "Epoch: 297, Batch number: 4, Loss: 6873.60888671875\n",
      "Epoch: 298, Batch number: 28, Loss: 7183.50537109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299, Batch number: 52, Loss: 7260.18798828125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 15432.474609375\n",
      "Epoch: 2, Batch number: 24, Loss: 13443.7060546875\n",
      "Epoch: 3, Batch number: 48, Loss: 12907.533203125\n",
      "Epoch: 4, Batch number: 72, Loss: 11872.9599609375\n",
      "Epoch: 6, Batch number: 20, Loss: 11257.751953125\n",
      "Epoch: 7, Batch number: 44, Loss: 10547.10546875\n",
      "Epoch: 8, Batch number: 68, Loss: 10411.87890625\n",
      "Epoch: 10, Batch number: 16, Loss: 9567.6826171875\n",
      "Epoch: 11, Batch number: 40, Loss: 9308.9345703125\n",
      "Epoch: 12, Batch number: 64, Loss: 9207.2705078125\n",
      "Epoch: 14, Batch number: 12, Loss: 8457.4833984375\n",
      "Epoch: 15, Batch number: 36, Loss: 8430.0458984375\n",
      "Epoch: 16, Batch number: 60, Loss: 8502.6767578125\n",
      "Epoch: 18, Batch number: 8, Loss: 8168.78857421875\n",
      "Epoch: 19, Batch number: 32, Loss: 8006.03857421875\n",
      "Epoch: 20, Batch number: 56, Loss: 8383.7138671875\n",
      "Epoch: 22, Batch number: 4, Loss: 7852.10009765625\n",
      "Epoch: 23, Batch number: 28, Loss: 7947.8017578125\n",
      "Epoch: 24, Batch number: 52, Loss: 7958.07373046875\n",
      "Epoch: 26, Batch number: 0, Loss: 7864.4150390625\n",
      "Epoch: 27, Batch number: 24, Loss: 7710.25732421875\n",
      "Epoch: 28, Batch number: 48, Loss: 7665.55126953125\n",
      "Epoch: 29, Batch number: 72, Loss: 7621.529296875\n",
      "Epoch: 31, Batch number: 20, Loss: 7477.0322265625\n",
      "Epoch: 32, Batch number: 44, Loss: 7470.736328125\n",
      "Epoch: 33, Batch number: 68, Loss: 7621.310546875\n",
      "Epoch: 35, Batch number: 16, Loss: 7457.64501953125\n",
      "Epoch: 36, Batch number: 40, Loss: 7360.60498046875\n",
      "Epoch: 37, Batch number: 64, Loss: 7238.37060546875\n",
      "Epoch: 39, Batch number: 12, Loss: 7246.34423828125\n",
      "Epoch: 40, Batch number: 36, Loss: 7378.30322265625\n",
      "Epoch: 41, Batch number: 60, Loss: 7201.412109375\n",
      "Epoch: 43, Batch number: 8, Loss: 7183.5908203125\n",
      "Epoch: 44, Batch number: 32, Loss: 7192.00634765625\n",
      "Epoch: 45, Batch number: 56, Loss: 7475.4833984375\n",
      "Epoch: 47, Batch number: 4, Loss: 7240.6494140625\n",
      "Epoch: 48, Batch number: 28, Loss: 7131.59130859375\n",
      "Epoch: 49, Batch number: 52, Loss: 7046.93408203125\n",
      "Epoch: 51, Batch number: 0, Loss: 7042.6337890625\n",
      "Epoch: 52, Batch number: 24, Loss: 7372.9697265625\n",
      "Epoch: 53, Batch number: 48, Loss: 7342.24267578125\n",
      "Epoch: 54, Batch number: 72, Loss: 7232.1845703125\n",
      "Epoch: 56, Batch number: 20, Loss: 7201.2236328125\n",
      "Epoch: 57, Batch number: 44, Loss: 7365.697265625\n",
      "Epoch: 58, Batch number: 68, Loss: 6833.4697265625\n",
      "Epoch: 60, Batch number: 16, Loss: 7093.46875\n",
      "Epoch: 61, Batch number: 40, Loss: 7171.548828125\n",
      "Epoch: 62, Batch number: 64, Loss: 7299.87451171875\n",
      "Epoch: 64, Batch number: 12, Loss: 7049.02587890625\n",
      "Epoch: 65, Batch number: 36, Loss: 7114.4248046875\n",
      "Epoch: 66, Batch number: 60, Loss: 7248.88037109375\n",
      "Epoch: 68, Batch number: 8, Loss: 7060.3291015625\n",
      "Epoch: 69, Batch number: 32, Loss: 7058.85498046875\n",
      "Epoch: 70, Batch number: 56, Loss: 7218.6796875\n",
      "Epoch: 72, Batch number: 4, Loss: 6829.07373046875\n",
      "Epoch: 73, Batch number: 28, Loss: 7021.3017578125\n",
      "Epoch: 74, Batch number: 52, Loss: 7185.75830078125\n",
      "Epoch: 76, Batch number: 0, Loss: 6900.06640625\n",
      "Epoch: 77, Batch number: 24, Loss: 7041.5419921875\n",
      "Epoch: 78, Batch number: 48, Loss: 7227.87158203125\n",
      "Epoch: 79, Batch number: 72, Loss: 6972.6611328125\n",
      "Epoch: 81, Batch number: 20, Loss: 7186.767578125\n",
      "Epoch: 82, Batch number: 44, Loss: 6761.9892578125\n",
      "Epoch: 83, Batch number: 68, Loss: 7159.8359375\n",
      "Epoch: 85, Batch number: 16, Loss: 6711.4033203125\n",
      "Epoch: 86, Batch number: 40, Loss: 7092.8447265625\n",
      "Epoch: 87, Batch number: 64, Loss: 7199.77197265625\n",
      "Epoch: 89, Batch number: 12, Loss: 7258.376953125\n",
      "Epoch: 90, Batch number: 36, Loss: 7137.20947265625\n",
      "Epoch: 91, Batch number: 60, Loss: 7257.54345703125\n",
      "Epoch: 93, Batch number: 8, Loss: 6922.4208984375\n",
      "Epoch: 94, Batch number: 32, Loss: 7138.626953125\n",
      "Epoch: 95, Batch number: 56, Loss: 7121.73779296875\n",
      "Epoch: 97, Batch number: 4, Loss: 6957.87744140625\n",
      "Epoch: 98, Batch number: 28, Loss: 7031.59716796875\n",
      "Epoch: 99, Batch number: 52, Loss: 6964.57275390625\n",
      "Epoch: 101, Batch number: 0, Loss: 6832.5732421875\n",
      "Epoch: 102, Batch number: 24, Loss: 7019.66552734375\n",
      "Epoch: 103, Batch number: 48, Loss: 7183.64404296875\n",
      "Epoch: 104, Batch number: 72, Loss: 7246.31298828125\n",
      "Epoch: 106, Batch number: 20, Loss: 6901.0732421875\n",
      "Epoch: 107, Batch number: 44, Loss: 7107.03173828125\n",
      "Epoch: 108, Batch number: 68, Loss: 7097.11376953125\n",
      "Epoch: 110, Batch number: 16, Loss: 7106.97314453125\n",
      "Epoch: 111, Batch number: 40, Loss: 6992.79052734375\n",
      "Epoch: 112, Batch number: 64, Loss: 6822.22021484375\n",
      "Epoch: 114, Batch number: 12, Loss: 7023.44287109375\n",
      "Epoch: 115, Batch number: 36, Loss: 6957.0400390625\n",
      "Epoch: 116, Batch number: 60, Loss: 6925.32177734375\n",
      "Epoch: 118, Batch number: 8, Loss: 6874.11474609375\n",
      "Epoch: 119, Batch number: 32, Loss: 6879.89794921875\n",
      "Epoch: 120, Batch number: 56, Loss: 7212.27880859375\n",
      "Epoch: 122, Batch number: 4, Loss: 7081.6123046875\n",
      "Epoch: 123, Batch number: 28, Loss: 7208.80615234375\n",
      "Epoch: 124, Batch number: 52, Loss: 7026.75537109375\n",
      "Epoch: 126, Batch number: 0, Loss: 6924.37939453125\n",
      "Epoch: 127, Batch number: 24, Loss: 7102.4345703125\n",
      "Epoch: 128, Batch number: 48, Loss: 6912.9501953125\n",
      "Epoch: 129, Batch number: 72, Loss: 7062.64501953125\n",
      "Epoch: 131, Batch number: 20, Loss: 7463.26171875\n",
      "Epoch: 132, Batch number: 44, Loss: 7086.93017578125\n",
      "Epoch: 133, Batch number: 68, Loss: 6926.06982421875\n",
      "Epoch: 135, Batch number: 16, Loss: 6999.87548828125\n",
      "Epoch: 136, Batch number: 40, Loss: 7072.4453125\n",
      "Epoch: 137, Batch number: 64, Loss: 6979.2138671875\n",
      "Epoch: 139, Batch number: 12, Loss: 7021.0341796875\n",
      "Epoch: 140, Batch number: 36, Loss: 7495.94384765625\n",
      "Epoch: 141, Batch number: 60, Loss: 6968.3740234375\n",
      "Epoch: 143, Batch number: 8, Loss: 6702.4228515625\n",
      "Epoch: 144, Batch number: 32, Loss: 6970.8876953125\n",
      "Epoch: 145, Batch number: 56, Loss: 7127.2216796875\n",
      "Epoch: 147, Batch number: 4, Loss: 6807.888671875\n",
      "Epoch: 148, Batch number: 28, Loss: 6993.126953125\n",
      "Epoch: 149, Batch number: 52, Loss: 6973.95458984375\n",
      "Epoch: 151, Batch number: 0, Loss: 6989.14794921875\n",
      "Epoch: 152, Batch number: 24, Loss: 6943.6162109375\n",
      "Epoch: 153, Batch number: 48, Loss: 6899.70263671875\n",
      "Epoch: 154, Batch number: 72, Loss: 7498.07666015625\n",
      "Epoch: 156, Batch number: 20, Loss: 6895.05859375\n",
      "Epoch: 157, Batch number: 44, Loss: 7001.740234375\n",
      "Epoch: 158, Batch number: 68, Loss: 7254.5830078125\n",
      "Epoch: 160, Batch number: 16, Loss: 7057.53857421875\n",
      "Epoch: 161, Batch number: 40, Loss: 6937.2138671875\n",
      "Epoch: 162, Batch number: 64, Loss: 7245.2373046875\n",
      "Epoch: 164, Batch number: 12, Loss: 7161.033203125\n",
      "Epoch: 165, Batch number: 36, Loss: 6679.40283203125\n",
      "Epoch: 166, Batch number: 60, Loss: 7375.72314453125\n",
      "Epoch: 168, Batch number: 8, Loss: 6679.57421875\n",
      "Epoch: 169, Batch number: 32, Loss: 7111.38134765625\n",
      "Epoch: 170, Batch number: 56, Loss: 7100.439453125\n",
      "Epoch: 172, Batch number: 4, Loss: 7066.267578125\n",
      "Epoch: 173, Batch number: 28, Loss: 6840.70263671875\n",
      "Epoch: 174, Batch number: 52, Loss: 7053.30126953125\n",
      "Epoch: 176, Batch number: 0, Loss: 7114.3857421875\n",
      "Epoch: 177, Batch number: 24, Loss: 7104.29833984375\n",
      "Epoch: 178, Batch number: 48, Loss: 7206.18505859375\n",
      "Epoch: 179, Batch number: 72, Loss: 7146.25537109375\n",
      "Epoch: 181, Batch number: 20, Loss: 6898.431640625\n",
      "Epoch: 182, Batch number: 44, Loss: 7136.64794921875\n",
      "Epoch: 183, Batch number: 68, Loss: 7142.81787109375\n",
      "Epoch: 185, Batch number: 16, Loss: 6746.18994140625\n",
      "Epoch: 186, Batch number: 40, Loss: 6653.2490234375\n",
      "Epoch: 187, Batch number: 64, Loss: 6973.29248046875\n",
      "Epoch: 189, Batch number: 12, Loss: 7073.2509765625\n",
      "Epoch: 190, Batch number: 36, Loss: 7159.736328125\n",
      "Epoch: 191, Batch number: 60, Loss: 7259.02685546875\n",
      "Epoch: 193, Batch number: 8, Loss: 7084.53955078125\n",
      "Epoch: 194, Batch number: 32, Loss: 7002.92333984375\n",
      "Epoch: 195, Batch number: 56, Loss: 7172.423828125\n",
      "Epoch: 197, Batch number: 4, Loss: 6879.53369140625\n",
      "Epoch: 198, Batch number: 28, Loss: 7173.6298828125\n",
      "Epoch: 199, Batch number: 52, Loss: 6913.091796875\n",
      "Epoch: 201, Batch number: 0, Loss: 6807.75146484375\n",
      "Epoch: 202, Batch number: 24, Loss: 7154.802734375\n",
      "Epoch: 203, Batch number: 48, Loss: 7056.7958984375\n",
      "Epoch: 204, Batch number: 72, Loss: 7167.751953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 206, Batch number: 20, Loss: 7079.3193359375\n",
      "Epoch: 207, Batch number: 44, Loss: 7175.67333984375\n",
      "Epoch: 208, Batch number: 68, Loss: 6957.24609375\n",
      "Epoch: 210, Batch number: 16, Loss: 6716.90673828125\n",
      "Epoch: 211, Batch number: 40, Loss: 6947.52001953125\n",
      "Epoch: 212, Batch number: 64, Loss: 7065.7138671875\n",
      "Epoch: 214, Batch number: 12, Loss: 6777.7490234375\n",
      "Epoch: 215, Batch number: 36, Loss: 6738.94921875\n",
      "Epoch: 216, Batch number: 60, Loss: 6893.47265625\n",
      "Epoch: 218, Batch number: 8, Loss: 6920.6923828125\n",
      "Epoch: 219, Batch number: 32, Loss: 6933.88623046875\n",
      "Epoch: 220, Batch number: 56, Loss: 7150.83154296875\n",
      "Epoch: 222, Batch number: 4, Loss: 6609.7255859375\n",
      "Epoch: 223, Batch number: 28, Loss: 7144.12841796875\n",
      "Epoch: 224, Batch number: 52, Loss: 7060.8798828125\n",
      "Epoch: 226, Batch number: 0, Loss: 6762.61767578125\n",
      "Epoch: 227, Batch number: 24, Loss: 6826.2705078125\n",
      "Epoch: 228, Batch number: 48, Loss: 7068.05517578125\n",
      "Epoch: 229, Batch number: 72, Loss: 6904.880859375\n",
      "Epoch: 231, Batch number: 20, Loss: 7036.68359375\n",
      "Epoch: 232, Batch number: 44, Loss: 6812.193359375\n",
      "Epoch: 233, Batch number: 68, Loss: 7137.068359375\n",
      "Epoch: 235, Batch number: 16, Loss: 6999.8828125\n",
      "Epoch: 236, Batch number: 40, Loss: 7042.50634765625\n",
      "Epoch: 237, Batch number: 64, Loss: 7231.150390625\n",
      "Epoch: 239, Batch number: 12, Loss: 7018.04736328125\n",
      "Epoch: 240, Batch number: 36, Loss: 7019.4072265625\n",
      "Epoch: 241, Batch number: 60, Loss: 7192.65185546875\n",
      "Epoch: 243, Batch number: 8, Loss: 7173.40576171875\n",
      "Epoch: 244, Batch number: 32, Loss: 6765.57861328125\n",
      "Epoch: 245, Batch number: 56, Loss: 7177.29150390625\n",
      "Epoch: 247, Batch number: 4, Loss: 6870.59912109375\n",
      "Epoch: 248, Batch number: 28, Loss: 6622.046875\n",
      "Epoch: 249, Batch number: 52, Loss: 7333.53857421875\n",
      "Epoch: 251, Batch number: 0, Loss: 6773.59619140625\n",
      "Epoch: 252, Batch number: 24, Loss: 7017.98486328125\n",
      "Epoch: 253, Batch number: 48, Loss: 6924.708984375\n",
      "Epoch: 254, Batch number: 72, Loss: 7003.31103515625\n",
      "Epoch: 256, Batch number: 20, Loss: 6959.6357421875\n",
      "Epoch: 257, Batch number: 44, Loss: 6804.314453125\n",
      "Epoch: 258, Batch number: 68, Loss: 7112.36669921875\n",
      "Epoch: 260, Batch number: 16, Loss: 7181.7294921875\n",
      "Epoch: 261, Batch number: 40, Loss: 6936.1826171875\n",
      "Epoch: 262, Batch number: 64, Loss: 7333.96240234375\n",
      "Epoch: 264, Batch number: 12, Loss: 7129.13037109375\n",
      "Epoch: 265, Batch number: 36, Loss: 7201.5625\n",
      "Epoch: 266, Batch number: 60, Loss: 7165.70703125\n",
      "Epoch: 268, Batch number: 8, Loss: 6846.05615234375\n",
      "Epoch: 269, Batch number: 32, Loss: 7134.18798828125\n",
      "Epoch: 270, Batch number: 56, Loss: 7179.92236328125\n",
      "Epoch: 272, Batch number: 4, Loss: 7023.4140625\n",
      "Epoch: 273, Batch number: 28, Loss: 6828.35546875\n",
      "Epoch: 274, Batch number: 52, Loss: 7234.15234375\n",
      "Epoch: 276, Batch number: 0, Loss: 6994.64794921875\n",
      "Epoch: 277, Batch number: 24, Loss: 7207.57421875\n",
      "Epoch: 278, Batch number: 48, Loss: 7139.6455078125\n",
      "Epoch: 279, Batch number: 72, Loss: 7406.24658203125\n",
      "Epoch: 281, Batch number: 20, Loss: 7105.86474609375\n",
      "Epoch: 282, Batch number: 44, Loss: 7193.69189453125\n",
      "Epoch: 283, Batch number: 68, Loss: 7195.07373046875\n",
      "Epoch: 285, Batch number: 16, Loss: 6937.3779296875\n",
      "Epoch: 286, Batch number: 40, Loss: 6979.41162109375\n",
      "Epoch: 287, Batch number: 64, Loss: 7008.30126953125\n",
      "Epoch: 289, Batch number: 12, Loss: 6835.75048828125\n",
      "Epoch: 290, Batch number: 36, Loss: 7051.1279296875\n",
      "Epoch: 291, Batch number: 60, Loss: 6951.333984375\n",
      "Epoch: 293, Batch number: 8, Loss: 6994.29833984375\n",
      "Epoch: 294, Batch number: 32, Loss: 7098.3466796875\n",
      "Epoch: 295, Batch number: 56, Loss: 7316.16259765625\n",
      "Epoch: 297, Batch number: 4, Loss: 6891.74755859375\n",
      "Epoch: 298, Batch number: 28, Loss: 7009.25390625\n",
      "Epoch: 299, Batch number: 52, Loss: 7085.8037109375\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 21347.255859375\n",
      "Epoch: 2, Batch number: 24, Loss: 21285.88671875\n",
      "Epoch: 3, Batch number: 48, Loss: 20884.75390625\n",
      "Epoch: 4, Batch number: 72, Loss: 19624.15234375\n",
      "Epoch: 6, Batch number: 20, Loss: 19107.05859375\n",
      "Epoch: 7, Batch number: 44, Loss: 18898.400390625\n",
      "Epoch: 8, Batch number: 68, Loss: 17951.189453125\n",
      "Epoch: 10, Batch number: 16, Loss: 17757.953125\n",
      "Epoch: 11, Batch number: 40, Loss: 17298.05859375\n",
      "Epoch: 12, Batch number: 64, Loss: 17154.888671875\n",
      "Epoch: 14, Batch number: 12, Loss: 16653.5703125\n",
      "Epoch: 15, Batch number: 36, Loss: 16598.90625\n",
      "Epoch: 16, Batch number: 60, Loss: 16551.78515625\n",
      "Epoch: 18, Batch number: 8, Loss: 16008.0546875\n",
      "Epoch: 19, Batch number: 32, Loss: 16117.451171875\n",
      "Epoch: 20, Batch number: 56, Loss: 15799.9267578125\n",
      "Epoch: 22, Batch number: 4, Loss: 15462.3330078125\n",
      "Epoch: 23, Batch number: 28, Loss: 15480.255859375\n",
      "Epoch: 24, Batch number: 52, Loss: 15618.078125\n",
      "Epoch: 26, Batch number: 0, Loss: 15603.044921875\n",
      "Epoch: 27, Batch number: 24, Loss: 15527.95703125\n",
      "Epoch: 28, Batch number: 48, Loss: 15266.5771484375\n",
      "Epoch: 29, Batch number: 72, Loss: 15177.21484375\n",
      "Epoch: 31, Batch number: 20, Loss: 14482.6962890625\n",
      "Epoch: 32, Batch number: 44, Loss: 14776.1953125\n",
      "Epoch: 33, Batch number: 68, Loss: 14616.814453125\n",
      "Epoch: 35, Batch number: 16, Loss: 14608.994140625\n",
      "Epoch: 36, Batch number: 40, Loss: 14904.7724609375\n",
      "Epoch: 37, Batch number: 64, Loss: 14602.384765625\n",
      "Epoch: 39, Batch number: 12, Loss: 14533.6240234375\n",
      "Epoch: 40, Batch number: 36, Loss: 14015.9208984375\n",
      "Epoch: 41, Batch number: 60, Loss: 14357.3310546875\n",
      "Epoch: 43, Batch number: 8, Loss: 14171.580078125\n",
      "Epoch: 44, Batch number: 32, Loss: 13933.2734375\n",
      "Epoch: 45, Batch number: 56, Loss: 14098.5439453125\n",
      "Epoch: 47, Batch number: 4, Loss: 14092.6171875\n",
      "Epoch: 48, Batch number: 28, Loss: 14113.8408203125\n",
      "Epoch: 49, Batch number: 52, Loss: 14113.8837890625\n",
      "Epoch: 51, Batch number: 0, Loss: 13929.9384765625\n",
      "Epoch: 52, Batch number: 24, Loss: 14260.5302734375\n",
      "Epoch: 53, Batch number: 48, Loss: 14050.705078125\n",
      "Epoch: 54, Batch number: 72, Loss: 14249.0234375\n",
      "Epoch: 56, Batch number: 20, Loss: 13690.5712890625\n",
      "Epoch: 57, Batch number: 44, Loss: 13782.44921875\n",
      "Epoch: 58, Batch number: 68, Loss: 14000.5732421875\n",
      "Epoch: 60, Batch number: 16, Loss: 13599.6826171875\n",
      "Epoch: 61, Batch number: 40, Loss: 14034.3212890625\n",
      "Epoch: 62, Batch number: 64, Loss: 13714.5498046875\n",
      "Epoch: 64, Batch number: 12, Loss: 13689.537109375\n",
      "Epoch: 65, Batch number: 36, Loss: 13656.607421875\n",
      "Epoch: 66, Batch number: 60, Loss: 13616.255859375\n",
      "Epoch: 68, Batch number: 8, Loss: 13512.724609375\n",
      "Epoch: 69, Batch number: 32, Loss: 13564.6669921875\n",
      "Epoch: 70, Batch number: 56, Loss: 13371.759765625\n",
      "Epoch: 72, Batch number: 4, Loss: 13280.1123046875\n",
      "Epoch: 73, Batch number: 28, Loss: 13359.4140625\n",
      "Epoch: 74, Batch number: 52, Loss: 13542.8310546875\n",
      "Epoch: 76, Batch number: 0, Loss: 13352.326171875\n",
      "Epoch: 77, Batch number: 24, Loss: 13270.4365234375\n",
      "Epoch: 78, Batch number: 48, Loss: 13106.462890625\n",
      "Epoch: 79, Batch number: 72, Loss: 13071.7529296875\n",
      "Epoch: 81, Batch number: 20, Loss: 13248.6044921875\n",
      "Epoch: 82, Batch number: 44, Loss: 13288.9541015625\n",
      "Epoch: 83, Batch number: 68, Loss: 13470.169921875\n",
      "Epoch: 85, Batch number: 16, Loss: 13320.2998046875\n",
      "Epoch: 86, Batch number: 40, Loss: 13147.23828125\n",
      "Epoch: 87, Batch number: 64, Loss: 13157.9755859375\n",
      "Epoch: 89, Batch number: 12, Loss: 13142.19140625\n",
      "Epoch: 90, Batch number: 36, Loss: 12825.6181640625\n",
      "Epoch: 91, Batch number: 60, Loss: 13243.328125\n",
      "Epoch: 93, Batch number: 8, Loss: 12923.4453125\n",
      "Epoch: 94, Batch number: 32, Loss: 13314.6337890625\n",
      "Epoch: 95, Batch number: 56, Loss: 12939.6435546875\n",
      "Epoch: 97, Batch number: 4, Loss: 12793.701171875\n",
      "Epoch: 98, Batch number: 28, Loss: 12865.3642578125\n",
      "Epoch: 99, Batch number: 52, Loss: 12973.275390625\n",
      "Epoch: 101, Batch number: 0, Loss: 12997.94921875\n",
      "Epoch: 102, Batch number: 24, Loss: 12890.3759765625\n",
      "Epoch: 103, Batch number: 48, Loss: 13038.5595703125\n",
      "Epoch: 104, Batch number: 72, Loss: 13001.07421875\n",
      "Epoch: 106, Batch number: 20, Loss: 13023.6025390625\n",
      "Epoch: 107, Batch number: 44, Loss: 12612.1826171875\n",
      "Epoch: 108, Batch number: 68, Loss: 12752.9921875\n",
      "Epoch: 110, Batch number: 16, Loss: 12733.3232421875\n",
      "Epoch: 111, Batch number: 40, Loss: 12662.349609375\n",
      "Epoch: 112, Batch number: 64, Loss: 12764.1455078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Batch number: 12, Loss: 12582.25\n",
      "Epoch: 115, Batch number: 36, Loss: 12762.6767578125\n",
      "Epoch: 116, Batch number: 60, Loss: 12692.861328125\n",
      "Epoch: 118, Batch number: 8, Loss: 12646.0224609375\n",
      "Epoch: 119, Batch number: 32, Loss: 12429.189453125\n",
      "Epoch: 120, Batch number: 56, Loss: 12869.2578125\n",
      "Epoch: 122, Batch number: 4, Loss: 12260.2451171875\n",
      "Epoch: 123, Batch number: 28, Loss: 12787.6005859375\n",
      "Epoch: 124, Batch number: 52, Loss: 12611.2626953125\n",
      "Epoch: 126, Batch number: 0, Loss: 12505.82421875\n",
      "Epoch: 127, Batch number: 24, Loss: 12644.7705078125\n",
      "Epoch: 128, Batch number: 48, Loss: 12555.1728515625\n",
      "Epoch: 129, Batch number: 72, Loss: 12600.5703125\n",
      "Epoch: 131, Batch number: 20, Loss: 12286.701171875\n",
      "Epoch: 132, Batch number: 44, Loss: 12341.0712890625\n",
      "Epoch: 133, Batch number: 68, Loss: 12309.2236328125\n",
      "Epoch: 135, Batch number: 16, Loss: 12393.833984375\n",
      "Epoch: 136, Batch number: 40, Loss: 12275.1962890625\n",
      "Epoch: 137, Batch number: 64, Loss: 12452.91796875\n",
      "Epoch: 139, Batch number: 12, Loss: 12403.015625\n",
      "Epoch: 140, Batch number: 36, Loss: 12416.931640625\n",
      "Epoch: 141, Batch number: 60, Loss: 11910.8037109375\n",
      "Epoch: 143, Batch number: 8, Loss: 12296.5576171875\n",
      "Epoch: 144, Batch number: 32, Loss: 12411.71484375\n",
      "Epoch: 145, Batch number: 56, Loss: 12132.263671875\n",
      "Epoch: 147, Batch number: 4, Loss: 12199.5908203125\n",
      "Epoch: 148, Batch number: 28, Loss: 12321.68359375\n",
      "Epoch: 149, Batch number: 52, Loss: 12234.58203125\n",
      "Epoch: 151, Batch number: 0, Loss: 12185.1083984375\n",
      "Epoch: 152, Batch number: 24, Loss: 12275.8779296875\n",
      "Epoch: 153, Batch number: 48, Loss: 12149.9970703125\n",
      "Epoch: 154, Batch number: 72, Loss: 12353.5869140625\n",
      "Epoch: 156, Batch number: 20, Loss: 11806.5361328125\n",
      "Epoch: 157, Batch number: 44, Loss: 12246.84375\n",
      "Epoch: 158, Batch number: 68, Loss: 12220.7646484375\n",
      "Epoch: 160, Batch number: 16, Loss: 12211.353515625\n",
      "Epoch: 161, Batch number: 40, Loss: 11998.78515625\n",
      "Epoch: 162, Batch number: 64, Loss: 12303.1650390625\n",
      "Epoch: 164, Batch number: 12, Loss: 12152.232421875\n",
      "Epoch: 165, Batch number: 36, Loss: 12295.296875\n",
      "Epoch: 166, Batch number: 60, Loss: 12262.142578125\n",
      "Epoch: 168, Batch number: 8, Loss: 11838.4873046875\n",
      "Epoch: 169, Batch number: 32, Loss: 11982.626953125\n",
      "Epoch: 170, Batch number: 56, Loss: 12267.1484375\n",
      "Epoch: 172, Batch number: 4, Loss: 11664.654296875\n",
      "Epoch: 173, Batch number: 28, Loss: 11829.9072265625\n",
      "Epoch: 174, Batch number: 52, Loss: 12325.92578125\n",
      "Epoch: 176, Batch number: 0, Loss: 12041.7705078125\n",
      "Epoch: 177, Batch number: 24, Loss: 12018.9033203125\n",
      "Epoch: 178, Batch number: 48, Loss: 12061.40625\n",
      "Epoch: 179, Batch number: 72, Loss: 11827.8037109375\n",
      "Epoch: 181, Batch number: 20, Loss: 11559.8134765625\n",
      "Epoch: 182, Batch number: 44, Loss: 11821.98828125\n",
      "Epoch: 183, Batch number: 68, Loss: 11825.1416015625\n",
      "Epoch: 185, Batch number: 16, Loss: 12005.6748046875\n",
      "Epoch: 186, Batch number: 40, Loss: 11831.119140625\n",
      "Epoch: 187, Batch number: 64, Loss: 12057.49609375\n",
      "Epoch: 189, Batch number: 12, Loss: 11653.0224609375\n",
      "Epoch: 190, Batch number: 36, Loss: 11867.2275390625\n",
      "Epoch: 191, Batch number: 60, Loss: 12069.53515625\n",
      "Epoch: 193, Batch number: 8, Loss: 11982.4833984375\n",
      "Epoch: 194, Batch number: 32, Loss: 11674.384765625\n",
      "Epoch: 195, Batch number: 56, Loss: 11752.654296875\n",
      "Epoch: 197, Batch number: 4, Loss: 11893.4765625\n",
      "Epoch: 198, Batch number: 28, Loss: 12001.650390625\n",
      "Epoch: 199, Batch number: 52, Loss: 11671.8994140625\n",
      "Epoch: 201, Batch number: 0, Loss: 12117.841796875\n",
      "Epoch: 202, Batch number: 24, Loss: 11664.55859375\n",
      "Epoch: 203, Batch number: 48, Loss: 11510.6484375\n",
      "Epoch: 204, Batch number: 72, Loss: 11727.4853515625\n",
      "Epoch: 206, Batch number: 20, Loss: 11659.40234375\n",
      "Epoch: 207, Batch number: 44, Loss: 11791.8388671875\n",
      "Epoch: 208, Batch number: 68, Loss: 11422.978515625\n",
      "Epoch: 210, Batch number: 16, Loss: 11331.4794921875\n",
      "Epoch: 211, Batch number: 40, Loss: 11644.9306640625\n",
      "Epoch: 212, Batch number: 64, Loss: 11706.845703125\n",
      "Epoch: 214, Batch number: 12, Loss: 11689.423828125\n",
      "Epoch: 215, Batch number: 36, Loss: 11862.1572265625\n",
      "Epoch: 216, Batch number: 60, Loss: 11739.1572265625\n",
      "Epoch: 218, Batch number: 8, Loss: 11633.8955078125\n",
      "Epoch: 219, Batch number: 32, Loss: 11675.1357421875\n",
      "Epoch: 220, Batch number: 56, Loss: 11572.6337890625\n",
      "Epoch: 222, Batch number: 4, Loss: 12065.5986328125\n",
      "Epoch: 223, Batch number: 28, Loss: 11407.712890625\n",
      "Epoch: 224, Batch number: 52, Loss: 11487.962890625\n",
      "Epoch: 226, Batch number: 0, Loss: 11740.7568359375\n",
      "Epoch: 227, Batch number: 24, Loss: 11565.7958984375\n",
      "Epoch: 228, Batch number: 48, Loss: 11676.5107421875\n",
      "Epoch: 229, Batch number: 72, Loss: 11741.578125\n",
      "Epoch: 231, Batch number: 20, Loss: 11094.4814453125\n",
      "Epoch: 232, Batch number: 44, Loss: 11711.060546875\n",
      "Epoch: 233, Batch number: 68, Loss: 11857.9501953125\n",
      "Epoch: 235, Batch number: 16, Loss: 11495.009765625\n",
      "Epoch: 236, Batch number: 40, Loss: 11569.501953125\n",
      "Epoch: 237, Batch number: 64, Loss: 11372.205078125\n",
      "Epoch: 239, Batch number: 12, Loss: 11777.697265625\n",
      "Epoch: 240, Batch number: 36, Loss: 11727.65625\n",
      "Epoch: 241, Batch number: 60, Loss: 11479.560546875\n",
      "Epoch: 243, Batch number: 8, Loss: 11860.5634765625\n",
      "Epoch: 244, Batch number: 32, Loss: 11738.91796875\n",
      "Epoch: 245, Batch number: 56, Loss: 11874.8818359375\n",
      "Epoch: 247, Batch number: 4, Loss: 11266.181640625\n",
      "Epoch: 248, Batch number: 28, Loss: 11664.3623046875\n",
      "Epoch: 249, Batch number: 52, Loss: 11875.806640625\n",
      "Epoch: 251, Batch number: 0, Loss: 11797.8486328125\n",
      "Epoch: 252, Batch number: 24, Loss: 11336.9443359375\n",
      "Epoch: 253, Batch number: 48, Loss: 11610.154296875\n",
      "Epoch: 254, Batch number: 72, Loss: 11544.9208984375\n",
      "Epoch: 256, Batch number: 20, Loss: 11790.3955078125\n",
      "Epoch: 257, Batch number: 44, Loss: 11442.72265625\n",
      "Epoch: 258, Batch number: 68, Loss: 11369.9501953125\n",
      "Epoch: 260, Batch number: 16, Loss: 11690.806640625\n",
      "Epoch: 261, Batch number: 40, Loss: 11547.0712890625\n",
      "Epoch: 262, Batch number: 64, Loss: 11771.3896484375\n",
      "Epoch: 264, Batch number: 12, Loss: 11370.6953125\n",
      "Epoch: 265, Batch number: 36, Loss: 11396.8818359375\n",
      "Epoch: 266, Batch number: 60, Loss: 11383.1953125\n",
      "Epoch: 268, Batch number: 8, Loss: 11464.7939453125\n",
      "Epoch: 269, Batch number: 32, Loss: 11602.1640625\n",
      "Epoch: 270, Batch number: 56, Loss: 11309.326171875\n",
      "Epoch: 272, Batch number: 4, Loss: 11306.9384765625\n",
      "Epoch: 273, Batch number: 28, Loss: 11661.6923828125\n",
      "Epoch: 274, Batch number: 52, Loss: 11196.4140625\n",
      "Epoch: 276, Batch number: 0, Loss: 11383.513671875\n",
      "Epoch: 277, Batch number: 24, Loss: 11366.2578125\n",
      "Epoch: 278, Batch number: 48, Loss: 11257.0361328125\n",
      "Epoch: 279, Batch number: 72, Loss: 11266.728515625\n",
      "Epoch: 281, Batch number: 20, Loss: 11261.029296875\n",
      "Epoch: 282, Batch number: 44, Loss: 11402.333984375\n",
      "Epoch: 283, Batch number: 68, Loss: 11047.62890625\n",
      "Epoch: 285, Batch number: 16, Loss: 11356.3740234375\n",
      "Epoch: 286, Batch number: 40, Loss: 11719.787109375\n",
      "Epoch: 287, Batch number: 64, Loss: 11534.498046875\n",
      "Epoch: 289, Batch number: 12, Loss: 11435.462890625\n",
      "Epoch: 290, Batch number: 36, Loss: 11292.1416015625\n",
      "Epoch: 291, Batch number: 60, Loss: 11035.474609375\n",
      "Epoch: 293, Batch number: 8, Loss: 11182.9677734375\n",
      "Epoch: 294, Batch number: 32, Loss: 11331.6171875\n",
      "Epoch: 295, Batch number: 56, Loss: 10993.5908203125\n",
      "Epoch: 297, Batch number: 4, Loss: 11327.0478515625\n",
      "Epoch: 298, Batch number: 28, Loss: 10931.2080078125\n",
      "Epoch: 299, Batch number: 52, Loss: 11432.681640625\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 20841.26953125\n",
      "Epoch: 2, Batch number: 24, Loss: 20697.3671875\n",
      "Epoch: 3, Batch number: 48, Loss: 20122.638671875\n",
      "Epoch: 4, Batch number: 72, Loss: 19287.728515625\n",
      "Epoch: 6, Batch number: 20, Loss: 18124.923828125\n",
      "Epoch: 7, Batch number: 44, Loss: 18184.611328125\n",
      "Epoch: 8, Batch number: 68, Loss: 16742.109375\n",
      "Epoch: 10, Batch number: 16, Loss: 16326.4765625\n",
      "Epoch: 11, Batch number: 40, Loss: 16381.1474609375\n",
      "Epoch: 12, Batch number: 64, Loss: 16178.4306640625\n",
      "Epoch: 14, Batch number: 12, Loss: 15728.1484375\n",
      "Epoch: 15, Batch number: 36, Loss: 15473.970703125\n",
      "Epoch: 16, Batch number: 60, Loss: 15315.107421875\n",
      "Epoch: 18, Batch number: 8, Loss: 14794.1572265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Batch number: 32, Loss: 14750.654296875\n",
      "Epoch: 20, Batch number: 56, Loss: 14792.708984375\n",
      "Epoch: 22, Batch number: 4, Loss: 14511.626953125\n",
      "Epoch: 23, Batch number: 28, Loss: 14392.876953125\n",
      "Epoch: 24, Batch number: 52, Loss: 14137.0498046875\n",
      "Epoch: 26, Batch number: 0, Loss: 14087.060546875\n",
      "Epoch: 27, Batch number: 24, Loss: 13837.533203125\n",
      "Epoch: 28, Batch number: 48, Loss: 13759.900390625\n",
      "Epoch: 29, Batch number: 72, Loss: 14142.1552734375\n",
      "Epoch: 31, Batch number: 20, Loss: 13421.8681640625\n",
      "Epoch: 32, Batch number: 44, Loss: 13394.4306640625\n",
      "Epoch: 33, Batch number: 68, Loss: 13385.1826171875\n",
      "Epoch: 35, Batch number: 16, Loss: 13283.384765625\n",
      "Epoch: 36, Batch number: 40, Loss: 13173.3115234375\n",
      "Epoch: 37, Batch number: 64, Loss: 13194.1318359375\n",
      "Epoch: 39, Batch number: 12, Loss: 12878.8779296875\n",
      "Epoch: 40, Batch number: 36, Loss: 13140.0263671875\n",
      "Epoch: 41, Batch number: 60, Loss: 12959.33203125\n",
      "Epoch: 43, Batch number: 8, Loss: 12996.9873046875\n",
      "Epoch: 44, Batch number: 32, Loss: 12601.4560546875\n",
      "Epoch: 45, Batch number: 56, Loss: 12884.7421875\n",
      "Epoch: 47, Batch number: 4, Loss: 12703.556640625\n",
      "Epoch: 48, Batch number: 28, Loss: 12334.0576171875\n",
      "Epoch: 49, Batch number: 52, Loss: 12647.73046875\n",
      "Epoch: 51, Batch number: 0, Loss: 12604.3125\n",
      "Epoch: 52, Batch number: 24, Loss: 12733.021484375\n",
      "Epoch: 53, Batch number: 48, Loss: 12526.849609375\n",
      "Epoch: 54, Batch number: 72, Loss: 12626.677734375\n",
      "Epoch: 56, Batch number: 20, Loss: 12506.0771484375\n",
      "Epoch: 57, Batch number: 44, Loss: 12595.728515625\n",
      "Epoch: 58, Batch number: 68, Loss: 12529.615234375\n",
      "Epoch: 60, Batch number: 16, Loss: 12220.662109375\n",
      "Epoch: 61, Batch number: 40, Loss: 12114.5068359375\n",
      "Epoch: 62, Batch number: 64, Loss: 12613.67578125\n",
      "Epoch: 64, Batch number: 12, Loss: 12248.31640625\n",
      "Epoch: 65, Batch number: 36, Loss: 12232.13671875\n",
      "Epoch: 66, Batch number: 60, Loss: 12093.9521484375\n",
      "Epoch: 68, Batch number: 8, Loss: 12309.7197265625\n",
      "Epoch: 69, Batch number: 32, Loss: 12121.509765625\n",
      "Epoch: 70, Batch number: 56, Loss: 12187.119140625\n",
      "Epoch: 72, Batch number: 4, Loss: 12227.9091796875\n",
      "Epoch: 73, Batch number: 28, Loss: 12214.0859375\n",
      "Epoch: 74, Batch number: 52, Loss: 11908.2900390625\n",
      "Epoch: 76, Batch number: 0, Loss: 12236.4365234375\n",
      "Epoch: 77, Batch number: 24, Loss: 12225.794921875\n",
      "Epoch: 78, Batch number: 48, Loss: 11746.947265625\n",
      "Epoch: 79, Batch number: 72, Loss: 12221.12890625\n",
      "Epoch: 81, Batch number: 20, Loss: 12186.9599609375\n",
      "Epoch: 82, Batch number: 44, Loss: 12024.884765625\n",
      "Epoch: 83, Batch number: 68, Loss: 11978.4462890625\n",
      "Epoch: 85, Batch number: 16, Loss: 11719.6796875\n",
      "Epoch: 86, Batch number: 40, Loss: 11996.861328125\n",
      "Epoch: 87, Batch number: 64, Loss: 12005.640625\n",
      "Epoch: 89, Batch number: 12, Loss: 12056.9267578125\n",
      "Epoch: 90, Batch number: 36, Loss: 11649.84765625\n",
      "Epoch: 91, Batch number: 60, Loss: 12411.796875\n",
      "Epoch: 93, Batch number: 8, Loss: 11994.25\n",
      "Epoch: 94, Batch number: 32, Loss: 11562.28125\n",
      "Epoch: 95, Batch number: 56, Loss: 11538.9306640625\n",
      "Epoch: 97, Batch number: 4, Loss: 11383.74609375\n",
      "Epoch: 98, Batch number: 28, Loss: 11378.5224609375\n",
      "Epoch: 99, Batch number: 52, Loss: 11751.740234375\n",
      "Epoch: 101, Batch number: 0, Loss: 11832.623046875\n",
      "Epoch: 102, Batch number: 24, Loss: 11709.2978515625\n",
      "Epoch: 103, Batch number: 48, Loss: 11457.732421875\n",
      "Epoch: 104, Batch number: 72, Loss: 11503.607421875\n",
      "Epoch: 106, Batch number: 20, Loss: 11473.814453125\n",
      "Epoch: 107, Batch number: 44, Loss: 11496.697265625\n",
      "Epoch: 108, Batch number: 68, Loss: 11804.689453125\n",
      "Epoch: 110, Batch number: 16, Loss: 11667.943359375\n",
      "Epoch: 111, Batch number: 40, Loss: 11343.5791015625\n",
      "Epoch: 112, Batch number: 64, Loss: 11475.1259765625\n",
      "Epoch: 114, Batch number: 12, Loss: 11789.41796875\n",
      "Epoch: 115, Batch number: 36, Loss: 11161.380859375\n",
      "Epoch: 116, Batch number: 60, Loss: 11580.21875\n",
      "Epoch: 118, Batch number: 8, Loss: 11311.1943359375\n",
      "Epoch: 119, Batch number: 32, Loss: 11236.1396484375\n",
      "Epoch: 120, Batch number: 56, Loss: 11703.6826171875\n",
      "Epoch: 122, Batch number: 4, Loss: 11503.984375\n",
      "Epoch: 123, Batch number: 28, Loss: 11194.369140625\n",
      "Epoch: 124, Batch number: 52, Loss: 11120.650390625\n",
      "Epoch: 126, Batch number: 0, Loss: 11176.6826171875\n",
      "Epoch: 127, Batch number: 24, Loss: 11413.1298828125\n",
      "Epoch: 128, Batch number: 48, Loss: 11514.11328125\n",
      "Epoch: 129, Batch number: 72, Loss: 11236.2763671875\n",
      "Epoch: 131, Batch number: 20, Loss: 11264.7197265625\n",
      "Epoch: 132, Batch number: 44, Loss: 11518.7900390625\n",
      "Epoch: 133, Batch number: 68, Loss: 11461.2216796875\n",
      "Epoch: 135, Batch number: 16, Loss: 11246.0947265625\n",
      "Epoch: 136, Batch number: 40, Loss: 10940.7001953125\n",
      "Epoch: 137, Batch number: 64, Loss: 11088.3837890625\n",
      "Epoch: 139, Batch number: 12, Loss: 11777.4638671875\n",
      "Epoch: 140, Batch number: 36, Loss: 10963.0986328125\n",
      "Epoch: 141, Batch number: 60, Loss: 11263.0947265625\n",
      "Epoch: 143, Batch number: 8, Loss: 11323.193359375\n",
      "Epoch: 144, Batch number: 32, Loss: 11274.6083984375\n",
      "Epoch: 145, Batch number: 56, Loss: 11247.1845703125\n",
      "Epoch: 147, Batch number: 4, Loss: 11207.728515625\n",
      "Epoch: 148, Batch number: 28, Loss: 11037.4931640625\n",
      "Epoch: 149, Batch number: 52, Loss: 10871.9716796875\n",
      "Epoch: 151, Batch number: 0, Loss: 11081.28125\n",
      "Epoch: 152, Batch number: 24, Loss: 11071.328125\n",
      "Epoch: 153, Batch number: 48, Loss: 11157.94140625\n",
      "Epoch: 154, Batch number: 72, Loss: 11206.443359375\n",
      "Epoch: 156, Batch number: 20, Loss: 11054.8203125\n",
      "Epoch: 157, Batch number: 44, Loss: 11398.0537109375\n",
      "Epoch: 158, Batch number: 68, Loss: 11053.7431640625\n",
      "Epoch: 160, Batch number: 16, Loss: 11111.765625\n",
      "Epoch: 161, Batch number: 40, Loss: 11066.0693359375\n",
      "Epoch: 162, Batch number: 64, Loss: 10757.5634765625\n",
      "Epoch: 164, Batch number: 12, Loss: 11122.0302734375\n",
      "Epoch: 165, Batch number: 36, Loss: 10967.49609375\n",
      "Epoch: 166, Batch number: 60, Loss: 10962.09765625\n",
      "Epoch: 168, Batch number: 8, Loss: 11075.0322265625\n",
      "Epoch: 169, Batch number: 32, Loss: 11215.4404296875\n",
      "Epoch: 170, Batch number: 56, Loss: 10746.7822265625\n",
      "Epoch: 172, Batch number: 4, Loss: 10950.60546875\n",
      "Epoch: 173, Batch number: 28, Loss: 11031.9931640625\n",
      "Epoch: 174, Batch number: 52, Loss: 10620.0458984375\n",
      "Epoch: 176, Batch number: 0, Loss: 10757.59375\n",
      "Epoch: 177, Batch number: 24, Loss: 11265.19921875\n",
      "Epoch: 178, Batch number: 48, Loss: 10906.46875\n",
      "Epoch: 179, Batch number: 72, Loss: 11267.8671875\n",
      "Epoch: 181, Batch number: 20, Loss: 10989.037109375\n",
      "Epoch: 182, Batch number: 44, Loss: 11264.5185546875\n",
      "Epoch: 183, Batch number: 68, Loss: 10794.4921875\n",
      "Epoch: 185, Batch number: 16, Loss: 11094.521484375\n",
      "Epoch: 186, Batch number: 40, Loss: 10945.271484375\n",
      "Epoch: 187, Batch number: 64, Loss: 10971.37109375\n",
      "Epoch: 189, Batch number: 12, Loss: 11083.1904296875\n",
      "Epoch: 190, Batch number: 36, Loss: 10872.7734375\n",
      "Epoch: 191, Batch number: 60, Loss: 11373.837890625\n",
      "Epoch: 193, Batch number: 8, Loss: 10964.0224609375\n",
      "Epoch: 194, Batch number: 32, Loss: 10763.2822265625\n",
      "Epoch: 195, Batch number: 56, Loss: 10851.890625\n",
      "Epoch: 197, Batch number: 4, Loss: 10668.4482421875\n",
      "Epoch: 198, Batch number: 28, Loss: 10712.203125\n",
      "Epoch: 199, Batch number: 52, Loss: 11172.630859375\n",
      "Epoch: 201, Batch number: 0, Loss: 10778.671875\n",
      "Epoch: 202, Batch number: 24, Loss: 10967.7890625\n",
      "Epoch: 203, Batch number: 48, Loss: 10961.5146484375\n",
      "Epoch: 204, Batch number: 72, Loss: 10611.6337890625\n",
      "Epoch: 206, Batch number: 20, Loss: 10793.923828125\n",
      "Epoch: 207, Batch number: 44, Loss: 10940.1083984375\n",
      "Epoch: 208, Batch number: 68, Loss: 10802.9482421875\n",
      "Epoch: 210, Batch number: 16, Loss: 10813.83984375\n",
      "Epoch: 211, Batch number: 40, Loss: 10536.748046875\n",
      "Epoch: 212, Batch number: 64, Loss: 10846.611328125\n",
      "Epoch: 214, Batch number: 12, Loss: 10903.4853515625\n",
      "Epoch: 215, Batch number: 36, Loss: 10873.7763671875\n",
      "Epoch: 216, Batch number: 60, Loss: 10643.1748046875\n",
      "Epoch: 218, Batch number: 8, Loss: 10888.5966796875\n",
      "Epoch: 219, Batch number: 32, Loss: 10745.849609375\n",
      "Epoch: 220, Batch number: 56, Loss: 10711.3251953125\n",
      "Epoch: 222, Batch number: 4, Loss: 10751.2548828125\n",
      "Epoch: 223, Batch number: 28, Loss: 10790.890625\n",
      "Epoch: 224, Batch number: 52, Loss: 10637.6533203125\n",
      "Epoch: 226, Batch number: 0, Loss: 10621.4677734375\n",
      "Epoch: 227, Batch number: 24, Loss: 10777.67578125\n",
      "Epoch: 228, Batch number: 48, Loss: 10925.7373046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229, Batch number: 72, Loss: 10742.4189453125\n",
      "Epoch: 231, Batch number: 20, Loss: 10672.2119140625\n",
      "Epoch: 232, Batch number: 44, Loss: 10804.955078125\n",
      "Epoch: 233, Batch number: 68, Loss: 10713.4716796875\n",
      "Epoch: 235, Batch number: 16, Loss: 10622.3447265625\n",
      "Epoch: 236, Batch number: 40, Loss: 10773.947265625\n",
      "Epoch: 237, Batch number: 64, Loss: 11022.5751953125\n",
      "Epoch: 239, Batch number: 12, Loss: 10386.7236328125\n",
      "Epoch: 240, Batch number: 36, Loss: 10563.5947265625\n",
      "Epoch: 241, Batch number: 60, Loss: 10978.1015625\n",
      "Epoch: 243, Batch number: 8, Loss: 10642.7138671875\n",
      "Epoch: 244, Batch number: 32, Loss: 10817.9814453125\n",
      "Epoch: 245, Batch number: 56, Loss: 10489.6083984375\n",
      "Epoch: 247, Batch number: 4, Loss: 10807.1875\n",
      "Epoch: 248, Batch number: 28, Loss: 10436.7685546875\n",
      "Epoch: 249, Batch number: 52, Loss: 10851.2421875\n",
      "Epoch: 251, Batch number: 0, Loss: 10482.29296875\n",
      "Epoch: 252, Batch number: 24, Loss: 10342.8876953125\n",
      "Epoch: 253, Batch number: 48, Loss: 10825.421875\n",
      "Epoch: 254, Batch number: 72, Loss: 10573.9091796875\n",
      "Epoch: 256, Batch number: 20, Loss: 10676.5751953125\n",
      "Epoch: 257, Batch number: 44, Loss: 10788.96484375\n",
      "Epoch: 258, Batch number: 68, Loss: 11045.33203125\n",
      "Epoch: 260, Batch number: 16, Loss: 10695.9609375\n",
      "Epoch: 261, Batch number: 40, Loss: 10794.099609375\n",
      "Epoch: 262, Batch number: 64, Loss: 10630.9150390625\n",
      "Epoch: 264, Batch number: 12, Loss: 10908.8037109375\n",
      "Epoch: 265, Batch number: 36, Loss: 10910.046875\n",
      "Epoch: 266, Batch number: 60, Loss: 10365.4765625\n",
      "Epoch: 268, Batch number: 8, Loss: 10535.716796875\n",
      "Epoch: 269, Batch number: 32, Loss: 10457.1259765625\n",
      "Epoch: 270, Batch number: 56, Loss: 11185.7802734375\n",
      "Epoch: 272, Batch number: 4, Loss: 10343.60546875\n",
      "Epoch: 273, Batch number: 28, Loss: 10701.1279296875\n",
      "Epoch: 274, Batch number: 52, Loss: 10552.0869140625\n",
      "Epoch: 276, Batch number: 0, Loss: 10430.0625\n",
      "Epoch: 277, Batch number: 24, Loss: 10780.2353515625\n",
      "Epoch: 278, Batch number: 48, Loss: 10749.21875\n",
      "Epoch: 279, Batch number: 72, Loss: 10878.65234375\n",
      "Epoch: 281, Batch number: 20, Loss: 10690.9228515625\n",
      "Epoch: 282, Batch number: 44, Loss: 10806.578125\n",
      "Epoch: 283, Batch number: 68, Loss: 10548.70703125\n",
      "Epoch: 285, Batch number: 16, Loss: 10712.287109375\n",
      "Epoch: 286, Batch number: 40, Loss: 10547.6953125\n",
      "Epoch: 287, Batch number: 64, Loss: 10446.732421875\n",
      "Epoch: 289, Batch number: 12, Loss: 10395.4521484375\n",
      "Epoch: 290, Batch number: 36, Loss: 10502.7392578125\n",
      "Epoch: 291, Batch number: 60, Loss: 11112.6796875\n",
      "Epoch: 293, Batch number: 8, Loss: 10676.896484375\n",
      "Epoch: 294, Batch number: 32, Loss: 10973.6865234375\n",
      "Epoch: 295, Batch number: 56, Loss: 10681.09765625\n",
      "Epoch: 297, Batch number: 4, Loss: 10386.0703125\n",
      "Epoch: 298, Batch number: 28, Loss: 10785.224609375\n",
      "Epoch: 299, Batch number: 52, Loss: 10743.083984375\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 21886.248046875\n",
      "Epoch: 2, Batch number: 24, Loss: 20454.783203125\n",
      "Epoch: 3, Batch number: 48, Loss: 19163.138671875\n",
      "Epoch: 4, Batch number: 72, Loss: 18493.41796875\n",
      "Epoch: 6, Batch number: 20, Loss: 17431.171875\n",
      "Epoch: 7, Batch number: 44, Loss: 17097.61328125\n",
      "Epoch: 8, Batch number: 68, Loss: 16817.814453125\n",
      "Epoch: 10, Batch number: 16, Loss: 15998.2626953125\n",
      "Epoch: 11, Batch number: 40, Loss: 15647.677734375\n",
      "Epoch: 12, Batch number: 64, Loss: 15498.03125\n",
      "Epoch: 14, Batch number: 12, Loss: 14865.2646484375\n",
      "Epoch: 15, Batch number: 36, Loss: 14634.3876953125\n",
      "Epoch: 16, Batch number: 60, Loss: 14173.138671875\n",
      "Epoch: 18, Batch number: 8, Loss: 14129.283203125\n",
      "Epoch: 19, Batch number: 32, Loss: 13942.6953125\n",
      "Epoch: 20, Batch number: 56, Loss: 13631.2578125\n",
      "Epoch: 22, Batch number: 4, Loss: 13567.96484375\n",
      "Epoch: 23, Batch number: 28, Loss: 13536.0517578125\n",
      "Epoch: 24, Batch number: 52, Loss: 13349.9560546875\n",
      "Epoch: 26, Batch number: 0, Loss: 13229.9365234375\n",
      "Epoch: 27, Batch number: 24, Loss: 12750.1181640625\n",
      "Epoch: 28, Batch number: 48, Loss: 13241.7099609375\n",
      "Epoch: 29, Batch number: 72, Loss: 12982.298828125\n",
      "Epoch: 31, Batch number: 20, Loss: 12902.4990234375\n",
      "Epoch: 32, Batch number: 44, Loss: 12580.6298828125\n",
      "Epoch: 33, Batch number: 68, Loss: 12723.48046875\n",
      "Epoch: 35, Batch number: 16, Loss: 12694.9443359375\n",
      "Epoch: 36, Batch number: 40, Loss: 12927.1298828125\n",
      "Epoch: 37, Batch number: 64, Loss: 12393.828125\n",
      "Epoch: 39, Batch number: 12, Loss: 12301.515625\n",
      "Epoch: 40, Batch number: 36, Loss: 12345.029296875\n",
      "Epoch: 41, Batch number: 60, Loss: 12399.306640625\n",
      "Epoch: 43, Batch number: 8, Loss: 12216.1025390625\n",
      "Epoch: 44, Batch number: 32, Loss: 12374.2109375\n",
      "Epoch: 45, Batch number: 56, Loss: 12213.4189453125\n",
      "Epoch: 47, Batch number: 4, Loss: 12116.841796875\n",
      "Epoch: 48, Batch number: 28, Loss: 11818.25390625\n",
      "Epoch: 49, Batch number: 52, Loss: 11956.580078125\n",
      "Epoch: 51, Batch number: 0, Loss: 11853.734375\n",
      "Epoch: 52, Batch number: 24, Loss: 11936.984375\n",
      "Epoch: 53, Batch number: 48, Loss: 11873.2470703125\n",
      "Epoch: 54, Batch number: 72, Loss: 12198.6591796875\n",
      "Epoch: 56, Batch number: 20, Loss: 11940.7314453125\n",
      "Epoch: 57, Batch number: 44, Loss: 12181.1796875\n",
      "Epoch: 58, Batch number: 68, Loss: 12104.98828125\n",
      "Epoch: 60, Batch number: 16, Loss: 11824.40234375\n",
      "Epoch: 61, Batch number: 40, Loss: 11803.6826171875\n",
      "Epoch: 62, Batch number: 64, Loss: 11948.0615234375\n",
      "Epoch: 64, Batch number: 12, Loss: 11471.533203125\n",
      "Epoch: 65, Batch number: 36, Loss: 11660.9794921875\n",
      "Epoch: 66, Batch number: 60, Loss: 11395.35546875\n",
      "Epoch: 68, Batch number: 8, Loss: 11286.2353515625\n",
      "Epoch: 69, Batch number: 32, Loss: 11446.814453125\n",
      "Epoch: 70, Batch number: 56, Loss: 11298.3427734375\n",
      "Epoch: 72, Batch number: 4, Loss: 11505.05859375\n",
      "Epoch: 73, Batch number: 28, Loss: 11697.830078125\n",
      "Epoch: 74, Batch number: 52, Loss: 11755.90625\n",
      "Epoch: 76, Batch number: 0, Loss: 11472.3662109375\n",
      "Epoch: 77, Batch number: 24, Loss: 11456.7333984375\n",
      "Epoch: 78, Batch number: 48, Loss: 11502.884765625\n",
      "Epoch: 79, Batch number: 72, Loss: 11520.634765625\n",
      "Epoch: 81, Batch number: 20, Loss: 11155.0556640625\n",
      "Epoch: 82, Batch number: 44, Loss: 11325.88671875\n",
      "Epoch: 83, Batch number: 68, Loss: 11226.1474609375\n",
      "Epoch: 85, Batch number: 16, Loss: 11269.58984375\n",
      "Epoch: 86, Batch number: 40, Loss: 11434.568359375\n",
      "Epoch: 87, Batch number: 64, Loss: 11058.0927734375\n",
      "Epoch: 89, Batch number: 12, Loss: 11479.87109375\n",
      "Epoch: 90, Batch number: 36, Loss: 11513.720703125\n",
      "Epoch: 91, Batch number: 60, Loss: 11447.279296875\n",
      "Epoch: 93, Batch number: 8, Loss: 11056.1220703125\n",
      "Epoch: 94, Batch number: 32, Loss: 11631.3330078125\n",
      "Epoch: 95, Batch number: 56, Loss: 11118.177734375\n",
      "Epoch: 97, Batch number: 4, Loss: 11143.5146484375\n",
      "Epoch: 98, Batch number: 28, Loss: 11152.81640625\n",
      "Epoch: 99, Batch number: 52, Loss: 11316.4482421875\n",
      "Epoch: 101, Batch number: 0, Loss: 10802.2919921875\n",
      "Epoch: 102, Batch number: 24, Loss: 11027.75390625\n",
      "Epoch: 103, Batch number: 48, Loss: 11205.86328125\n",
      "Epoch: 104, Batch number: 72, Loss: 11173.21875\n",
      "Epoch: 106, Batch number: 20, Loss: 10911.923828125\n",
      "Epoch: 107, Batch number: 44, Loss: 10883.255859375\n",
      "Epoch: 108, Batch number: 68, Loss: 11430.75\n",
      "Epoch: 110, Batch number: 16, Loss: 11047.4990234375\n",
      "Epoch: 111, Batch number: 40, Loss: 11249.1259765625\n",
      "Epoch: 112, Batch number: 64, Loss: 11360.5244140625\n",
      "Epoch: 114, Batch number: 12, Loss: 10876.1201171875\n",
      "Epoch: 115, Batch number: 36, Loss: 10913.861328125\n",
      "Epoch: 116, Batch number: 60, Loss: 10987.046875\n",
      "Epoch: 118, Batch number: 8, Loss: 11222.033203125\n",
      "Epoch: 119, Batch number: 32, Loss: 10418.033203125\n",
      "Epoch: 120, Batch number: 56, Loss: 10586.2607421875\n",
      "Epoch: 122, Batch number: 4, Loss: 10918.90234375\n",
      "Epoch: 123, Batch number: 28, Loss: 10870.912109375\n",
      "Epoch: 124, Batch number: 52, Loss: 11112.5390625\n",
      "Epoch: 126, Batch number: 0, Loss: 11175.5634765625\n",
      "Epoch: 127, Batch number: 24, Loss: 10746.076171875\n",
      "Epoch: 128, Batch number: 48, Loss: 10636.3203125\n",
      "Epoch: 129, Batch number: 72, Loss: 10877.16796875\n",
      "Epoch: 131, Batch number: 20, Loss: 11228.564453125\n",
      "Epoch: 132, Batch number: 44, Loss: 10998.2578125\n",
      "Epoch: 133, Batch number: 68, Loss: 10732.333984375\n",
      "Epoch: 135, Batch number: 16, Loss: 10542.9130859375\n",
      "Epoch: 136, Batch number: 40, Loss: 10942.341796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137, Batch number: 64, Loss: 11250.8662109375\n",
      "Epoch: 139, Batch number: 12, Loss: 10387.9736328125\n",
      "Epoch: 140, Batch number: 36, Loss: 10750.3994140625\n",
      "Epoch: 141, Batch number: 60, Loss: 10828.140625\n",
      "Epoch: 143, Batch number: 8, Loss: 10730.53125\n",
      "Epoch: 144, Batch number: 32, Loss: 10647.5390625\n",
      "Epoch: 145, Batch number: 56, Loss: 10928.205078125\n",
      "Epoch: 147, Batch number: 4, Loss: 11062.0556640625\n",
      "Epoch: 148, Batch number: 28, Loss: 10664.5126953125\n",
      "Epoch: 149, Batch number: 52, Loss: 10920.767578125\n",
      "Epoch: 151, Batch number: 0, Loss: 10916.05859375\n",
      "Epoch: 152, Batch number: 24, Loss: 10647.703125\n",
      "Epoch: 153, Batch number: 48, Loss: 10627.1318359375\n",
      "Epoch: 154, Batch number: 72, Loss: 10430.84765625\n",
      "Epoch: 156, Batch number: 20, Loss: 11144.205078125\n",
      "Epoch: 157, Batch number: 44, Loss: 10748.083984375\n",
      "Epoch: 158, Batch number: 68, Loss: 10809.8447265625\n",
      "Epoch: 160, Batch number: 16, Loss: 10650.2509765625\n",
      "Epoch: 161, Batch number: 40, Loss: 10798.4375\n",
      "Epoch: 162, Batch number: 64, Loss: 10791.4375\n",
      "Epoch: 164, Batch number: 12, Loss: 10619.896484375\n",
      "Epoch: 165, Batch number: 36, Loss: 10547.6787109375\n",
      "Epoch: 166, Batch number: 60, Loss: 10474.86328125\n",
      "Epoch: 168, Batch number: 8, Loss: 10238.5166015625\n",
      "Epoch: 169, Batch number: 32, Loss: 10614.4609375\n",
      "Epoch: 170, Batch number: 56, Loss: 10871.451171875\n",
      "Epoch: 172, Batch number: 4, Loss: 10533.15625\n",
      "Epoch: 173, Batch number: 28, Loss: 10592.31640625\n",
      "Epoch: 174, Batch number: 52, Loss: 10686.8134765625\n",
      "Epoch: 176, Batch number: 0, Loss: 10459.06640625\n",
      "Epoch: 177, Batch number: 24, Loss: 11232.6669921875\n",
      "Epoch: 178, Batch number: 48, Loss: 10482.953125\n",
      "Epoch: 179, Batch number: 72, Loss: 10576.6015625\n",
      "Epoch: 181, Batch number: 20, Loss: 10530.6640625\n",
      "Epoch: 182, Batch number: 44, Loss: 10602.4462890625\n",
      "Epoch: 183, Batch number: 68, Loss: 10928.7265625\n",
      "Epoch: 185, Batch number: 16, Loss: 10470.1943359375\n",
      "Epoch: 186, Batch number: 40, Loss: 10658.841796875\n",
      "Epoch: 187, Batch number: 64, Loss: 10612.001953125\n",
      "Epoch: 189, Batch number: 12, Loss: 10245.1328125\n",
      "Epoch: 190, Batch number: 36, Loss: 10453.3134765625\n",
      "Epoch: 191, Batch number: 60, Loss: 10643.8271484375\n",
      "Epoch: 193, Batch number: 8, Loss: 10583.1884765625\n",
      "Epoch: 194, Batch number: 32, Loss: 10898.3310546875\n",
      "Epoch: 195, Batch number: 56, Loss: 10401.337890625\n",
      "Epoch: 197, Batch number: 4, Loss: 10129.2236328125\n",
      "Epoch: 198, Batch number: 28, Loss: 10513.5419921875\n",
      "Epoch: 199, Batch number: 52, Loss: 10689.580078125\n",
      "Epoch: 201, Batch number: 0, Loss: 10284.658203125\n",
      "Epoch: 202, Batch number: 24, Loss: 10362.8779296875\n",
      "Epoch: 203, Batch number: 48, Loss: 10807.3173828125\n",
      "Epoch: 204, Batch number: 72, Loss: 10845.4189453125\n",
      "Epoch: 206, Batch number: 20, Loss: 10401.3515625\n",
      "Epoch: 207, Batch number: 44, Loss: 10495.9443359375\n",
      "Epoch: 208, Batch number: 68, Loss: 10638.1083984375\n",
      "Epoch: 210, Batch number: 16, Loss: 10352.7021484375\n",
      "Epoch: 211, Batch number: 40, Loss: 10401.4287109375\n",
      "Epoch: 212, Batch number: 64, Loss: 10520.955078125\n",
      "Epoch: 214, Batch number: 12, Loss: 10601.71484375\n",
      "Epoch: 215, Batch number: 36, Loss: 10290.19140625\n",
      "Epoch: 216, Batch number: 60, Loss: 10639.6171875\n",
      "Epoch: 218, Batch number: 8, Loss: 10595.8984375\n",
      "Epoch: 219, Batch number: 32, Loss: 10321.3232421875\n",
      "Epoch: 220, Batch number: 56, Loss: 10232.03515625\n",
      "Epoch: 222, Batch number: 4, Loss: 10769.0849609375\n",
      "Epoch: 223, Batch number: 28, Loss: 10425.5673828125\n",
      "Epoch: 224, Batch number: 52, Loss: 10908.49609375\n",
      "Epoch: 226, Batch number: 0, Loss: 10025.4365234375\n",
      "Epoch: 227, Batch number: 24, Loss: 10625.546875\n",
      "Epoch: 228, Batch number: 48, Loss: 10460.3447265625\n",
      "Epoch: 229, Batch number: 72, Loss: 10447.0654296875\n",
      "Epoch: 231, Batch number: 20, Loss: 10731.861328125\n",
      "Epoch: 232, Batch number: 44, Loss: 10615.9775390625\n",
      "Epoch: 233, Batch number: 68, Loss: 10926.255859375\n",
      "Epoch: 235, Batch number: 16, Loss: 10707.80078125\n",
      "Epoch: 236, Batch number: 40, Loss: 10879.953125\n",
      "Epoch: 237, Batch number: 64, Loss: 10779.228515625\n",
      "Epoch: 239, Batch number: 12, Loss: 10672.1376953125\n",
      "Epoch: 240, Batch number: 36, Loss: 10816.0751953125\n",
      "Epoch: 241, Batch number: 60, Loss: 10683.6025390625\n",
      "Epoch: 243, Batch number: 8, Loss: 10403.2685546875\n",
      "Epoch: 244, Batch number: 32, Loss: 10774.486328125\n",
      "Epoch: 245, Batch number: 56, Loss: 10451.9697265625\n",
      "Epoch: 247, Batch number: 4, Loss: 10335.7255859375\n",
      "Epoch: 248, Batch number: 28, Loss: 10436.2607421875\n",
      "Epoch: 249, Batch number: 52, Loss: 10709.060546875\n",
      "Epoch: 251, Batch number: 0, Loss: 10362.8779296875\n",
      "Epoch: 252, Batch number: 24, Loss: 10504.5673828125\n",
      "Epoch: 253, Batch number: 48, Loss: 10215.994140625\n",
      "Epoch: 254, Batch number: 72, Loss: 10665.5029296875\n",
      "Epoch: 256, Batch number: 20, Loss: 10356.2314453125\n",
      "Epoch: 257, Batch number: 44, Loss: 10093.4970703125\n",
      "Epoch: 258, Batch number: 68, Loss: 10613.009765625\n",
      "Epoch: 260, Batch number: 16, Loss: 10639.4169921875\n",
      "Epoch: 261, Batch number: 40, Loss: 10143.9296875\n",
      "Epoch: 262, Batch number: 64, Loss: 10479.2783203125\n",
      "Epoch: 264, Batch number: 12, Loss: 10202.0654296875\n",
      "Epoch: 265, Batch number: 36, Loss: 10451.8818359375\n",
      "Epoch: 266, Batch number: 60, Loss: 10760.828125\n",
      "Epoch: 268, Batch number: 8, Loss: 10177.2666015625\n",
      "Epoch: 269, Batch number: 32, Loss: 10317.970703125\n",
      "Epoch: 270, Batch number: 56, Loss: 10626.998046875\n",
      "Epoch: 272, Batch number: 4, Loss: 10388.7431640625\n",
      "Epoch: 273, Batch number: 28, Loss: 10470.607421875\n",
      "Epoch: 274, Batch number: 52, Loss: 10339.361328125\n",
      "Epoch: 276, Batch number: 0, Loss: 10299.0595703125\n",
      "Epoch: 277, Batch number: 24, Loss: 10641.8916015625\n",
      "Epoch: 278, Batch number: 48, Loss: 10529.51171875\n",
      "Epoch: 279, Batch number: 72, Loss: 10598.3271484375\n",
      "Epoch: 281, Batch number: 20, Loss: 10518.955078125\n",
      "Epoch: 282, Batch number: 44, Loss: 10236.1220703125\n",
      "Epoch: 283, Batch number: 68, Loss: 10461.8916015625\n",
      "Epoch: 285, Batch number: 16, Loss: 10455.9736328125\n",
      "Epoch: 286, Batch number: 40, Loss: 10822.0087890625\n",
      "Epoch: 287, Batch number: 64, Loss: 10432.5361328125\n",
      "Epoch: 289, Batch number: 12, Loss: 10569.62890625\n",
      "Epoch: 290, Batch number: 36, Loss: 10821.0625\n",
      "Epoch: 291, Batch number: 60, Loss: 10654.3603515625\n",
      "Epoch: 293, Batch number: 8, Loss: 10565.0625\n",
      "Epoch: 294, Batch number: 32, Loss: 10523.548828125\n",
      "Epoch: 295, Batch number: 56, Loss: 10132.1923828125\n",
      "Epoch: 297, Batch number: 4, Loss: 10183.5927734375\n",
      "Epoch: 298, Batch number: 28, Loss: 10443.087890625\n",
      "Epoch: 299, Batch number: 52, Loss: 10441.5751953125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 21790.189453125\n",
      "Epoch: 2, Batch number: 24, Loss: 20001.333984375\n",
      "Epoch: 3, Batch number: 48, Loss: 18810.037109375\n",
      "Epoch: 4, Batch number: 72, Loss: 17874.291015625\n",
      "Epoch: 6, Batch number: 20, Loss: 17171.7421875\n",
      "Epoch: 7, Batch number: 44, Loss: 16707.57421875\n",
      "Epoch: 8, Batch number: 68, Loss: 15962.025390625\n",
      "Epoch: 10, Batch number: 16, Loss: 15340.1943359375\n",
      "Epoch: 11, Batch number: 40, Loss: 15180.16796875\n",
      "Epoch: 12, Batch number: 64, Loss: 14534.0537109375\n",
      "Epoch: 14, Batch number: 12, Loss: 14307.724609375\n",
      "Epoch: 15, Batch number: 36, Loss: 14177.18359375\n",
      "Epoch: 16, Batch number: 60, Loss: 13618.669921875\n",
      "Epoch: 18, Batch number: 8, Loss: 13348.9873046875\n",
      "Epoch: 19, Batch number: 32, Loss: 13459.0712890625\n",
      "Epoch: 20, Batch number: 56, Loss: 13393.123046875\n",
      "Epoch: 22, Batch number: 4, Loss: 13133.3056640625\n",
      "Epoch: 23, Batch number: 28, Loss: 12585.3359375\n",
      "Epoch: 24, Batch number: 52, Loss: 12767.9228515625\n",
      "Epoch: 26, Batch number: 0, Loss: 12726.939453125\n",
      "Epoch: 27, Batch number: 24, Loss: 12245.1044921875\n",
      "Epoch: 28, Batch number: 48, Loss: 12404.1669921875\n",
      "Epoch: 29, Batch number: 72, Loss: 12469.9453125\n",
      "Epoch: 31, Batch number: 20, Loss: 12423.1884765625\n",
      "Epoch: 32, Batch number: 44, Loss: 12335.326171875\n",
      "Epoch: 33, Batch number: 68, Loss: 12077.796875\n",
      "Epoch: 35, Batch number: 16, Loss: 12142.95703125\n",
      "Epoch: 36, Batch number: 40, Loss: 11734.6533203125\n",
      "Epoch: 37, Batch number: 64, Loss: 11961.5302734375\n",
      "Epoch: 39, Batch number: 12, Loss: 11859.8671875\n",
      "Epoch: 40, Batch number: 36, Loss: 11977.4091796875\n",
      "Epoch: 41, Batch number: 60, Loss: 12089.509765625\n",
      "Epoch: 43, Batch number: 8, Loss: 11902.7119140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Batch number: 32, Loss: 11609.5947265625\n",
      "Epoch: 45, Batch number: 56, Loss: 11778.9228515625\n",
      "Epoch: 47, Batch number: 4, Loss: 11508.7890625\n",
      "Epoch: 48, Batch number: 28, Loss: 11614.8837890625\n",
      "Epoch: 49, Batch number: 52, Loss: 11431.408203125\n",
      "Epoch: 51, Batch number: 0, Loss: 11601.1904296875\n",
      "Epoch: 52, Batch number: 24, Loss: 11506.4736328125\n",
      "Epoch: 53, Batch number: 48, Loss: 11355.53515625\n",
      "Epoch: 54, Batch number: 72, Loss: 11891.5625\n",
      "Epoch: 56, Batch number: 20, Loss: 11635.84375\n",
      "Epoch: 57, Batch number: 44, Loss: 11608.421875\n",
      "Epoch: 58, Batch number: 68, Loss: 11184.7802734375\n",
      "Epoch: 60, Batch number: 16, Loss: 11282.4541015625\n",
      "Epoch: 61, Batch number: 40, Loss: 11322.5283203125\n",
      "Epoch: 62, Batch number: 64, Loss: 11238.7314453125\n",
      "Epoch: 64, Batch number: 12, Loss: 11473.349609375\n",
      "Epoch: 65, Batch number: 36, Loss: 11357.7626953125\n",
      "Epoch: 66, Batch number: 60, Loss: 11481.400390625\n",
      "Epoch: 68, Batch number: 8, Loss: 11039.1259765625\n",
      "Epoch: 69, Batch number: 32, Loss: 11009.34765625\n",
      "Epoch: 70, Batch number: 56, Loss: 11438.537109375\n",
      "Epoch: 72, Batch number: 4, Loss: 11012.634765625\n",
      "Epoch: 73, Batch number: 28, Loss: 10938.205078125\n",
      "Epoch: 74, Batch number: 52, Loss: 11024.3994140625\n",
      "Epoch: 76, Batch number: 0, Loss: 10842.2705078125\n",
      "Epoch: 77, Batch number: 24, Loss: 10884.265625\n",
      "Epoch: 78, Batch number: 48, Loss: 11258.6181640625\n",
      "Epoch: 79, Batch number: 72, Loss: 11303.1982421875\n",
      "Epoch: 81, Batch number: 20, Loss: 11101.7080078125\n",
      "Epoch: 82, Batch number: 44, Loss: 11608.4072265625\n",
      "Epoch: 83, Batch number: 68, Loss: 10656.611328125\n",
      "Epoch: 85, Batch number: 16, Loss: 10981.171875\n",
      "Epoch: 86, Batch number: 40, Loss: 11026.2099609375\n",
      "Epoch: 87, Batch number: 64, Loss: 11217.0107421875\n",
      "Epoch: 89, Batch number: 12, Loss: 11176.6103515625\n",
      "Epoch: 90, Batch number: 36, Loss: 10914.0693359375\n",
      "Epoch: 91, Batch number: 60, Loss: 10936.69921875\n",
      "Epoch: 93, Batch number: 8, Loss: 10663.87109375\n",
      "Epoch: 94, Batch number: 32, Loss: 10742.634765625\n",
      "Epoch: 95, Batch number: 56, Loss: 10915.0927734375\n",
      "Epoch: 97, Batch number: 4, Loss: 10702.59765625\n",
      "Epoch: 98, Batch number: 28, Loss: 10987.27734375\n",
      "Epoch: 99, Batch number: 52, Loss: 10730.1533203125\n",
      "Epoch: 101, Batch number: 0, Loss: 10642.1533203125\n",
      "Epoch: 102, Batch number: 24, Loss: 10809.3486328125\n",
      "Epoch: 103, Batch number: 48, Loss: 10741.5712890625\n",
      "Epoch: 104, Batch number: 72, Loss: 11282.6279296875\n",
      "Epoch: 106, Batch number: 20, Loss: 10441.23828125\n",
      "Epoch: 107, Batch number: 44, Loss: 10934.072265625\n",
      "Epoch: 108, Batch number: 68, Loss: 10489.8935546875\n",
      "Epoch: 110, Batch number: 16, Loss: 10743.73046875\n",
      "Epoch: 111, Batch number: 40, Loss: 10809.244140625\n",
      "Epoch: 112, Batch number: 64, Loss: 10687.3359375\n",
      "Epoch: 114, Batch number: 12, Loss: 10561.833984375\n",
      "Epoch: 115, Batch number: 36, Loss: 10770.2109375\n",
      "Epoch: 116, Batch number: 60, Loss: 10868.966796875\n",
      "Epoch: 118, Batch number: 8, Loss: 10468.2705078125\n",
      "Epoch: 119, Batch number: 32, Loss: 10555.2900390625\n",
      "Epoch: 120, Batch number: 56, Loss: 10650.2255859375\n",
      "Epoch: 122, Batch number: 4, Loss: 10481.2666015625\n",
      "Epoch: 123, Batch number: 28, Loss: 10916.2060546875\n",
      "Epoch: 124, Batch number: 52, Loss: 10751.251953125\n",
      "Epoch: 126, Batch number: 0, Loss: 10650.3935546875\n",
      "Epoch: 127, Batch number: 24, Loss: 10610.7333984375\n",
      "Epoch: 128, Batch number: 48, Loss: 10467.171875\n",
      "Epoch: 129, Batch number: 72, Loss: 10603.0888671875\n",
      "Epoch: 131, Batch number: 20, Loss: 10071.150390625\n",
      "Epoch: 132, Batch number: 44, Loss: 10937.6591796875\n",
      "Epoch: 133, Batch number: 68, Loss: 10814.900390625\n",
      "Epoch: 135, Batch number: 16, Loss: 10696.8837890625\n",
      "Epoch: 136, Batch number: 40, Loss: 10704.376953125\n",
      "Epoch: 137, Batch number: 64, Loss: 10610.728515625\n",
      "Epoch: 139, Batch number: 12, Loss: 10222.3720703125\n",
      "Epoch: 140, Batch number: 36, Loss: 10621.1943359375\n",
      "Epoch: 141, Batch number: 60, Loss: 10454.037109375\n",
      "Epoch: 143, Batch number: 8, Loss: 11034.380859375\n",
      "Epoch: 144, Batch number: 32, Loss: 10815.962890625\n",
      "Epoch: 145, Batch number: 56, Loss: 10635.0400390625\n",
      "Epoch: 147, Batch number: 4, Loss: 10251.65625\n",
      "Epoch: 148, Batch number: 28, Loss: 10468.6015625\n",
      "Epoch: 149, Batch number: 52, Loss: 10532.67578125\n",
      "Epoch: 151, Batch number: 0, Loss: 10571.8291015625\n",
      "Epoch: 152, Batch number: 24, Loss: 10544.1728515625\n",
      "Epoch: 153, Batch number: 48, Loss: 10368.056640625\n",
      "Epoch: 154, Batch number: 72, Loss: 10705.962890625\n",
      "Epoch: 156, Batch number: 20, Loss: 10337.4677734375\n",
      "Epoch: 157, Batch number: 44, Loss: 10425.7578125\n",
      "Epoch: 158, Batch number: 68, Loss: 10733.091796875\n",
      "Epoch: 160, Batch number: 16, Loss: 10823.279296875\n",
      "Epoch: 161, Batch number: 40, Loss: 10214.9814453125\n",
      "Epoch: 162, Batch number: 64, Loss: 10803.4150390625\n",
      "Epoch: 164, Batch number: 12, Loss: 10796.5830078125\n",
      "Epoch: 165, Batch number: 36, Loss: 10621.78515625\n",
      "Epoch: 166, Batch number: 60, Loss: 10753.3916015625\n",
      "Epoch: 168, Batch number: 8, Loss: 10460.0048828125\n",
      "Epoch: 169, Batch number: 32, Loss: 10433.94921875\n",
      "Epoch: 170, Batch number: 56, Loss: 10818.66796875\n",
      "Epoch: 172, Batch number: 4, Loss: 10413.5068359375\n",
      "Epoch: 173, Batch number: 28, Loss: 10611.2666015625\n",
      "Epoch: 174, Batch number: 52, Loss: 10736.76953125\n",
      "Epoch: 176, Batch number: 0, Loss: 10468.92578125\n",
      "Epoch: 177, Batch number: 24, Loss: 10458.201171875\n",
      "Epoch: 178, Batch number: 48, Loss: 10753.642578125\n",
      "Epoch: 179, Batch number: 72, Loss: 10477.369140625\n",
      "Epoch: 181, Batch number: 20, Loss: 10403.1796875\n",
      "Epoch: 182, Batch number: 44, Loss: 10832.9794921875\n",
      "Epoch: 183, Batch number: 68, Loss: 10561.4248046875\n",
      "Epoch: 185, Batch number: 16, Loss: 10574.3056640625\n",
      "Epoch: 186, Batch number: 40, Loss: 10863.9931640625\n",
      "Epoch: 187, Batch number: 64, Loss: 10340.474609375\n",
      "Epoch: 189, Batch number: 12, Loss: 10271.255859375\n",
      "Epoch: 190, Batch number: 36, Loss: 10665.533203125\n",
      "Epoch: 191, Batch number: 60, Loss: 10295.095703125\n",
      "Epoch: 193, Batch number: 8, Loss: 10568.4130859375\n",
      "Epoch: 194, Batch number: 32, Loss: 10531.9072265625\n",
      "Epoch: 195, Batch number: 56, Loss: 10526.259765625\n",
      "Epoch: 197, Batch number: 4, Loss: 10353.7802734375\n",
      "Epoch: 198, Batch number: 28, Loss: 10556.8193359375\n",
      "Epoch: 199, Batch number: 52, Loss: 11005.2138671875\n",
      "Epoch: 201, Batch number: 0, Loss: 10178.0712890625\n",
      "Epoch: 202, Batch number: 24, Loss: 10143.4560546875\n",
      "Epoch: 203, Batch number: 48, Loss: 10665.1220703125\n",
      "Epoch: 204, Batch number: 72, Loss: 10347.7587890625\n",
      "Epoch: 206, Batch number: 20, Loss: 10318.638671875\n",
      "Epoch: 207, Batch number: 44, Loss: 10344.158203125\n",
      "Epoch: 208, Batch number: 68, Loss: 10297.3779296875\n",
      "Epoch: 210, Batch number: 16, Loss: 10644.2958984375\n",
      "Epoch: 211, Batch number: 40, Loss: 10917.646484375\n",
      "Epoch: 212, Batch number: 64, Loss: 10340.6357421875\n",
      "Epoch: 214, Batch number: 12, Loss: 10262.849609375\n",
      "Epoch: 215, Batch number: 36, Loss: 10350.1181640625\n",
      "Epoch: 216, Batch number: 60, Loss: 10102.701171875\n",
      "Epoch: 218, Batch number: 8, Loss: 10489.671875\n",
      "Epoch: 219, Batch number: 32, Loss: 10407.5087890625\n",
      "Epoch: 220, Batch number: 56, Loss: 10274.0771484375\n",
      "Epoch: 222, Batch number: 4, Loss: 10606.0361328125\n",
      "Epoch: 223, Batch number: 28, Loss: 10332.7978515625\n",
      "Epoch: 224, Batch number: 52, Loss: 10432.0166015625\n",
      "Epoch: 226, Batch number: 0, Loss: 10314.4638671875\n",
      "Epoch: 227, Batch number: 24, Loss: 10488.3154296875\n",
      "Epoch: 228, Batch number: 48, Loss: 10627.9130859375\n",
      "Epoch: 229, Batch number: 72, Loss: 10815.7158203125\n",
      "Epoch: 231, Batch number: 20, Loss: 10640.8642578125\n",
      "Epoch: 232, Batch number: 44, Loss: 10533.017578125\n",
      "Epoch: 233, Batch number: 68, Loss: 10280.3271484375\n",
      "Epoch: 235, Batch number: 16, Loss: 10465.234375\n",
      "Epoch: 236, Batch number: 40, Loss: 10834.21875\n",
      "Epoch: 237, Batch number: 64, Loss: 10413.59765625\n",
      "Epoch: 239, Batch number: 12, Loss: 10556.0908203125\n",
      "Epoch: 240, Batch number: 36, Loss: 10422.73828125\n",
      "Epoch: 241, Batch number: 60, Loss: 10474.5146484375\n",
      "Epoch: 243, Batch number: 8, Loss: 10460.6865234375\n",
      "Epoch: 244, Batch number: 32, Loss: 10695.3740234375\n",
      "Epoch: 245, Batch number: 56, Loss: 10563.09765625\n",
      "Epoch: 247, Batch number: 4, Loss: 10738.4794921875\n",
      "Epoch: 248, Batch number: 28, Loss: 10285.6162109375\n",
      "Epoch: 249, Batch number: 52, Loss: 10601.310546875\n",
      "Epoch: 251, Batch number: 0, Loss: 10211.201171875\n",
      "Epoch: 252, Batch number: 24, Loss: 10275.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253, Batch number: 48, Loss: 10612.76171875\n",
      "Epoch: 254, Batch number: 72, Loss: 10762.8798828125\n",
      "Epoch: 256, Batch number: 20, Loss: 10708.3583984375\n",
      "Epoch: 257, Batch number: 44, Loss: 10855.720703125\n",
      "Epoch: 258, Batch number: 68, Loss: 10625.4775390625\n",
      "Epoch: 260, Batch number: 16, Loss: 10249.2783203125\n",
      "Epoch: 261, Batch number: 40, Loss: 10295.3486328125\n",
      "Epoch: 262, Batch number: 64, Loss: 10496.6396484375\n",
      "Epoch: 264, Batch number: 12, Loss: 10317.2236328125\n",
      "Epoch: 265, Batch number: 36, Loss: 10470.765625\n",
      "Epoch: 266, Batch number: 60, Loss: 10321.064453125\n",
      "Epoch: 268, Batch number: 8, Loss: 10239.1181640625\n",
      "Epoch: 269, Batch number: 32, Loss: 10507.0625\n",
      "Epoch: 270, Batch number: 56, Loss: 10424.455078125\n",
      "Epoch: 272, Batch number: 4, Loss: 10508.064453125\n",
      "Epoch: 273, Batch number: 28, Loss: 10501.9375\n",
      "Epoch: 274, Batch number: 52, Loss: 10704.9755859375\n",
      "Epoch: 276, Batch number: 0, Loss: 10268.794921875\n",
      "Epoch: 277, Batch number: 24, Loss: 10616.419921875\n",
      "Epoch: 278, Batch number: 48, Loss: 10896.146484375\n",
      "Epoch: 279, Batch number: 72, Loss: 10759.1953125\n",
      "Epoch: 281, Batch number: 20, Loss: 10452.689453125\n",
      "Epoch: 282, Batch number: 44, Loss: 10692.3583984375\n",
      "Epoch: 283, Batch number: 68, Loss: 10540.4013671875\n",
      "Epoch: 285, Batch number: 16, Loss: 10199.533203125\n",
      "Epoch: 286, Batch number: 40, Loss: 10379.2294921875\n",
      "Epoch: 287, Batch number: 64, Loss: 10603.419921875\n",
      "Epoch: 289, Batch number: 12, Loss: 10486.6826171875\n",
      "Epoch: 290, Batch number: 36, Loss: 10534.36328125\n",
      "Epoch: 291, Batch number: 60, Loss: 10375.5654296875\n",
      "Epoch: 293, Batch number: 8, Loss: 10135.0693359375\n",
      "Epoch: 294, Batch number: 32, Loss: 10365.3291015625\n",
      "Epoch: 295, Batch number: 56, Loss: 10789.2041015625\n",
      "Epoch: 297, Batch number: 4, Loss: 10219.9326171875\n",
      "Epoch: 298, Batch number: 28, Loss: 10442.029296875\n",
      "Epoch: 299, Batch number: 52, Loss: 10600.908203125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 21436.822265625\n",
      "Epoch: 2, Batch number: 24, Loss: 19637.314453125\n",
      "Epoch: 3, Batch number: 48, Loss: 17518.87890625\n",
      "Epoch: 4, Batch number: 72, Loss: 17594.908203125\n",
      "Epoch: 6, Batch number: 20, Loss: 16545.01171875\n",
      "Epoch: 7, Batch number: 44, Loss: 15639.978515625\n",
      "Epoch: 8, Batch number: 68, Loss: 15464.9208984375\n",
      "Epoch: 10, Batch number: 16, Loss: 14484.2861328125\n",
      "Epoch: 11, Batch number: 40, Loss: 14003.087890625\n",
      "Epoch: 12, Batch number: 64, Loss: 14024.4033203125\n",
      "Epoch: 14, Batch number: 12, Loss: 13396.337890625\n",
      "Epoch: 15, Batch number: 36, Loss: 13127.2041015625\n",
      "Epoch: 16, Batch number: 60, Loss: 13284.5654296875\n",
      "Epoch: 18, Batch number: 8, Loss: 12516.791015625\n",
      "Epoch: 19, Batch number: 32, Loss: 12529.9736328125\n",
      "Epoch: 20, Batch number: 56, Loss: 12413.65234375\n",
      "Epoch: 22, Batch number: 4, Loss: 12021.8310546875\n",
      "Epoch: 23, Batch number: 28, Loss: 12281.5830078125\n",
      "Epoch: 24, Batch number: 52, Loss: 12104.4404296875\n",
      "Epoch: 26, Batch number: 0, Loss: 12262.791015625\n",
      "Epoch: 27, Batch number: 24, Loss: 12241.1767578125\n",
      "Epoch: 28, Batch number: 48, Loss: 12111.8984375\n",
      "Epoch: 29, Batch number: 72, Loss: 11773.3837890625\n",
      "Epoch: 31, Batch number: 20, Loss: 11575.2431640625\n",
      "Epoch: 32, Batch number: 44, Loss: 11671.486328125\n",
      "Epoch: 33, Batch number: 68, Loss: 11702.9384765625\n",
      "Epoch: 35, Batch number: 16, Loss: 11371.2529296875\n",
      "Epoch: 36, Batch number: 40, Loss: 11366.5810546875\n",
      "Epoch: 37, Batch number: 64, Loss: 11665.24609375\n",
      "Epoch: 39, Batch number: 12, Loss: 11261.865234375\n",
      "Epoch: 40, Batch number: 36, Loss: 11398.1318359375\n",
      "Epoch: 41, Batch number: 60, Loss: 11512.2490234375\n",
      "Epoch: 43, Batch number: 8, Loss: 11170.0244140625\n",
      "Epoch: 44, Batch number: 32, Loss: 11082.111328125\n",
      "Epoch: 45, Batch number: 56, Loss: 11089.25\n",
      "Epoch: 47, Batch number: 4, Loss: 11046.119140625\n",
      "Epoch: 48, Batch number: 28, Loss: 11090.6455078125\n",
      "Epoch: 49, Batch number: 52, Loss: 11516.3623046875\n",
      "Epoch: 51, Batch number: 0, Loss: 11307.685546875\n",
      "Epoch: 52, Batch number: 24, Loss: 11315.48046875\n",
      "Epoch: 53, Batch number: 48, Loss: 11140.134765625\n",
      "Epoch: 54, Batch number: 72, Loss: 11219.7529296875\n",
      "Epoch: 56, Batch number: 20, Loss: 11006.4541015625\n",
      "Epoch: 57, Batch number: 44, Loss: 11167.2333984375\n",
      "Epoch: 58, Batch number: 68, Loss: 10924.619140625\n",
      "Epoch: 60, Batch number: 16, Loss: 10886.7763671875\n",
      "Epoch: 61, Batch number: 40, Loss: 10918.478515625\n",
      "Epoch: 62, Batch number: 64, Loss: 11053.580078125\n",
      "Epoch: 64, Batch number: 12, Loss: 10740.5224609375\n",
      "Epoch: 65, Batch number: 36, Loss: 10798.9873046875\n",
      "Epoch: 66, Batch number: 60, Loss: 11077.6787109375\n",
      "Epoch: 68, Batch number: 8, Loss: 10928.3408203125\n",
      "Epoch: 69, Batch number: 32, Loss: 10393.2373046875\n",
      "Epoch: 70, Batch number: 56, Loss: 11008.560546875\n",
      "Epoch: 72, Batch number: 4, Loss: 10859.15625\n",
      "Epoch: 73, Batch number: 28, Loss: 10843.865234375\n",
      "Epoch: 74, Batch number: 52, Loss: 11046.859375\n",
      "Epoch: 76, Batch number: 0, Loss: 10562.32421875\n",
      "Epoch: 77, Batch number: 24, Loss: 10852.1767578125\n",
      "Epoch: 78, Batch number: 48, Loss: 11172.8955078125\n",
      "Epoch: 79, Batch number: 72, Loss: 11024.978515625\n",
      "Epoch: 81, Batch number: 20, Loss: 10538.4755859375\n",
      "Epoch: 82, Batch number: 44, Loss: 10908.9423828125\n",
      "Epoch: 83, Batch number: 68, Loss: 11258.6005859375\n",
      "Epoch: 85, Batch number: 16, Loss: 10833.697265625\n",
      "Epoch: 86, Batch number: 40, Loss: 10605.0009765625\n",
      "Epoch: 87, Batch number: 64, Loss: 10545.0029296875\n",
      "Epoch: 89, Batch number: 12, Loss: 10900.7763671875\n",
      "Epoch: 90, Batch number: 36, Loss: 10840.615234375\n",
      "Epoch: 91, Batch number: 60, Loss: 10993.1201171875\n",
      "Epoch: 93, Batch number: 8, Loss: 10564.3037109375\n",
      "Epoch: 94, Batch number: 32, Loss: 10588.73046875\n",
      "Epoch: 95, Batch number: 56, Loss: 10804.076171875\n",
      "Epoch: 97, Batch number: 4, Loss: 10816.125\n",
      "Epoch: 98, Batch number: 28, Loss: 10765.654296875\n",
      "Epoch: 99, Batch number: 52, Loss: 10701.5888671875\n",
      "Epoch: 101, Batch number: 0, Loss: 10517.5615234375\n",
      "Epoch: 102, Batch number: 24, Loss: 10623.9150390625\n",
      "Epoch: 103, Batch number: 48, Loss: 10577.533203125\n",
      "Epoch: 104, Batch number: 72, Loss: 10715.0390625\n",
      "Epoch: 106, Batch number: 20, Loss: 10717.578125\n",
      "Epoch: 107, Batch number: 44, Loss: 10737.591796875\n",
      "Epoch: 108, Batch number: 68, Loss: 10862.2939453125\n",
      "Epoch: 110, Batch number: 16, Loss: 10500.375\n",
      "Epoch: 111, Batch number: 40, Loss: 10233.6318359375\n",
      "Epoch: 112, Batch number: 64, Loss: 10404.40625\n",
      "Epoch: 114, Batch number: 12, Loss: 10401.80859375\n",
      "Epoch: 115, Batch number: 36, Loss: 10727.7900390625\n",
      "Epoch: 116, Batch number: 60, Loss: 10475.1025390625\n",
      "Epoch: 118, Batch number: 8, Loss: 10796.5517578125\n",
      "Epoch: 119, Batch number: 32, Loss: 10945.19140625\n",
      "Epoch: 120, Batch number: 56, Loss: 10534.6142578125\n",
      "Epoch: 122, Batch number: 4, Loss: 10743.37109375\n",
      "Epoch: 123, Batch number: 28, Loss: 10475.8154296875\n",
      "Epoch: 124, Batch number: 52, Loss: 10782.49609375\n",
      "Epoch: 126, Batch number: 0, Loss: 10714.4697265625\n",
      "Epoch: 127, Batch number: 24, Loss: 10552.0302734375\n",
      "Epoch: 128, Batch number: 48, Loss: 10796.63671875\n",
      "Epoch: 129, Batch number: 72, Loss: 10832.8779296875\n",
      "Epoch: 131, Batch number: 20, Loss: 10429.666015625\n",
      "Epoch: 132, Batch number: 44, Loss: 10719.6767578125\n",
      "Epoch: 133, Batch number: 68, Loss: 10809.365234375\n",
      "Epoch: 135, Batch number: 16, Loss: 10354.8330078125\n",
      "Epoch: 136, Batch number: 40, Loss: 10703.833984375\n",
      "Epoch: 137, Batch number: 64, Loss: 10683.9052734375\n",
      "Epoch: 139, Batch number: 12, Loss: 10353.4013671875\n",
      "Epoch: 140, Batch number: 36, Loss: 10436.197265625\n",
      "Epoch: 141, Batch number: 60, Loss: 10556.6484375\n",
      "Epoch: 143, Batch number: 8, Loss: 10610.791015625\n",
      "Epoch: 144, Batch number: 32, Loss: 10624.095703125\n",
      "Epoch: 145, Batch number: 56, Loss: 11097.4599609375\n",
      "Epoch: 147, Batch number: 4, Loss: 10406.87890625\n",
      "Epoch: 148, Batch number: 28, Loss: 10605.2890625\n",
      "Epoch: 149, Batch number: 52, Loss: 10571.177734375\n",
      "Epoch: 151, Batch number: 0, Loss: 10235.373046875\n",
      "Epoch: 152, Batch number: 24, Loss: 10342.353515625\n",
      "Epoch: 153, Batch number: 48, Loss: 10268.9892578125\n",
      "Epoch: 154, Batch number: 72, Loss: 10831.720703125\n",
      "Epoch: 156, Batch number: 20, Loss: 10556.021484375\n",
      "Epoch: 157, Batch number: 44, Loss: 10521.7373046875\n",
      "Epoch: 158, Batch number: 68, Loss: 10480.2705078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Batch number: 16, Loss: 10382.34375\n",
      "Epoch: 161, Batch number: 40, Loss: 10821.2314453125\n",
      "Epoch: 162, Batch number: 64, Loss: 10276.1318359375\n",
      "Epoch: 164, Batch number: 12, Loss: 10609.1572265625\n",
      "Epoch: 165, Batch number: 36, Loss: 10598.97265625\n",
      "Epoch: 166, Batch number: 60, Loss: 10664.578125\n",
      "Epoch: 168, Batch number: 8, Loss: 10454.0751953125\n",
      "Epoch: 169, Batch number: 32, Loss: 10586.8017578125\n",
      "Epoch: 170, Batch number: 56, Loss: 10736.8916015625\n",
      "Epoch: 172, Batch number: 4, Loss: 10018.2109375\n",
      "Epoch: 173, Batch number: 28, Loss: 10556.7373046875\n",
      "Epoch: 174, Batch number: 52, Loss: 10582.1357421875\n",
      "Epoch: 176, Batch number: 0, Loss: 10218.6123046875\n",
      "Epoch: 177, Batch number: 24, Loss: 10334.2412109375\n",
      "Epoch: 178, Batch number: 48, Loss: 10320.6962890625\n",
      "Epoch: 179, Batch number: 72, Loss: 10302.521484375\n",
      "Epoch: 181, Batch number: 20, Loss: 10662.283203125\n",
      "Epoch: 182, Batch number: 44, Loss: 10508.140625\n",
      "Epoch: 183, Batch number: 68, Loss: 10527.8232421875\n",
      "Epoch: 185, Batch number: 16, Loss: 10476.5\n",
      "Epoch: 186, Batch number: 40, Loss: 10508.4150390625\n",
      "Epoch: 187, Batch number: 64, Loss: 10425.5751953125\n",
      "Epoch: 189, Batch number: 12, Loss: 10245.669921875\n",
      "Epoch: 190, Batch number: 36, Loss: 10180.15234375\n",
      "Epoch: 191, Batch number: 60, Loss: 10864.5419921875\n",
      "Epoch: 193, Batch number: 8, Loss: 10484.7919921875\n",
      "Epoch: 194, Batch number: 32, Loss: 10492.01953125\n",
      "Epoch: 195, Batch number: 56, Loss: 10867.00390625\n",
      "Epoch: 197, Batch number: 4, Loss: 10426.8671875\n",
      "Epoch: 198, Batch number: 28, Loss: 10459.892578125\n",
      "Epoch: 199, Batch number: 52, Loss: 10233.9423828125\n",
      "Epoch: 201, Batch number: 0, Loss: 10512.029296875\n",
      "Epoch: 202, Batch number: 24, Loss: 10402.6357421875\n",
      "Epoch: 203, Batch number: 48, Loss: 10396.3271484375\n",
      "Epoch: 204, Batch number: 72, Loss: 10441.9248046875\n",
      "Epoch: 206, Batch number: 20, Loss: 10589.7353515625\n",
      "Epoch: 207, Batch number: 44, Loss: 10492.3720703125\n",
      "Epoch: 208, Batch number: 68, Loss: 10582.1279296875\n",
      "Epoch: 210, Batch number: 16, Loss: 10433.3662109375\n",
      "Epoch: 211, Batch number: 40, Loss: 10200.46875\n",
      "Epoch: 212, Batch number: 64, Loss: 10520.0458984375\n",
      "Epoch: 214, Batch number: 12, Loss: 10347.953125\n",
      "Epoch: 215, Batch number: 36, Loss: 10238.6962890625\n",
      "Epoch: 216, Batch number: 60, Loss: 10474.240234375\n",
      "Epoch: 218, Batch number: 8, Loss: 10080.48828125\n",
      "Epoch: 219, Batch number: 32, Loss: 10855.744140625\n",
      "Epoch: 220, Batch number: 56, Loss: 10626.61328125\n",
      "Epoch: 222, Batch number: 4, Loss: 10146.4921875\n",
      "Epoch: 223, Batch number: 28, Loss: 10604.337890625\n",
      "Epoch: 224, Batch number: 52, Loss: 10296.693359375\n",
      "Epoch: 226, Batch number: 0, Loss: 10213.927734375\n",
      "Epoch: 227, Batch number: 24, Loss: 10614.2568359375\n",
      "Epoch: 228, Batch number: 48, Loss: 10742.76171875\n",
      "Epoch: 229, Batch number: 72, Loss: 10845.2421875\n",
      "Epoch: 231, Batch number: 20, Loss: 10321.6806640625\n",
      "Epoch: 232, Batch number: 44, Loss: 10521.5380859375\n",
      "Epoch: 233, Batch number: 68, Loss: 10701.4638671875\n",
      "Epoch: 235, Batch number: 16, Loss: 10306.8408203125\n",
      "Epoch: 236, Batch number: 40, Loss: 10722.923828125\n",
      "Epoch: 237, Batch number: 64, Loss: 10752.7763671875\n",
      "Epoch: 239, Batch number: 12, Loss: 10536.4384765625\n",
      "Epoch: 240, Batch number: 36, Loss: 10291.44921875\n",
      "Epoch: 241, Batch number: 60, Loss: 10233.5576171875\n",
      "Epoch: 243, Batch number: 8, Loss: 10579.193359375\n",
      "Epoch: 244, Batch number: 32, Loss: 10613.0869140625\n",
      "Epoch: 245, Batch number: 56, Loss: 10617.7021484375\n",
      "Epoch: 247, Batch number: 4, Loss: 9977.1171875\n",
      "Epoch: 248, Batch number: 28, Loss: 10295.3359375\n",
      "Epoch: 249, Batch number: 52, Loss: 11001.15625\n",
      "Epoch: 251, Batch number: 0, Loss: 10154.142578125\n",
      "Epoch: 252, Batch number: 24, Loss: 10195.4208984375\n",
      "Epoch: 253, Batch number: 48, Loss: 10722.8583984375\n",
      "Epoch: 254, Batch number: 72, Loss: 10576.80859375\n",
      "Epoch: 256, Batch number: 20, Loss: 10332.70703125\n",
      "Epoch: 257, Batch number: 44, Loss: 10539.4052734375\n",
      "Epoch: 258, Batch number: 68, Loss: 10614.5244140625\n",
      "Epoch: 260, Batch number: 16, Loss: 10494.2080078125\n",
      "Epoch: 261, Batch number: 40, Loss: 10520.841796875\n",
      "Epoch: 262, Batch number: 64, Loss: 10888.908203125\n",
      "Epoch: 264, Batch number: 12, Loss: 10289.078125\n",
      "Epoch: 265, Batch number: 36, Loss: 10524.98046875\n",
      "Epoch: 266, Batch number: 60, Loss: 10701.0087890625\n",
      "Epoch: 268, Batch number: 8, Loss: 10088.4208984375\n",
      "Epoch: 269, Batch number: 32, Loss: 10260.4794921875\n",
      "Epoch: 270, Batch number: 56, Loss: 10254.2734375\n",
      "Epoch: 272, Batch number: 4, Loss: 10629.798828125\n",
      "Epoch: 273, Batch number: 28, Loss: 10601.3818359375\n",
      "Epoch: 274, Batch number: 52, Loss: 10590.9892578125\n",
      "Epoch: 276, Batch number: 0, Loss: 10326.6171875\n",
      "Epoch: 277, Batch number: 24, Loss: 10516.2216796875\n",
      "Epoch: 278, Batch number: 48, Loss: 10443.97265625\n",
      "Epoch: 279, Batch number: 72, Loss: 10339.6494140625\n",
      "Epoch: 281, Batch number: 20, Loss: 10619.4296875\n",
      "Epoch: 282, Batch number: 44, Loss: 10458.0830078125\n",
      "Epoch: 283, Batch number: 68, Loss: 10997.8330078125\n",
      "Epoch: 285, Batch number: 16, Loss: 10254.521484375\n",
      "Epoch: 286, Batch number: 40, Loss: 10684.3916015625\n",
      "Epoch: 287, Batch number: 64, Loss: 10249.9560546875\n",
      "Epoch: 289, Batch number: 12, Loss: 10273.328125\n",
      "Epoch: 290, Batch number: 36, Loss: 10572.9951171875\n",
      "Epoch: 291, Batch number: 60, Loss: 10314.4921875\n",
      "Epoch: 293, Batch number: 8, Loss: 10323.349609375\n",
      "Epoch: 294, Batch number: 32, Loss: 10309.6064453125\n",
      "Epoch: 295, Batch number: 56, Loss: 10429.306640625\n",
      "Epoch: 297, Batch number: 4, Loss: 10041.29296875\n",
      "Epoch: 298, Batch number: 28, Loss: 10391.9580078125\n",
      "Epoch: 299, Batch number: 52, Loss: 10744.6826171875\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 21584.052734375\n",
      "Epoch: 2, Batch number: 24, Loss: 19035.28125\n",
      "Epoch: 3, Batch number: 48, Loss: 17603.705078125\n",
      "Epoch: 4, Batch number: 72, Loss: 16751.173828125\n",
      "Epoch: 6, Batch number: 20, Loss: 15888.7548828125\n",
      "Epoch: 7, Batch number: 44, Loss: 14966.6328125\n",
      "Epoch: 8, Batch number: 68, Loss: 14584.2294921875\n",
      "Epoch: 10, Batch number: 16, Loss: 13535.67578125\n",
      "Epoch: 11, Batch number: 40, Loss: 13587.328125\n",
      "Epoch: 12, Batch number: 64, Loss: 13277.162109375\n",
      "Epoch: 14, Batch number: 12, Loss: 12381.6748046875\n",
      "Epoch: 15, Batch number: 36, Loss: 12775.6865234375\n",
      "Epoch: 16, Batch number: 60, Loss: 12326.060546875\n",
      "Epoch: 18, Batch number: 8, Loss: 11890.2578125\n",
      "Epoch: 19, Batch number: 32, Loss: 12249.0048828125\n",
      "Epoch: 20, Batch number: 56, Loss: 11909.470703125\n",
      "Epoch: 22, Batch number: 4, Loss: 11778.091796875\n",
      "Epoch: 23, Batch number: 28, Loss: 11892.4501953125\n",
      "Epoch: 24, Batch number: 52, Loss: 11794.1630859375\n",
      "Epoch: 26, Batch number: 0, Loss: 11346.431640625\n",
      "Epoch: 27, Batch number: 24, Loss: 11834.505859375\n",
      "Epoch: 28, Batch number: 48, Loss: 11618.8701171875\n",
      "Epoch: 29, Batch number: 72, Loss: 11624.947265625\n",
      "Epoch: 31, Batch number: 20, Loss: 11105.1259765625\n",
      "Epoch: 32, Batch number: 44, Loss: 11287.234375\n",
      "Epoch: 33, Batch number: 68, Loss: 11517.1689453125\n",
      "Epoch: 35, Batch number: 16, Loss: 11021.76171875\n",
      "Epoch: 36, Batch number: 40, Loss: 11557.994140625\n",
      "Epoch: 37, Batch number: 64, Loss: 11370.8330078125\n",
      "Epoch: 39, Batch number: 12, Loss: 11184.966796875\n",
      "Epoch: 40, Batch number: 36, Loss: 10960.744140625\n",
      "Epoch: 41, Batch number: 60, Loss: 10711.943359375\n",
      "Epoch: 43, Batch number: 8, Loss: 11330.984375\n",
      "Epoch: 44, Batch number: 32, Loss: 11205.40234375\n",
      "Epoch: 45, Batch number: 56, Loss: 10837.9169921875\n",
      "Epoch: 47, Batch number: 4, Loss: 10549.7861328125\n",
      "Epoch: 48, Batch number: 28, Loss: 10951.126953125\n",
      "Epoch: 49, Batch number: 52, Loss: 11090.015625\n",
      "Epoch: 51, Batch number: 0, Loss: 10912.54296875\n",
      "Epoch: 52, Batch number: 24, Loss: 11258.015625\n",
      "Epoch: 53, Batch number: 48, Loss: 10883.857421875\n",
      "Epoch: 54, Batch number: 72, Loss: 10909.4755859375\n",
      "Epoch: 56, Batch number: 20, Loss: 10925.2763671875\n",
      "Epoch: 57, Batch number: 44, Loss: 10903.251953125\n",
      "Epoch: 58, Batch number: 68, Loss: 11306.052734375\n",
      "Epoch: 60, Batch number: 16, Loss: 10973.1640625\n",
      "Epoch: 61, Batch number: 40, Loss: 10781.2724609375\n",
      "Epoch: 62, Batch number: 64, Loss: 11104.9873046875\n",
      "Epoch: 64, Batch number: 12, Loss: 10628.99609375\n",
      "Epoch: 65, Batch number: 36, Loss: 11185.89453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Batch number: 60, Loss: 10890.7919921875\n",
      "Epoch: 68, Batch number: 8, Loss: 10402.3271484375\n",
      "Epoch: 69, Batch number: 32, Loss: 10561.6650390625\n",
      "Epoch: 70, Batch number: 56, Loss: 10653.2158203125\n",
      "Epoch: 72, Batch number: 4, Loss: 10394.9287109375\n",
      "Epoch: 73, Batch number: 28, Loss: 10761.4375\n",
      "Epoch: 74, Batch number: 52, Loss: 10902.21875\n",
      "Epoch: 76, Batch number: 0, Loss: 10589.751953125\n",
      "Epoch: 77, Batch number: 24, Loss: 10310.82421875\n",
      "Epoch: 78, Batch number: 48, Loss: 10637.6923828125\n",
      "Epoch: 79, Batch number: 72, Loss: 11062.0927734375\n",
      "Epoch: 81, Batch number: 20, Loss: 10678.5703125\n",
      "Epoch: 82, Batch number: 44, Loss: 10621.7451171875\n",
      "Epoch: 83, Batch number: 68, Loss: 11310.8447265625\n",
      "Epoch: 85, Batch number: 16, Loss: 10409.6513671875\n",
      "Epoch: 86, Batch number: 40, Loss: 10403.0400390625\n",
      "Epoch: 87, Batch number: 64, Loss: 10611.21484375\n",
      "Epoch: 89, Batch number: 12, Loss: 10127.8017578125\n",
      "Epoch: 90, Batch number: 36, Loss: 10566.6865234375\n",
      "Epoch: 91, Batch number: 60, Loss: 10719.93359375\n",
      "Epoch: 93, Batch number: 8, Loss: 10420.6748046875\n",
      "Epoch: 94, Batch number: 32, Loss: 10543.5791015625\n",
      "Epoch: 95, Batch number: 56, Loss: 10994.0517578125\n",
      "Epoch: 97, Batch number: 4, Loss: 10402.310546875\n",
      "Epoch: 98, Batch number: 28, Loss: 10667.8466796875\n",
      "Epoch: 99, Batch number: 52, Loss: 10957.7783203125\n",
      "Epoch: 101, Batch number: 0, Loss: 10732.1728515625\n",
      "Epoch: 102, Batch number: 24, Loss: 10505.978515625\n",
      "Epoch: 103, Batch number: 48, Loss: 10847.435546875\n",
      "Epoch: 104, Batch number: 72, Loss: 10842.9970703125\n",
      "Epoch: 106, Batch number: 20, Loss: 10465.31640625\n",
      "Epoch: 107, Batch number: 44, Loss: 10587.1064453125\n",
      "Epoch: 108, Batch number: 68, Loss: 10616.2890625\n",
      "Epoch: 110, Batch number: 16, Loss: 10639.595703125\n",
      "Epoch: 111, Batch number: 40, Loss: 10413.83984375\n",
      "Epoch: 112, Batch number: 64, Loss: 10376.544921875\n",
      "Epoch: 114, Batch number: 12, Loss: 10417.0791015625\n",
      "Epoch: 115, Batch number: 36, Loss: 10688.5263671875\n",
      "Epoch: 116, Batch number: 60, Loss: 10540.3583984375\n",
      "Epoch: 118, Batch number: 8, Loss: 10572.2783203125\n",
      "Epoch: 119, Batch number: 32, Loss: 10611.99609375\n",
      "Epoch: 120, Batch number: 56, Loss: 11181.9345703125\n",
      "Epoch: 122, Batch number: 4, Loss: 10549.1171875\n",
      "Epoch: 123, Batch number: 28, Loss: 10317.599609375\n",
      "Epoch: 124, Batch number: 52, Loss: 10947.2470703125\n",
      "Epoch: 126, Batch number: 0, Loss: 10716.5498046875\n",
      "Epoch: 127, Batch number: 24, Loss: 10658.794921875\n",
      "Epoch: 128, Batch number: 48, Loss: 10373.3369140625\n",
      "Epoch: 129, Batch number: 72, Loss: 10590.7978515625\n",
      "Epoch: 131, Batch number: 20, Loss: 10440.9150390625\n",
      "Epoch: 132, Batch number: 44, Loss: 10059.3876953125\n",
      "Epoch: 133, Batch number: 68, Loss: 10740.240234375\n",
      "Epoch: 135, Batch number: 16, Loss: 10595.3798828125\n",
      "Epoch: 136, Batch number: 40, Loss: 10496.4697265625\n",
      "Epoch: 137, Batch number: 64, Loss: 10746.3134765625\n",
      "Epoch: 139, Batch number: 12, Loss: 10400.369140625\n",
      "Epoch: 140, Batch number: 36, Loss: 10983.376953125\n",
      "Epoch: 141, Batch number: 60, Loss: 10452.25\n",
      "Epoch: 143, Batch number: 8, Loss: 10248.2998046875\n",
      "Epoch: 144, Batch number: 32, Loss: 10430.3046875\n",
      "Epoch: 145, Batch number: 56, Loss: 10750.41796875\n",
      "Epoch: 147, Batch number: 4, Loss: 10280.2734375\n",
      "Epoch: 148, Batch number: 28, Loss: 10477.9765625\n",
      "Epoch: 149, Batch number: 52, Loss: 10625.6201171875\n",
      "Epoch: 151, Batch number: 0, Loss: 10069.5693359375\n",
      "Epoch: 152, Batch number: 24, Loss: 10433.1904296875\n",
      "Epoch: 153, Batch number: 48, Loss: 10298.373046875\n",
      "Epoch: 154, Batch number: 72, Loss: 10471.8466796875\n",
      "Epoch: 156, Batch number: 20, Loss: 10387.7080078125\n",
      "Epoch: 157, Batch number: 44, Loss: 10556.8173828125\n",
      "Epoch: 158, Batch number: 68, Loss: 10771.6123046875\n",
      "Epoch: 160, Batch number: 16, Loss: 10343.74609375\n",
      "Epoch: 161, Batch number: 40, Loss: 10646.3203125\n",
      "Epoch: 162, Batch number: 64, Loss: 10610.4541015625\n",
      "Epoch: 164, Batch number: 12, Loss: 10138.546875\n",
      "Epoch: 165, Batch number: 36, Loss: 10399.708984375\n",
      "Epoch: 166, Batch number: 60, Loss: 10444.07421875\n",
      "Epoch: 168, Batch number: 8, Loss: 10687.466796875\n",
      "Epoch: 169, Batch number: 32, Loss: 10629.3662109375\n",
      "Epoch: 170, Batch number: 56, Loss: 10719.908203125\n",
      "Epoch: 172, Batch number: 4, Loss: 10509.69140625\n",
      "Epoch: 173, Batch number: 28, Loss: 10273.3046875\n",
      "Epoch: 174, Batch number: 52, Loss: 10650.5498046875\n",
      "Epoch: 176, Batch number: 0, Loss: 10628.619140625\n",
      "Epoch: 177, Batch number: 24, Loss: 10308.875\n",
      "Epoch: 178, Batch number: 48, Loss: 10338.9013671875\n",
      "Epoch: 179, Batch number: 72, Loss: 10851.189453125\n",
      "Epoch: 181, Batch number: 20, Loss: 10462.255859375\n",
      "Epoch: 182, Batch number: 44, Loss: 10565.02734375\n",
      "Epoch: 183, Batch number: 68, Loss: 10694.0126953125\n",
      "Epoch: 185, Batch number: 16, Loss: 10360.4111328125\n",
      "Epoch: 186, Batch number: 40, Loss: 10537.9052734375\n",
      "Epoch: 187, Batch number: 64, Loss: 10772.06640625\n",
      "Epoch: 189, Batch number: 12, Loss: 10104.14453125\n",
      "Epoch: 190, Batch number: 36, Loss: 10532.2041015625\n",
      "Epoch: 191, Batch number: 60, Loss: 10576.4150390625\n",
      "Epoch: 193, Batch number: 8, Loss: 10519.1240234375\n",
      "Epoch: 194, Batch number: 32, Loss: 10480.3251953125\n",
      "Epoch: 195, Batch number: 56, Loss: 10481.2392578125\n",
      "Epoch: 197, Batch number: 4, Loss: 10216.1591796875\n",
      "Epoch: 198, Batch number: 28, Loss: 10677.498046875\n",
      "Epoch: 199, Batch number: 52, Loss: 11106.8173828125\n",
      "Epoch: 201, Batch number: 0, Loss: 10775.4375\n",
      "Epoch: 202, Batch number: 24, Loss: 10709.392578125\n",
      "Epoch: 203, Batch number: 48, Loss: 10775.064453125\n",
      "Epoch: 204, Batch number: 72, Loss: 10732.4365234375\n",
      "Epoch: 206, Batch number: 20, Loss: 10573.72265625\n",
      "Epoch: 207, Batch number: 44, Loss: 10494.552734375\n",
      "Epoch: 208, Batch number: 68, Loss: 11084.9013671875\n",
      "Epoch: 210, Batch number: 16, Loss: 10292.732421875\n",
      "Epoch: 211, Batch number: 40, Loss: 10479.6982421875\n",
      "Epoch: 212, Batch number: 64, Loss: 10467.7841796875\n",
      "Epoch: 214, Batch number: 12, Loss: 10583.1103515625\n",
      "Epoch: 215, Batch number: 36, Loss: 10660.759765625\n",
      "Epoch: 216, Batch number: 60, Loss: 10428.3681640625\n",
      "Epoch: 218, Batch number: 8, Loss: 10459.7783203125\n",
      "Epoch: 219, Batch number: 32, Loss: 10332.8056640625\n",
      "Epoch: 220, Batch number: 56, Loss: 10319.29296875\n",
      "Epoch: 222, Batch number: 4, Loss: 10642.66796875\n",
      "Epoch: 223, Batch number: 28, Loss: 10489.78515625\n",
      "Epoch: 224, Batch number: 52, Loss: 10609.98046875\n",
      "Epoch: 226, Batch number: 0, Loss: 10202.7255859375\n",
      "Epoch: 227, Batch number: 24, Loss: 10486.3330078125\n",
      "Epoch: 228, Batch number: 48, Loss: 10123.31640625\n",
      "Epoch: 229, Batch number: 72, Loss: 10504.06640625\n",
      "Epoch: 231, Batch number: 20, Loss: 10229.14453125\n",
      "Epoch: 232, Batch number: 44, Loss: 10358.8466796875\n",
      "Epoch: 233, Batch number: 68, Loss: 10393.4404296875\n",
      "Epoch: 235, Batch number: 16, Loss: 10241.6220703125\n",
      "Epoch: 236, Batch number: 40, Loss: 11113.86328125\n",
      "Epoch: 237, Batch number: 64, Loss: 10590.2041015625\n",
      "Epoch: 239, Batch number: 12, Loss: 10713.8291015625\n",
      "Epoch: 240, Batch number: 36, Loss: 10724.9248046875\n",
      "Epoch: 241, Batch number: 60, Loss: 10330.8935546875\n",
      "Epoch: 243, Batch number: 8, Loss: 10647.1787109375\n",
      "Epoch: 244, Batch number: 32, Loss: 10163.9404296875\n",
      "Epoch: 245, Batch number: 56, Loss: 10869.912109375\n",
      "Epoch: 247, Batch number: 4, Loss: 10956.6484375\n",
      "Epoch: 248, Batch number: 28, Loss: 10517.0419921875\n",
      "Epoch: 249, Batch number: 52, Loss: 10844.103515625\n",
      "Epoch: 251, Batch number: 0, Loss: 10534.736328125\n",
      "Epoch: 252, Batch number: 24, Loss: 10228.6865234375\n",
      "Epoch: 253, Batch number: 48, Loss: 10218.6806640625\n",
      "Epoch: 254, Batch number: 72, Loss: 10722.0107421875\n",
      "Epoch: 256, Batch number: 20, Loss: 10598.384765625\n",
      "Epoch: 257, Batch number: 44, Loss: 10655.1689453125\n",
      "Epoch: 258, Batch number: 68, Loss: 10686.880859375\n",
      "Epoch: 260, Batch number: 16, Loss: 10601.890625\n",
      "Epoch: 261, Batch number: 40, Loss: 10365.4599609375\n",
      "Epoch: 262, Batch number: 64, Loss: 10622.3671875\n",
      "Epoch: 264, Batch number: 12, Loss: 9937.5859375\n",
      "Epoch: 265, Batch number: 36, Loss: 10785.7890625\n",
      "Epoch: 266, Batch number: 60, Loss: 10887.3857421875\n",
      "Epoch: 268, Batch number: 8, Loss: 10317.5107421875\n",
      "Epoch: 269, Batch number: 32, Loss: 10517.556640625\n",
      "Epoch: 270, Batch number: 56, Loss: 10456.4462890625\n",
      "Epoch: 272, Batch number: 4, Loss: 10440.759765625\n",
      "Epoch: 273, Batch number: 28, Loss: 10518.4208984375\n",
      "Epoch: 274, Batch number: 52, Loss: 10712.8623046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276, Batch number: 0, Loss: 10649.076171875\n",
      "Epoch: 277, Batch number: 24, Loss: 10537.8994140625\n",
      "Epoch: 278, Batch number: 48, Loss: 10498.4365234375\n",
      "Epoch: 279, Batch number: 72, Loss: 10249.0400390625\n",
      "Epoch: 281, Batch number: 20, Loss: 10538.8447265625\n",
      "Epoch: 282, Batch number: 44, Loss: 10841.5556640625\n",
      "Epoch: 283, Batch number: 68, Loss: 10788.2919921875\n",
      "Epoch: 285, Batch number: 16, Loss: 10343.982421875\n",
      "Epoch: 286, Batch number: 40, Loss: 10969.4169921875\n",
      "Epoch: 287, Batch number: 64, Loss: 10391.2236328125\n",
      "Epoch: 289, Batch number: 12, Loss: 10200.966796875\n",
      "Epoch: 290, Batch number: 36, Loss: 10595.6201171875\n",
      "Epoch: 291, Batch number: 60, Loss: 10836.64453125\n",
      "Epoch: 293, Batch number: 8, Loss: 10571.10546875\n",
      "Epoch: 294, Batch number: 32, Loss: 10804.58203125\n",
      "Epoch: 295, Batch number: 56, Loss: 10434.880859375\n",
      "Epoch: 297, Batch number: 4, Loss: 10290.1240234375\n",
      "Epoch: 298, Batch number: 28, Loss: 10682.587890625\n",
      "Epoch: 299, Batch number: 52, Loss: 10516.5595703125\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 27365.255859375\n",
      "Epoch: 2, Batch number: 24, Loss: 26302.125\n",
      "Epoch: 3, Batch number: 48, Loss: 25870.802734375\n",
      "Epoch: 4, Batch number: 72, Loss: 24767.5546875\n",
      "Epoch: 6, Batch number: 20, Loss: 23483.611328125\n",
      "Epoch: 7, Batch number: 44, Loss: 23267.1875\n",
      "Epoch: 8, Batch number: 68, Loss: 22133.5390625\n",
      "Epoch: 10, Batch number: 16, Loss: 21979.7265625\n",
      "Epoch: 11, Batch number: 40, Loss: 21753.79296875\n",
      "Epoch: 12, Batch number: 64, Loss: 21543.0390625\n",
      "Epoch: 14, Batch number: 12, Loss: 20624.1484375\n",
      "Epoch: 15, Batch number: 36, Loss: 20603.509765625\n",
      "Epoch: 16, Batch number: 60, Loss: 20039.640625\n",
      "Epoch: 18, Batch number: 8, Loss: 20675.333984375\n",
      "Epoch: 19, Batch number: 32, Loss: 19975.9609375\n",
      "Epoch: 20, Batch number: 56, Loss: 19627.40625\n",
      "Epoch: 22, Batch number: 4, Loss: 19168.224609375\n",
      "Epoch: 23, Batch number: 28, Loss: 19722.171875\n",
      "Epoch: 24, Batch number: 52, Loss: 19411.818359375\n",
      "Epoch: 26, Batch number: 0, Loss: 19237.30859375\n",
      "Epoch: 27, Batch number: 24, Loss: 19146.71875\n",
      "Epoch: 28, Batch number: 48, Loss: 18544.15625\n",
      "Epoch: 29, Batch number: 72, Loss: 18763.8359375\n",
      "Epoch: 31, Batch number: 20, Loss: 18943.5859375\n",
      "Epoch: 32, Batch number: 44, Loss: 18527.70703125\n",
      "Epoch: 33, Batch number: 68, Loss: 18517.490234375\n",
      "Epoch: 35, Batch number: 16, Loss: 18928.11328125\n",
      "Epoch: 36, Batch number: 40, Loss: 18295.154296875\n",
      "Epoch: 37, Batch number: 64, Loss: 18207.591796875\n",
      "Epoch: 39, Batch number: 12, Loss: 18280.095703125\n",
      "Epoch: 40, Batch number: 36, Loss: 18531.078125\n",
      "Epoch: 41, Batch number: 60, Loss: 18682.62109375\n",
      "Epoch: 43, Batch number: 8, Loss: 18249.587890625\n",
      "Epoch: 44, Batch number: 32, Loss: 17810.24609375\n",
      "Epoch: 45, Batch number: 56, Loss: 18236.81640625\n",
      "Epoch: 47, Batch number: 4, Loss: 17815.55078125\n",
      "Epoch: 48, Batch number: 28, Loss: 17703.259765625\n",
      "Epoch: 49, Batch number: 52, Loss: 17756.396484375\n",
      "Epoch: 51, Batch number: 0, Loss: 17591.388671875\n",
      "Epoch: 52, Batch number: 24, Loss: 17661.990234375\n",
      "Epoch: 53, Batch number: 48, Loss: 17435.4140625\n",
      "Epoch: 54, Batch number: 72, Loss: 17702.81640625\n",
      "Epoch: 56, Batch number: 20, Loss: 17543.6875\n",
      "Epoch: 57, Batch number: 44, Loss: 18144.19921875\n",
      "Epoch: 58, Batch number: 68, Loss: 17480.0859375\n",
      "Epoch: 60, Batch number: 16, Loss: 17934.28125\n",
      "Epoch: 61, Batch number: 40, Loss: 17282.177734375\n",
      "Epoch: 62, Batch number: 64, Loss: 17292.0234375\n",
      "Epoch: 64, Batch number: 12, Loss: 17250.66796875\n",
      "Epoch: 65, Batch number: 36, Loss: 17198.6953125\n",
      "Epoch: 66, Batch number: 60, Loss: 16977.9296875\n",
      "Epoch: 68, Batch number: 8, Loss: 16933.75\n",
      "Epoch: 69, Batch number: 32, Loss: 17271.771484375\n",
      "Epoch: 70, Batch number: 56, Loss: 16905.9453125\n",
      "Epoch: 72, Batch number: 4, Loss: 17028.296875\n",
      "Epoch: 73, Batch number: 28, Loss: 17199.2265625\n",
      "Epoch: 74, Batch number: 52, Loss: 17284.853515625\n",
      "Epoch: 76, Batch number: 0, Loss: 17348.455078125\n",
      "Epoch: 77, Batch number: 24, Loss: 16943.474609375\n",
      "Epoch: 78, Batch number: 48, Loss: 16993.955078125\n",
      "Epoch: 79, Batch number: 72, Loss: 16914.474609375\n",
      "Epoch: 81, Batch number: 20, Loss: 17074.513671875\n",
      "Epoch: 82, Batch number: 44, Loss: 17067.87109375\n",
      "Epoch: 83, Batch number: 68, Loss: 16627.458984375\n",
      "Epoch: 85, Batch number: 16, Loss: 17063.87109375\n",
      "Epoch: 86, Batch number: 40, Loss: 16632.310546875\n",
      "Epoch: 87, Batch number: 64, Loss: 16891.193359375\n",
      "Epoch: 89, Batch number: 12, Loss: 16826.00390625\n",
      "Epoch: 90, Batch number: 36, Loss: 16541.19140625\n",
      "Epoch: 91, Batch number: 60, Loss: 16847.12890625\n",
      "Epoch: 93, Batch number: 8, Loss: 16479.3828125\n",
      "Epoch: 94, Batch number: 32, Loss: 16753.859375\n",
      "Epoch: 95, Batch number: 56, Loss: 17258.32421875\n",
      "Epoch: 97, Batch number: 4, Loss: 16216.3681640625\n",
      "Epoch: 98, Batch number: 28, Loss: 16792.34375\n",
      "Epoch: 99, Batch number: 52, Loss: 16425.564453125\n",
      "Epoch: 101, Batch number: 0, Loss: 16762.99609375\n",
      "Epoch: 102, Batch number: 24, Loss: 16701.044921875\n",
      "Epoch: 103, Batch number: 48, Loss: 16789.765625\n",
      "Epoch: 104, Batch number: 72, Loss: 16564.1171875\n",
      "Epoch: 106, Batch number: 20, Loss: 16612.921875\n",
      "Epoch: 107, Batch number: 44, Loss: 16222.119140625\n",
      "Epoch: 108, Batch number: 68, Loss: 16559.759765625\n",
      "Epoch: 110, Batch number: 16, Loss: 16344.4970703125\n",
      "Epoch: 111, Batch number: 40, Loss: 16339.0869140625\n",
      "Epoch: 112, Batch number: 64, Loss: 16410.365234375\n",
      "Epoch: 114, Batch number: 12, Loss: 16359.130859375\n",
      "Epoch: 115, Batch number: 36, Loss: 16312.5390625\n",
      "Epoch: 116, Batch number: 60, Loss: 16246.0771484375\n",
      "Epoch: 118, Batch number: 8, Loss: 16543.75\n",
      "Epoch: 119, Batch number: 32, Loss: 16583.521484375\n",
      "Epoch: 120, Batch number: 56, Loss: 15935.6083984375\n",
      "Epoch: 122, Batch number: 4, Loss: 15980.7275390625\n",
      "Epoch: 123, Batch number: 28, Loss: 16317.6630859375\n",
      "Epoch: 124, Batch number: 52, Loss: 16122.634765625\n",
      "Epoch: 126, Batch number: 0, Loss: 15797.533203125\n",
      "Epoch: 127, Batch number: 24, Loss: 16250.1513671875\n",
      "Epoch: 128, Batch number: 48, Loss: 16245.4208984375\n",
      "Epoch: 129, Batch number: 72, Loss: 15988.873046875\n",
      "Epoch: 131, Batch number: 20, Loss: 15945.0703125\n",
      "Epoch: 132, Batch number: 44, Loss: 16074.974609375\n",
      "Epoch: 133, Batch number: 68, Loss: 16269.4609375\n",
      "Epoch: 135, Batch number: 16, Loss: 16080.4638671875\n",
      "Epoch: 136, Batch number: 40, Loss: 16107.9482421875\n",
      "Epoch: 137, Batch number: 64, Loss: 15757.7900390625\n",
      "Epoch: 139, Batch number: 12, Loss: 15773.7529296875\n",
      "Epoch: 140, Batch number: 36, Loss: 16061.5166015625\n",
      "Epoch: 141, Batch number: 60, Loss: 15791.8681640625\n",
      "Epoch: 143, Batch number: 8, Loss: 16006.9912109375\n",
      "Epoch: 144, Batch number: 32, Loss: 15769.4619140625\n",
      "Epoch: 145, Batch number: 56, Loss: 15714.2236328125\n",
      "Epoch: 147, Batch number: 4, Loss: 15797.955078125\n",
      "Epoch: 148, Batch number: 28, Loss: 15841.4052734375\n",
      "Epoch: 149, Batch number: 52, Loss: 15608.8447265625\n",
      "Epoch: 151, Batch number: 0, Loss: 16021.767578125\n",
      "Epoch: 152, Batch number: 24, Loss: 15582.6767578125\n",
      "Epoch: 153, Batch number: 48, Loss: 15771.5537109375\n",
      "Epoch: 154, Batch number: 72, Loss: 15700.6953125\n",
      "Epoch: 156, Batch number: 20, Loss: 16003.2646484375\n",
      "Epoch: 157, Batch number: 44, Loss: 15780.9853515625\n",
      "Epoch: 158, Batch number: 68, Loss: 15476.3984375\n",
      "Epoch: 160, Batch number: 16, Loss: 15525.4609375\n",
      "Epoch: 161, Batch number: 40, Loss: 15940.9892578125\n",
      "Epoch: 162, Batch number: 64, Loss: 15641.1337890625\n",
      "Epoch: 164, Batch number: 12, Loss: 15609.794921875\n",
      "Epoch: 165, Batch number: 36, Loss: 15416.5478515625\n",
      "Epoch: 166, Batch number: 60, Loss: 15471.94140625\n",
      "Epoch: 168, Batch number: 8, Loss: 15800.0693359375\n",
      "Epoch: 169, Batch number: 32, Loss: 15831.69921875\n",
      "Epoch: 170, Batch number: 56, Loss: 15575.740234375\n",
      "Epoch: 172, Batch number: 4, Loss: 15641.453125\n",
      "Epoch: 173, Batch number: 28, Loss: 15435.345703125\n",
      "Epoch: 174, Batch number: 52, Loss: 15462.9072265625\n",
      "Epoch: 176, Batch number: 0, Loss: 15719.02734375\n",
      "Epoch: 177, Batch number: 24, Loss: 15515.173828125\n",
      "Epoch: 178, Batch number: 48, Loss: 15504.2138671875\n",
      "Epoch: 179, Batch number: 72, Loss: 15980.9609375\n",
      "Epoch: 181, Batch number: 20, Loss: 15193.6142578125\n",
      "Epoch: 182, Batch number: 44, Loss: 15647.3701171875\n",
      "Epoch: 183, Batch number: 68, Loss: 15299.4296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185, Batch number: 16, Loss: 15203.798828125\n",
      "Epoch: 186, Batch number: 40, Loss: 15117.064453125\n",
      "Epoch: 187, Batch number: 64, Loss: 15472.126953125\n",
      "Epoch: 189, Batch number: 12, Loss: 15399.2939453125\n",
      "Epoch: 190, Batch number: 36, Loss: 15469.0390625\n",
      "Epoch: 191, Batch number: 60, Loss: 15437.849609375\n",
      "Epoch: 193, Batch number: 8, Loss: 15420.275390625\n",
      "Epoch: 194, Batch number: 32, Loss: 15113.7197265625\n",
      "Epoch: 195, Batch number: 56, Loss: 15493.033203125\n",
      "Epoch: 197, Batch number: 4, Loss: 15402.9384765625\n",
      "Epoch: 198, Batch number: 28, Loss: 14979.6005859375\n",
      "Epoch: 199, Batch number: 52, Loss: 15468.3232421875\n",
      "Epoch: 201, Batch number: 0, Loss: 15169.2607421875\n",
      "Epoch: 202, Batch number: 24, Loss: 15195.0546875\n",
      "Epoch: 203, Batch number: 48, Loss: 15247.849609375\n",
      "Epoch: 204, Batch number: 72, Loss: 15606.4033203125\n",
      "Epoch: 206, Batch number: 20, Loss: 15382.947265625\n",
      "Epoch: 207, Batch number: 44, Loss: 15397.4501953125\n",
      "Epoch: 208, Batch number: 68, Loss: 15108.462890625\n",
      "Epoch: 210, Batch number: 16, Loss: 14820.857421875\n",
      "Epoch: 211, Batch number: 40, Loss: 15348.1416015625\n",
      "Epoch: 212, Batch number: 64, Loss: 14903.140625\n",
      "Epoch: 214, Batch number: 12, Loss: 15339.1357421875\n",
      "Epoch: 215, Batch number: 36, Loss: 14665.1474609375\n",
      "Epoch: 216, Batch number: 60, Loss: 15640.9375\n",
      "Epoch: 218, Batch number: 8, Loss: 14444.7392578125\n",
      "Epoch: 219, Batch number: 32, Loss: 15077.1455078125\n",
      "Epoch: 220, Batch number: 56, Loss: 15095.2548828125\n",
      "Epoch: 222, Batch number: 4, Loss: 15007.9541015625\n",
      "Epoch: 223, Batch number: 28, Loss: 14927.54296875\n",
      "Epoch: 224, Batch number: 52, Loss: 15014.505859375\n",
      "Epoch: 226, Batch number: 0, Loss: 15052.6220703125\n",
      "Epoch: 227, Batch number: 24, Loss: 14795.9482421875\n",
      "Epoch: 228, Batch number: 48, Loss: 14840.228515625\n",
      "Epoch: 229, Batch number: 72, Loss: 14655.185546875\n",
      "Epoch: 231, Batch number: 20, Loss: 14902.4443359375\n",
      "Epoch: 232, Batch number: 44, Loss: 14913.7412109375\n",
      "Epoch: 233, Batch number: 68, Loss: 15127.5810546875\n",
      "Epoch: 235, Batch number: 16, Loss: 14962.251953125\n",
      "Epoch: 236, Batch number: 40, Loss: 14803.8173828125\n",
      "Epoch: 237, Batch number: 64, Loss: 15001.490234375\n",
      "Epoch: 239, Batch number: 12, Loss: 15260.2119140625\n",
      "Epoch: 240, Batch number: 36, Loss: 15033.9794921875\n",
      "Epoch: 241, Batch number: 60, Loss: 15121.654296875\n",
      "Epoch: 243, Batch number: 8, Loss: 14838.4501953125\n",
      "Epoch: 244, Batch number: 32, Loss: 15094.5869140625\n",
      "Epoch: 245, Batch number: 56, Loss: 14804.513671875\n",
      "Epoch: 247, Batch number: 4, Loss: 14505.0322265625\n",
      "Epoch: 248, Batch number: 28, Loss: 15335.9580078125\n",
      "Epoch: 249, Batch number: 52, Loss: 14835.8583984375\n",
      "Epoch: 251, Batch number: 0, Loss: 14592.4814453125\n",
      "Epoch: 252, Batch number: 24, Loss: 14855.1787109375\n",
      "Epoch: 253, Batch number: 48, Loss: 15148.705078125\n",
      "Epoch: 254, Batch number: 72, Loss: 14817.0478515625\n",
      "Epoch: 256, Batch number: 20, Loss: 15339.4345703125\n",
      "Epoch: 257, Batch number: 44, Loss: 14627.4775390625\n",
      "Epoch: 258, Batch number: 68, Loss: 15178.298828125\n",
      "Epoch: 260, Batch number: 16, Loss: 14963.5400390625\n",
      "Epoch: 261, Batch number: 40, Loss: 14750.5830078125\n",
      "Epoch: 262, Batch number: 64, Loss: 14941.8896484375\n",
      "Epoch: 264, Batch number: 12, Loss: 15039.392578125\n",
      "Epoch: 265, Batch number: 36, Loss: 15105.12890625\n",
      "Epoch: 266, Batch number: 60, Loss: 15158.59375\n",
      "Epoch: 268, Batch number: 8, Loss: 14845.2998046875\n",
      "Epoch: 269, Batch number: 32, Loss: 15001.50390625\n",
      "Epoch: 270, Batch number: 56, Loss: 14879.2080078125\n",
      "Epoch: 272, Batch number: 4, Loss: 14339.3623046875\n",
      "Epoch: 273, Batch number: 28, Loss: 15179.8447265625\n",
      "Epoch: 274, Batch number: 52, Loss: 14650.3349609375\n",
      "Epoch: 276, Batch number: 0, Loss: 14736.9990234375\n",
      "Epoch: 277, Batch number: 24, Loss: 14704.4208984375\n",
      "Epoch: 278, Batch number: 48, Loss: 14830.7529296875\n",
      "Epoch: 279, Batch number: 72, Loss: 14865.9775390625\n",
      "Epoch: 281, Batch number: 20, Loss: 14869.751953125\n",
      "Epoch: 282, Batch number: 44, Loss: 14718.998046875\n",
      "Epoch: 283, Batch number: 68, Loss: 14853.7060546875\n",
      "Epoch: 285, Batch number: 16, Loss: 14846.5751953125\n",
      "Epoch: 286, Batch number: 40, Loss: 14783.18359375\n",
      "Epoch: 287, Batch number: 64, Loss: 15112.9287109375\n",
      "Epoch: 289, Batch number: 12, Loss: 14699.5595703125\n",
      "Epoch: 290, Batch number: 36, Loss: 14710.5732421875\n",
      "Epoch: 291, Batch number: 60, Loss: 14482.568359375\n",
      "Epoch: 293, Batch number: 8, Loss: 14665.0107421875\n",
      "Epoch: 294, Batch number: 32, Loss: 14413.5888671875\n",
      "Epoch: 295, Batch number: 56, Loss: 14613.099609375\n",
      "Epoch: 297, Batch number: 4, Loss: 14505.44140625\n",
      "Epoch: 298, Batch number: 28, Loss: 14779.1845703125\n",
      "Epoch: 299, Batch number: 52, Loss: 14658.7060546875\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 26976.064453125\n",
      "Epoch: 2, Batch number: 24, Loss: 25879.744140625\n",
      "Epoch: 3, Batch number: 48, Loss: 24361.400390625\n",
      "Epoch: 4, Batch number: 72, Loss: 23823.697265625\n",
      "Epoch: 6, Batch number: 20, Loss: 22350.744140625\n",
      "Epoch: 7, Batch number: 44, Loss: 21496.39453125\n",
      "Epoch: 8, Batch number: 68, Loss: 21025.361328125\n",
      "Epoch: 10, Batch number: 16, Loss: 21199.193359375\n",
      "Epoch: 11, Batch number: 40, Loss: 20416.861328125\n",
      "Epoch: 12, Batch number: 64, Loss: 20087.16796875\n",
      "Epoch: 14, Batch number: 12, Loss: 19503.10546875\n",
      "Epoch: 15, Batch number: 36, Loss: 19329.177734375\n",
      "Epoch: 16, Batch number: 60, Loss: 19358.392578125\n",
      "Epoch: 18, Batch number: 8, Loss: 18777.248046875\n",
      "Epoch: 19, Batch number: 32, Loss: 18992.25390625\n",
      "Epoch: 20, Batch number: 56, Loss: 18474.658203125\n",
      "Epoch: 22, Batch number: 4, Loss: 18572.283203125\n",
      "Epoch: 23, Batch number: 28, Loss: 17858.796875\n",
      "Epoch: 24, Batch number: 52, Loss: 17783.08984375\n",
      "Epoch: 26, Batch number: 0, Loss: 17510.62890625\n",
      "Epoch: 27, Batch number: 24, Loss: 17528.9140625\n",
      "Epoch: 28, Batch number: 48, Loss: 17684.208984375\n",
      "Epoch: 29, Batch number: 72, Loss: 17318.974609375\n",
      "Epoch: 31, Batch number: 20, Loss: 16999.482421875\n",
      "Epoch: 32, Batch number: 44, Loss: 17559.271484375\n",
      "Epoch: 33, Batch number: 68, Loss: 16902.5625\n",
      "Epoch: 35, Batch number: 16, Loss: 16893.650390625\n",
      "Epoch: 36, Batch number: 40, Loss: 16881.306640625\n",
      "Epoch: 37, Batch number: 64, Loss: 16802.083984375\n",
      "Epoch: 39, Batch number: 12, Loss: 16998.958984375\n",
      "Epoch: 40, Batch number: 36, Loss: 16728.75\n",
      "Epoch: 41, Batch number: 60, Loss: 16752.14453125\n",
      "Epoch: 43, Batch number: 8, Loss: 16474.181640625\n",
      "Epoch: 44, Batch number: 32, Loss: 16412.20703125\n",
      "Epoch: 45, Batch number: 56, Loss: 16594.25390625\n",
      "Epoch: 47, Batch number: 4, Loss: 16457.56640625\n",
      "Epoch: 48, Batch number: 28, Loss: 16433.697265625\n",
      "Epoch: 49, Batch number: 52, Loss: 16544.875\n",
      "Epoch: 51, Batch number: 0, Loss: 16552.5234375\n",
      "Epoch: 52, Batch number: 24, Loss: 16022.81640625\n",
      "Epoch: 53, Batch number: 48, Loss: 16104.0595703125\n",
      "Epoch: 54, Batch number: 72, Loss: 15859.3369140625\n",
      "Epoch: 56, Batch number: 20, Loss: 16369.82421875\n",
      "Epoch: 57, Batch number: 44, Loss: 16233.689453125\n",
      "Epoch: 58, Batch number: 68, Loss: 16000.646484375\n",
      "Epoch: 60, Batch number: 16, Loss: 16236.6328125\n",
      "Epoch: 61, Batch number: 40, Loss: 15662.638671875\n",
      "Epoch: 62, Batch number: 64, Loss: 16014.173828125\n",
      "Epoch: 64, Batch number: 12, Loss: 15796.7568359375\n",
      "Epoch: 65, Batch number: 36, Loss: 15871.603515625\n",
      "Epoch: 66, Batch number: 60, Loss: 16069.7587890625\n",
      "Epoch: 68, Batch number: 8, Loss: 15504.5166015625\n",
      "Epoch: 69, Batch number: 32, Loss: 15664.990234375\n",
      "Epoch: 70, Batch number: 56, Loss: 15783.7939453125\n",
      "Epoch: 72, Batch number: 4, Loss: 15579.900390625\n",
      "Epoch: 73, Batch number: 28, Loss: 15554.2978515625\n",
      "Epoch: 74, Batch number: 52, Loss: 15666.5439453125\n",
      "Epoch: 76, Batch number: 0, Loss: 15523.8291015625\n",
      "Epoch: 77, Batch number: 24, Loss: 15512.5478515625\n",
      "Epoch: 78, Batch number: 48, Loss: 15488.080078125\n",
      "Epoch: 79, Batch number: 72, Loss: 15717.16015625\n",
      "Epoch: 81, Batch number: 20, Loss: 15452.4697265625\n",
      "Epoch: 82, Batch number: 44, Loss: 15011.615234375\n",
      "Epoch: 83, Batch number: 68, Loss: 15505.435546875\n",
      "Epoch: 85, Batch number: 16, Loss: 15079.7421875\n",
      "Epoch: 86, Batch number: 40, Loss: 15056.6806640625\n",
      "Epoch: 87, Batch number: 64, Loss: 15294.2236328125\n",
      "Epoch: 89, Batch number: 12, Loss: 14999.83984375\n",
      "Epoch: 90, Batch number: 36, Loss: 15454.9462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Batch number: 60, Loss: 15190.3916015625\n",
      "Epoch: 93, Batch number: 8, Loss: 15177.2255859375\n",
      "Epoch: 94, Batch number: 32, Loss: 15148.2744140625\n",
      "Epoch: 95, Batch number: 56, Loss: 15050.77734375\n",
      "Epoch: 97, Batch number: 4, Loss: 15040.7841796875\n",
      "Epoch: 98, Batch number: 28, Loss: 15273.2578125\n",
      "Epoch: 99, Batch number: 52, Loss: 14996.3916015625\n",
      "Epoch: 101, Batch number: 0, Loss: 15108.5166015625\n",
      "Epoch: 102, Batch number: 24, Loss: 15266.017578125\n",
      "Epoch: 103, Batch number: 48, Loss: 14966.439453125\n",
      "Epoch: 104, Batch number: 72, Loss: 14675.4169921875\n",
      "Epoch: 106, Batch number: 20, Loss: 15060.2724609375\n",
      "Epoch: 107, Batch number: 44, Loss: 15023.240234375\n",
      "Epoch: 108, Batch number: 68, Loss: 15073.2109375\n",
      "Epoch: 110, Batch number: 16, Loss: 15081.869140625\n",
      "Epoch: 111, Batch number: 40, Loss: 15155.4853515625\n",
      "Epoch: 112, Batch number: 64, Loss: 15338.939453125\n",
      "Epoch: 114, Batch number: 12, Loss: 14906.2822265625\n",
      "Epoch: 115, Batch number: 36, Loss: 14915.0703125\n",
      "Epoch: 116, Batch number: 60, Loss: 15094.9765625\n",
      "Epoch: 118, Batch number: 8, Loss: 14870.111328125\n",
      "Epoch: 119, Batch number: 32, Loss: 14655.6484375\n",
      "Epoch: 120, Batch number: 56, Loss: 14542.3515625\n",
      "Epoch: 122, Batch number: 4, Loss: 14752.6376953125\n",
      "Epoch: 123, Batch number: 28, Loss: 14628.65234375\n",
      "Epoch: 124, Batch number: 52, Loss: 14979.1494140625\n",
      "Epoch: 126, Batch number: 0, Loss: 14408.365234375\n",
      "Epoch: 127, Batch number: 24, Loss: 14453.533203125\n",
      "Epoch: 128, Batch number: 48, Loss: 14994.9931640625\n",
      "Epoch: 129, Batch number: 72, Loss: 14780.876953125\n",
      "Epoch: 131, Batch number: 20, Loss: 14596.64453125\n",
      "Epoch: 132, Batch number: 44, Loss: 14509.107421875\n",
      "Epoch: 133, Batch number: 68, Loss: 14860.5439453125\n",
      "Epoch: 135, Batch number: 16, Loss: 14729.5087890625\n",
      "Epoch: 136, Batch number: 40, Loss: 14607.1796875\n",
      "Epoch: 137, Batch number: 64, Loss: 14622.4697265625\n",
      "Epoch: 139, Batch number: 12, Loss: 14198.529296875\n",
      "Epoch: 140, Batch number: 36, Loss: 14490.3466796875\n",
      "Epoch: 141, Batch number: 60, Loss: 14892.365234375\n",
      "Epoch: 143, Batch number: 8, Loss: 14521.47265625\n",
      "Epoch: 144, Batch number: 32, Loss: 14346.55859375\n",
      "Epoch: 145, Batch number: 56, Loss: 14935.580078125\n",
      "Epoch: 147, Batch number: 4, Loss: 14468.529296875\n",
      "Epoch: 148, Batch number: 28, Loss: 14583.185546875\n",
      "Epoch: 149, Batch number: 52, Loss: 14119.2216796875\n",
      "Epoch: 151, Batch number: 0, Loss: 14374.0244140625\n",
      "Epoch: 152, Batch number: 24, Loss: 13922.185546875\n",
      "Epoch: 153, Batch number: 48, Loss: 14148.91015625\n",
      "Epoch: 154, Batch number: 72, Loss: 14649.5888671875\n",
      "Epoch: 156, Batch number: 20, Loss: 14164.70703125\n",
      "Epoch: 157, Batch number: 44, Loss: 14094.7275390625\n",
      "Epoch: 158, Batch number: 68, Loss: 14493.443359375\n",
      "Epoch: 160, Batch number: 16, Loss: 14462.935546875\n",
      "Epoch: 161, Batch number: 40, Loss: 14243.8388671875\n",
      "Epoch: 162, Batch number: 64, Loss: 14776.404296875\n",
      "Epoch: 164, Batch number: 12, Loss: 14068.0185546875\n",
      "Epoch: 165, Batch number: 36, Loss: 13802.560546875\n",
      "Epoch: 166, Batch number: 60, Loss: 14644.255859375\n",
      "Epoch: 168, Batch number: 8, Loss: 14255.8740234375\n",
      "Epoch: 169, Batch number: 32, Loss: 14440.1181640625\n",
      "Epoch: 170, Batch number: 56, Loss: 14042.1494140625\n",
      "Epoch: 172, Batch number: 4, Loss: 14499.763671875\n",
      "Epoch: 173, Batch number: 28, Loss: 14157.27734375\n",
      "Epoch: 174, Batch number: 52, Loss: 14267.708984375\n",
      "Epoch: 176, Batch number: 0, Loss: 14415.3203125\n",
      "Epoch: 177, Batch number: 24, Loss: 14328.3115234375\n",
      "Epoch: 178, Batch number: 48, Loss: 14391.2333984375\n",
      "Epoch: 179, Batch number: 72, Loss: 14967.2080078125\n",
      "Epoch: 181, Batch number: 20, Loss: 14656.671875\n",
      "Epoch: 182, Batch number: 44, Loss: 13950.8671875\n",
      "Epoch: 183, Batch number: 68, Loss: 14254.798828125\n",
      "Epoch: 185, Batch number: 16, Loss: 14049.40234375\n",
      "Epoch: 186, Batch number: 40, Loss: 14115.29296875\n",
      "Epoch: 187, Batch number: 64, Loss: 14183.7060546875\n",
      "Epoch: 189, Batch number: 12, Loss: 14149.4677734375\n",
      "Epoch: 190, Batch number: 36, Loss: 14505.681640625\n",
      "Epoch: 191, Batch number: 60, Loss: 14074.0849609375\n",
      "Epoch: 193, Batch number: 8, Loss: 14120.4208984375\n",
      "Epoch: 194, Batch number: 32, Loss: 13934.3203125\n",
      "Epoch: 195, Batch number: 56, Loss: 14082.70703125\n",
      "Epoch: 197, Batch number: 4, Loss: 14075.572265625\n",
      "Epoch: 198, Batch number: 28, Loss: 13750.1689453125\n",
      "Epoch: 199, Batch number: 52, Loss: 14129.18359375\n",
      "Epoch: 201, Batch number: 0, Loss: 14218.60546875\n",
      "Epoch: 202, Batch number: 24, Loss: 14188.7880859375\n",
      "Epoch: 203, Batch number: 48, Loss: 13935.234375\n",
      "Epoch: 204, Batch number: 72, Loss: 14296.7998046875\n",
      "Epoch: 206, Batch number: 20, Loss: 14021.7568359375\n",
      "Epoch: 207, Batch number: 44, Loss: 14239.1748046875\n",
      "Epoch: 208, Batch number: 68, Loss: 14469.6767578125\n",
      "Epoch: 210, Batch number: 16, Loss: 13919.208984375\n",
      "Epoch: 211, Batch number: 40, Loss: 13917.0517578125\n",
      "Epoch: 212, Batch number: 64, Loss: 14100.525390625\n",
      "Epoch: 214, Batch number: 12, Loss: 13986.681640625\n",
      "Epoch: 215, Batch number: 36, Loss: 14411.017578125\n",
      "Epoch: 216, Batch number: 60, Loss: 14223.8486328125\n",
      "Epoch: 218, Batch number: 8, Loss: 14236.8134765625\n",
      "Epoch: 219, Batch number: 32, Loss: 14210.0849609375\n",
      "Epoch: 220, Batch number: 56, Loss: 14448.4677734375\n",
      "Epoch: 222, Batch number: 4, Loss: 13702.392578125\n",
      "Epoch: 223, Batch number: 28, Loss: 14235.9345703125\n",
      "Epoch: 224, Batch number: 52, Loss: 14061.953125\n",
      "Epoch: 226, Batch number: 0, Loss: 14146.8369140625\n",
      "Epoch: 227, Batch number: 24, Loss: 13852.421875\n",
      "Epoch: 228, Batch number: 48, Loss: 14131.4228515625\n",
      "Epoch: 229, Batch number: 72, Loss: 13732.154296875\n",
      "Epoch: 231, Batch number: 20, Loss: 13889.2646484375\n",
      "Epoch: 232, Batch number: 44, Loss: 14401.1552734375\n",
      "Epoch: 233, Batch number: 68, Loss: 13888.8173828125\n",
      "Epoch: 235, Batch number: 16, Loss: 13476.9462890625\n",
      "Epoch: 236, Batch number: 40, Loss: 13859.7099609375\n",
      "Epoch: 237, Batch number: 64, Loss: 13704.8583984375\n",
      "Epoch: 239, Batch number: 12, Loss: 14027.5703125\n",
      "Epoch: 240, Batch number: 36, Loss: 14351.1787109375\n",
      "Epoch: 241, Batch number: 60, Loss: 13917.576171875\n",
      "Epoch: 243, Batch number: 8, Loss: 13760.310546875\n",
      "Epoch: 244, Batch number: 32, Loss: 14077.0791015625\n",
      "Epoch: 245, Batch number: 56, Loss: 14034.294921875\n",
      "Epoch: 247, Batch number: 4, Loss: 13725.90625\n",
      "Epoch: 248, Batch number: 28, Loss: 14054.7314453125\n",
      "Epoch: 249, Batch number: 52, Loss: 13920.630859375\n",
      "Epoch: 251, Batch number: 0, Loss: 13899.5498046875\n",
      "Epoch: 252, Batch number: 24, Loss: 14151.275390625\n",
      "Epoch: 253, Batch number: 48, Loss: 13901.330078125\n",
      "Epoch: 254, Batch number: 72, Loss: 13639.662109375\n",
      "Epoch: 256, Batch number: 20, Loss: 13609.1728515625\n",
      "Epoch: 257, Batch number: 44, Loss: 13771.064453125\n",
      "Epoch: 258, Batch number: 68, Loss: 14061.2265625\n",
      "Epoch: 260, Batch number: 16, Loss: 14194.02734375\n",
      "Epoch: 261, Batch number: 40, Loss: 13536.865234375\n",
      "Epoch: 262, Batch number: 64, Loss: 13719.1083984375\n",
      "Epoch: 264, Batch number: 12, Loss: 13800.1279296875\n",
      "Epoch: 265, Batch number: 36, Loss: 14182.162109375\n",
      "Epoch: 266, Batch number: 60, Loss: 13661.22265625\n",
      "Epoch: 268, Batch number: 8, Loss: 13548.71875\n",
      "Epoch: 269, Batch number: 32, Loss: 13992.5859375\n",
      "Epoch: 270, Batch number: 56, Loss: 14067.2763671875\n",
      "Epoch: 272, Batch number: 4, Loss: 13617.7880859375\n",
      "Epoch: 273, Batch number: 28, Loss: 14213.650390625\n",
      "Epoch: 274, Batch number: 52, Loss: 14172.298828125\n",
      "Epoch: 276, Batch number: 0, Loss: 13447.5439453125\n",
      "Epoch: 277, Batch number: 24, Loss: 13431.4951171875\n",
      "Epoch: 278, Batch number: 48, Loss: 13887.251953125\n",
      "Epoch: 279, Batch number: 72, Loss: 13296.7275390625\n",
      "Epoch: 281, Batch number: 20, Loss: 13662.2265625\n",
      "Epoch: 282, Batch number: 44, Loss: 13867.1962890625\n",
      "Epoch: 283, Batch number: 68, Loss: 13793.001953125\n",
      "Epoch: 285, Batch number: 16, Loss: 13489.4716796875\n",
      "Epoch: 286, Batch number: 40, Loss: 13649.2744140625\n",
      "Epoch: 287, Batch number: 64, Loss: 13696.9912109375\n",
      "Epoch: 289, Batch number: 12, Loss: 13612.857421875\n",
      "Epoch: 290, Batch number: 36, Loss: 13725.6953125\n",
      "Epoch: 291, Batch number: 60, Loss: 13950.013671875\n",
      "Epoch: 293, Batch number: 8, Loss: 13773.4150390625\n",
      "Epoch: 294, Batch number: 32, Loss: 13955.0498046875\n",
      "Epoch: 295, Batch number: 56, Loss: 13817.990234375\n",
      "Epoch: 297, Batch number: 4, Loss: 13530.353515625\n",
      "Epoch: 298, Batch number: 28, Loss: 13682.1455078125\n",
      "Epoch: 299, Batch number: 52, Loss: 14017.4091796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 26608.291015625\n",
      "Epoch: 2, Batch number: 24, Loss: 25146.662109375\n",
      "Epoch: 3, Batch number: 48, Loss: 24218.951171875\n",
      "Epoch: 4, Batch number: 72, Loss: 22696.9921875\n",
      "Epoch: 6, Batch number: 20, Loss: 22228.76953125\n",
      "Epoch: 7, Batch number: 44, Loss: 21413.708984375\n",
      "Epoch: 8, Batch number: 68, Loss: 20603.623046875\n",
      "Epoch: 10, Batch number: 16, Loss: 19871.669921875\n",
      "Epoch: 11, Batch number: 40, Loss: 19689.046875\n",
      "Epoch: 12, Batch number: 64, Loss: 19304.666015625\n",
      "Epoch: 14, Batch number: 12, Loss: 18902.501953125\n",
      "Epoch: 15, Batch number: 36, Loss: 18402.875\n",
      "Epoch: 16, Batch number: 60, Loss: 18431.529296875\n",
      "Epoch: 18, Batch number: 8, Loss: 18167.341796875\n",
      "Epoch: 19, Batch number: 32, Loss: 17898.5234375\n",
      "Epoch: 20, Batch number: 56, Loss: 17497.6484375\n",
      "Epoch: 22, Batch number: 4, Loss: 17493.865234375\n",
      "Epoch: 23, Batch number: 28, Loss: 17024.259765625\n",
      "Epoch: 24, Batch number: 52, Loss: 17352.341796875\n",
      "Epoch: 26, Batch number: 0, Loss: 17081.498046875\n",
      "Epoch: 27, Batch number: 24, Loss: 16733.962890625\n",
      "Epoch: 28, Batch number: 48, Loss: 16396.119140625\n",
      "Epoch: 29, Batch number: 72, Loss: 16632.171875\n",
      "Epoch: 31, Batch number: 20, Loss: 16369.3486328125\n",
      "Epoch: 32, Batch number: 44, Loss: 16043.3046875\n",
      "Epoch: 33, Batch number: 68, Loss: 16417.595703125\n",
      "Epoch: 35, Batch number: 16, Loss: 16645.828125\n",
      "Epoch: 36, Batch number: 40, Loss: 15958.0751953125\n",
      "Epoch: 37, Batch number: 64, Loss: 16198.70703125\n",
      "Epoch: 39, Batch number: 12, Loss: 15684.0712890625\n",
      "Epoch: 40, Batch number: 36, Loss: 16090.4755859375\n",
      "Epoch: 41, Batch number: 60, Loss: 16023.2294921875\n",
      "Epoch: 43, Batch number: 8, Loss: 15408.515625\n",
      "Epoch: 44, Batch number: 32, Loss: 15595.8369140625\n",
      "Epoch: 45, Batch number: 56, Loss: 15443.1572265625\n",
      "Epoch: 47, Batch number: 4, Loss: 15214.919921875\n",
      "Epoch: 48, Batch number: 28, Loss: 15463.3994140625\n",
      "Epoch: 49, Batch number: 52, Loss: 15654.0859375\n",
      "Epoch: 51, Batch number: 0, Loss: 15079.3642578125\n",
      "Epoch: 52, Batch number: 24, Loss: 15403.7216796875\n",
      "Epoch: 53, Batch number: 48, Loss: 15729.63671875\n",
      "Epoch: 54, Batch number: 72, Loss: 15506.2822265625\n",
      "Epoch: 56, Batch number: 20, Loss: 15362.712890625\n",
      "Epoch: 57, Batch number: 44, Loss: 15353.4453125\n",
      "Epoch: 58, Batch number: 68, Loss: 15846.494140625\n",
      "Epoch: 60, Batch number: 16, Loss: 15041.0712890625\n",
      "Epoch: 61, Batch number: 40, Loss: 15014.46484375\n",
      "Epoch: 62, Batch number: 64, Loss: 14925.9814453125\n",
      "Epoch: 64, Batch number: 12, Loss: 15175.7021484375\n",
      "Epoch: 65, Batch number: 36, Loss: 14985.8935546875\n",
      "Epoch: 66, Batch number: 60, Loss: 15631.763671875\n",
      "Epoch: 68, Batch number: 8, Loss: 14790.599609375\n",
      "Epoch: 69, Batch number: 32, Loss: 14863.3701171875\n",
      "Epoch: 70, Batch number: 56, Loss: 15172.93359375\n",
      "Epoch: 72, Batch number: 4, Loss: 14738.337890625\n",
      "Epoch: 73, Batch number: 28, Loss: 14731.947265625\n",
      "Epoch: 74, Batch number: 52, Loss: 15230.109375\n",
      "Epoch: 76, Batch number: 0, Loss: 14578.1728515625\n",
      "Epoch: 77, Batch number: 24, Loss: 14746.2119140625\n",
      "Epoch: 78, Batch number: 48, Loss: 15094.0078125\n",
      "Epoch: 79, Batch number: 72, Loss: 14660.9130859375\n",
      "Epoch: 81, Batch number: 20, Loss: 14706.58984375\n",
      "Epoch: 82, Batch number: 44, Loss: 14781.22265625\n",
      "Epoch: 83, Batch number: 68, Loss: 14611.7578125\n",
      "Epoch: 85, Batch number: 16, Loss: 14752.9609375\n",
      "Epoch: 86, Batch number: 40, Loss: 14914.5810546875\n",
      "Epoch: 87, Batch number: 64, Loss: 14865.6171875\n",
      "Epoch: 89, Batch number: 12, Loss: 15013.5302734375\n",
      "Epoch: 90, Batch number: 36, Loss: 14364.853515625\n",
      "Epoch: 91, Batch number: 60, Loss: 14603.6494140625\n",
      "Epoch: 93, Batch number: 8, Loss: 14357.185546875\n",
      "Epoch: 94, Batch number: 32, Loss: 14715.779296875\n",
      "Epoch: 95, Batch number: 56, Loss: 14472.3115234375\n",
      "Epoch: 97, Batch number: 4, Loss: 14982.0712890625\n",
      "Epoch: 98, Batch number: 28, Loss: 14420.1875\n",
      "Epoch: 99, Batch number: 52, Loss: 14106.5029296875\n",
      "Epoch: 101, Batch number: 0, Loss: 14286.634765625\n",
      "Epoch: 102, Batch number: 24, Loss: 14612.2626953125\n",
      "Epoch: 103, Batch number: 48, Loss: 14649.2099609375\n",
      "Epoch: 104, Batch number: 72, Loss: 14463.9228515625\n",
      "Epoch: 106, Batch number: 20, Loss: 14317.3291015625\n",
      "Epoch: 107, Batch number: 44, Loss: 14484.490234375\n",
      "Epoch: 108, Batch number: 68, Loss: 14170.7060546875\n",
      "Epoch: 110, Batch number: 16, Loss: 14253.7578125\n",
      "Epoch: 111, Batch number: 40, Loss: 14525.0439453125\n",
      "Epoch: 112, Batch number: 64, Loss: 14503.50390625\n",
      "Epoch: 114, Batch number: 12, Loss: 14467.4208984375\n",
      "Epoch: 115, Batch number: 36, Loss: 13918.1806640625\n",
      "Epoch: 116, Batch number: 60, Loss: 14213.0390625\n",
      "Epoch: 118, Batch number: 8, Loss: 14218.322265625\n",
      "Epoch: 119, Batch number: 32, Loss: 14153.2578125\n",
      "Epoch: 120, Batch number: 56, Loss: 14404.818359375\n",
      "Epoch: 122, Batch number: 4, Loss: 14278.607421875\n",
      "Epoch: 123, Batch number: 28, Loss: 13781.2890625\n",
      "Epoch: 124, Batch number: 52, Loss: 13822.2548828125\n",
      "Epoch: 126, Batch number: 0, Loss: 14020.5595703125\n",
      "Epoch: 127, Batch number: 24, Loss: 14498.005859375\n",
      "Epoch: 128, Batch number: 48, Loss: 14295.33203125\n",
      "Epoch: 129, Batch number: 72, Loss: 14188.0791015625\n",
      "Epoch: 131, Batch number: 20, Loss: 13907.5517578125\n",
      "Epoch: 132, Batch number: 44, Loss: 14230.1591796875\n",
      "Epoch: 133, Batch number: 68, Loss: 14437.8427734375\n",
      "Epoch: 135, Batch number: 16, Loss: 13764.2470703125\n",
      "Epoch: 136, Batch number: 40, Loss: 14291.259765625\n",
      "Epoch: 137, Batch number: 64, Loss: 14104.35546875\n",
      "Epoch: 139, Batch number: 12, Loss: 13865.0556640625\n",
      "Epoch: 140, Batch number: 36, Loss: 14259.767578125\n",
      "Epoch: 141, Batch number: 60, Loss: 13706.7958984375\n",
      "Epoch: 143, Batch number: 8, Loss: 13980.578125\n",
      "Epoch: 144, Batch number: 32, Loss: 13871.4462890625\n",
      "Epoch: 145, Batch number: 56, Loss: 13753.17578125\n",
      "Epoch: 147, Batch number: 4, Loss: 14085.345703125\n",
      "Epoch: 148, Batch number: 28, Loss: 14179.9423828125\n",
      "Epoch: 149, Batch number: 52, Loss: 14360.009765625\n",
      "Epoch: 151, Batch number: 0, Loss: 14036.4541015625\n",
      "Epoch: 152, Batch number: 24, Loss: 13745.625\n",
      "Epoch: 153, Batch number: 48, Loss: 14158.9951171875\n",
      "Epoch: 154, Batch number: 72, Loss: 13939.60546875\n",
      "Epoch: 156, Batch number: 20, Loss: 13696.6611328125\n",
      "Epoch: 157, Batch number: 44, Loss: 14035.4443359375\n",
      "Epoch: 158, Batch number: 68, Loss: 13918.375\n",
      "Epoch: 160, Batch number: 16, Loss: 13239.5517578125\n",
      "Epoch: 161, Batch number: 40, Loss: 14297.431640625\n",
      "Epoch: 162, Batch number: 64, Loss: 13930.71875\n",
      "Epoch: 164, Batch number: 12, Loss: 14025.068359375\n",
      "Epoch: 165, Batch number: 36, Loss: 13773.68359375\n",
      "Epoch: 166, Batch number: 60, Loss: 14144.470703125\n",
      "Epoch: 168, Batch number: 8, Loss: 13920.173828125\n",
      "Epoch: 169, Batch number: 32, Loss: 13704.20703125\n",
      "Epoch: 170, Batch number: 56, Loss: 13550.0322265625\n",
      "Epoch: 172, Batch number: 4, Loss: 13436.9541015625\n",
      "Epoch: 173, Batch number: 28, Loss: 13457.25\n",
      "Epoch: 174, Batch number: 52, Loss: 14300.3681640625\n",
      "Epoch: 176, Batch number: 0, Loss: 14029.1396484375\n",
      "Epoch: 177, Batch number: 24, Loss: 14023.1181640625\n",
      "Epoch: 178, Batch number: 48, Loss: 13279.2509765625\n",
      "Epoch: 179, Batch number: 72, Loss: 13811.849609375\n",
      "Epoch: 181, Batch number: 20, Loss: 13997.416015625\n",
      "Epoch: 182, Batch number: 44, Loss: 13929.8466796875\n",
      "Epoch: 183, Batch number: 68, Loss: 13559.841796875\n",
      "Epoch: 185, Batch number: 16, Loss: 13578.9140625\n",
      "Epoch: 186, Batch number: 40, Loss: 13720.412109375\n",
      "Epoch: 187, Batch number: 64, Loss: 13150.93359375\n",
      "Epoch: 189, Batch number: 12, Loss: 13990.9599609375\n",
      "Epoch: 190, Batch number: 36, Loss: 13978.8408203125\n",
      "Epoch: 191, Batch number: 60, Loss: 13615.3125\n",
      "Epoch: 193, Batch number: 8, Loss: 13866.3056640625\n",
      "Epoch: 194, Batch number: 32, Loss: 13597.2958984375\n",
      "Epoch: 195, Batch number: 56, Loss: 13982.57421875\n",
      "Epoch: 197, Batch number: 4, Loss: 13905.0849609375\n",
      "Epoch: 198, Batch number: 28, Loss: 13784.681640625\n",
      "Epoch: 199, Batch number: 52, Loss: 13496.666015625\n",
      "Epoch: 201, Batch number: 0, Loss: 13895.4599609375\n",
      "Epoch: 202, Batch number: 24, Loss: 13759.2685546875\n",
      "Epoch: 203, Batch number: 48, Loss: 13851.90625\n",
      "Epoch: 204, Batch number: 72, Loss: 14237.15625\n",
      "Epoch: 206, Batch number: 20, Loss: 13830.3671875\n",
      "Epoch: 207, Batch number: 44, Loss: 13601.646484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208, Batch number: 68, Loss: 13759.900390625\n",
      "Epoch: 210, Batch number: 16, Loss: 13690.28515625\n",
      "Epoch: 211, Batch number: 40, Loss: 13808.392578125\n",
      "Epoch: 212, Batch number: 64, Loss: 14263.8876953125\n",
      "Epoch: 214, Batch number: 12, Loss: 13462.2802734375\n",
      "Epoch: 215, Batch number: 36, Loss: 13822.19140625\n",
      "Epoch: 216, Batch number: 60, Loss: 13707.8125\n",
      "Epoch: 218, Batch number: 8, Loss: 13416.1630859375\n",
      "Epoch: 219, Batch number: 32, Loss: 13557.248046875\n",
      "Epoch: 220, Batch number: 56, Loss: 13705.4853515625\n",
      "Epoch: 222, Batch number: 4, Loss: 13349.1328125\n",
      "Epoch: 223, Batch number: 28, Loss: 14098.853515625\n",
      "Epoch: 224, Batch number: 52, Loss: 13896.466796875\n",
      "Epoch: 226, Batch number: 0, Loss: 13904.12890625\n",
      "Epoch: 227, Batch number: 24, Loss: 13920.4140625\n",
      "Epoch: 228, Batch number: 48, Loss: 13748.265625\n",
      "Epoch: 229, Batch number: 72, Loss: 13457.3505859375\n",
      "Epoch: 231, Batch number: 20, Loss: 13951.29296875\n",
      "Epoch: 232, Batch number: 44, Loss: 13805.93359375\n",
      "Epoch: 233, Batch number: 68, Loss: 14117.4326171875\n",
      "Epoch: 235, Batch number: 16, Loss: 13462.775390625\n",
      "Epoch: 236, Batch number: 40, Loss: 13638.03515625\n",
      "Epoch: 237, Batch number: 64, Loss: 13854.4326171875\n",
      "Epoch: 239, Batch number: 12, Loss: 13695.7353515625\n",
      "Epoch: 240, Batch number: 36, Loss: 13405.09375\n",
      "Epoch: 241, Batch number: 60, Loss: 13376.7109375\n",
      "Epoch: 243, Batch number: 8, Loss: 13556.708984375\n",
      "Epoch: 244, Batch number: 32, Loss: 13754.6669921875\n",
      "Epoch: 245, Batch number: 56, Loss: 13983.583984375\n",
      "Epoch: 247, Batch number: 4, Loss: 13316.1259765625\n",
      "Epoch: 248, Batch number: 28, Loss: 13544.7958984375\n",
      "Epoch: 249, Batch number: 52, Loss: 13755.36328125\n",
      "Epoch: 251, Batch number: 0, Loss: 13622.396484375\n",
      "Epoch: 252, Batch number: 24, Loss: 13886.5234375\n",
      "Epoch: 253, Batch number: 48, Loss: 13609.6796875\n",
      "Epoch: 254, Batch number: 72, Loss: 13611.5087890625\n",
      "Epoch: 256, Batch number: 20, Loss: 13792.9052734375\n",
      "Epoch: 257, Batch number: 44, Loss: 13658.3916015625\n",
      "Epoch: 258, Batch number: 68, Loss: 13884.2783203125\n",
      "Epoch: 260, Batch number: 16, Loss: 13856.7431640625\n",
      "Epoch: 261, Batch number: 40, Loss: 13295.888671875\n",
      "Epoch: 262, Batch number: 64, Loss: 13910.3603515625\n",
      "Epoch: 264, Batch number: 12, Loss: 13600.2861328125\n",
      "Epoch: 265, Batch number: 36, Loss: 13448.6982421875\n",
      "Epoch: 266, Batch number: 60, Loss: 13739.396484375\n",
      "Epoch: 268, Batch number: 8, Loss: 13474.5703125\n",
      "Epoch: 269, Batch number: 32, Loss: 13456.892578125\n",
      "Epoch: 270, Batch number: 56, Loss: 13524.8310546875\n",
      "Epoch: 272, Batch number: 4, Loss: 13684.27734375\n",
      "Epoch: 273, Batch number: 28, Loss: 13847.560546875\n",
      "Epoch: 274, Batch number: 52, Loss: 13829.78515625\n",
      "Epoch: 276, Batch number: 0, Loss: 13303.5966796875\n",
      "Epoch: 277, Batch number: 24, Loss: 13464.923828125\n",
      "Epoch: 278, Batch number: 48, Loss: 13219.693359375\n",
      "Epoch: 279, Batch number: 72, Loss: 14089.06640625\n",
      "Epoch: 281, Batch number: 20, Loss: 13540.4599609375\n",
      "Epoch: 282, Batch number: 44, Loss: 13567.8740234375\n",
      "Epoch: 283, Batch number: 68, Loss: 13611.609375\n",
      "Epoch: 285, Batch number: 16, Loss: 13454.03515625\n",
      "Epoch: 286, Batch number: 40, Loss: 13504.54296875\n",
      "Epoch: 287, Batch number: 64, Loss: 14001.6103515625\n",
      "Epoch: 289, Batch number: 12, Loss: 13170.822265625\n",
      "Epoch: 290, Batch number: 36, Loss: 13776.50390625\n",
      "Epoch: 291, Batch number: 60, Loss: 13661.1826171875\n",
      "Epoch: 293, Batch number: 8, Loss: 13481.78125\n",
      "Epoch: 294, Batch number: 32, Loss: 14010.2783203125\n",
      "Epoch: 295, Batch number: 56, Loss: 14104.71875\n",
      "Epoch: 297, Batch number: 4, Loss: 13502.041015625\n",
      "Epoch: 298, Batch number: 28, Loss: 13671.7041015625\n",
      "Epoch: 299, Batch number: 52, Loss: 13404.435546875\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 27259.697265625\n",
      "Epoch: 2, Batch number: 24, Loss: 25032.541015625\n",
      "Epoch: 3, Batch number: 48, Loss: 23604.318359375\n",
      "Epoch: 4, Batch number: 72, Loss: 22370.19140625\n",
      "Epoch: 6, Batch number: 20, Loss: 21146.353515625\n",
      "Epoch: 7, Batch number: 44, Loss: 21050.40234375\n",
      "Epoch: 8, Batch number: 68, Loss: 20520.625\n",
      "Epoch: 10, Batch number: 16, Loss: 19236.458984375\n",
      "Epoch: 11, Batch number: 40, Loss: 18957.662109375\n",
      "Epoch: 12, Batch number: 64, Loss: 18513.6484375\n",
      "Epoch: 14, Batch number: 12, Loss: 18427.3125\n",
      "Epoch: 15, Batch number: 36, Loss: 17691.642578125\n",
      "Epoch: 16, Batch number: 60, Loss: 17683.13671875\n",
      "Epoch: 18, Batch number: 8, Loss: 17255.2265625\n",
      "Epoch: 19, Batch number: 32, Loss: 16973.4140625\n",
      "Epoch: 20, Batch number: 56, Loss: 16842.359375\n",
      "Epoch: 22, Batch number: 4, Loss: 16911.1875\n",
      "Epoch: 23, Batch number: 28, Loss: 16567.33203125\n",
      "Epoch: 24, Batch number: 52, Loss: 16461.966796875\n",
      "Epoch: 26, Batch number: 0, Loss: 15977.9775390625\n",
      "Epoch: 27, Batch number: 24, Loss: 16278.1455078125\n",
      "Epoch: 28, Batch number: 48, Loss: 16152.8984375\n",
      "Epoch: 29, Batch number: 72, Loss: 16037.203125\n",
      "Epoch: 31, Batch number: 20, Loss: 15867.2744140625\n",
      "Epoch: 32, Batch number: 44, Loss: 15853.3623046875\n",
      "Epoch: 33, Batch number: 68, Loss: 15589.4814453125\n",
      "Epoch: 35, Batch number: 16, Loss: 15704.1474609375\n",
      "Epoch: 36, Batch number: 40, Loss: 15447.8447265625\n",
      "Epoch: 37, Batch number: 64, Loss: 15695.54296875\n",
      "Epoch: 39, Batch number: 12, Loss: 15217.443359375\n",
      "Epoch: 40, Batch number: 36, Loss: 15018.1923828125\n",
      "Epoch: 41, Batch number: 60, Loss: 15082.751953125\n",
      "Epoch: 43, Batch number: 8, Loss: 15023.9814453125\n",
      "Epoch: 44, Batch number: 32, Loss: 15059.650390625\n",
      "Epoch: 45, Batch number: 56, Loss: 15020.6591796875\n",
      "Epoch: 47, Batch number: 4, Loss: 15029.369140625\n",
      "Epoch: 48, Batch number: 28, Loss: 14991.7421875\n",
      "Epoch: 49, Batch number: 52, Loss: 15088.4443359375\n",
      "Epoch: 51, Batch number: 0, Loss: 14639.7880859375\n",
      "Epoch: 52, Batch number: 24, Loss: 15006.3349609375\n",
      "Epoch: 53, Batch number: 48, Loss: 14884.177734375\n",
      "Epoch: 54, Batch number: 72, Loss: 15388.2861328125\n",
      "Epoch: 56, Batch number: 20, Loss: 14824.900390625\n",
      "Epoch: 57, Batch number: 44, Loss: 14748.8515625\n",
      "Epoch: 58, Batch number: 68, Loss: 14889.259765625\n",
      "Epoch: 60, Batch number: 16, Loss: 14695.2646484375\n",
      "Epoch: 61, Batch number: 40, Loss: 14550.8837890625\n",
      "Epoch: 62, Batch number: 64, Loss: 15365.69921875\n",
      "Epoch: 64, Batch number: 12, Loss: 14447.0400390625\n",
      "Epoch: 65, Batch number: 36, Loss: 14418.3974609375\n",
      "Epoch: 66, Batch number: 60, Loss: 14610.37109375\n",
      "Epoch: 68, Batch number: 8, Loss: 14215.708984375\n",
      "Epoch: 69, Batch number: 32, Loss: 14337.408203125\n",
      "Epoch: 70, Batch number: 56, Loss: 14440.5107421875\n",
      "Epoch: 72, Batch number: 4, Loss: 14614.396484375\n",
      "Epoch: 73, Batch number: 28, Loss: 14342.9228515625\n",
      "Epoch: 74, Batch number: 52, Loss: 14772.666015625\n",
      "Epoch: 76, Batch number: 0, Loss: 14157.046875\n",
      "Epoch: 77, Batch number: 24, Loss: 14226.9306640625\n",
      "Epoch: 78, Batch number: 48, Loss: 14595.291015625\n",
      "Epoch: 79, Batch number: 72, Loss: 14180.0703125\n",
      "Epoch: 81, Batch number: 20, Loss: 14014.7275390625\n",
      "Epoch: 82, Batch number: 44, Loss: 14658.42578125\n",
      "Epoch: 83, Batch number: 68, Loss: 14506.345703125\n",
      "Epoch: 85, Batch number: 16, Loss: 14465.3603515625\n",
      "Epoch: 86, Batch number: 40, Loss: 14547.7548828125\n",
      "Epoch: 87, Batch number: 64, Loss: 14306.462890625\n",
      "Epoch: 89, Batch number: 12, Loss: 14127.279296875\n",
      "Epoch: 90, Batch number: 36, Loss: 14120.0087890625\n",
      "Epoch: 91, Batch number: 60, Loss: 14053.6103515625\n",
      "Epoch: 93, Batch number: 8, Loss: 14170.7431640625\n",
      "Epoch: 94, Batch number: 32, Loss: 13751.9716796875\n",
      "Epoch: 95, Batch number: 56, Loss: 13984.1708984375\n",
      "Epoch: 97, Batch number: 4, Loss: 14031.5927734375\n",
      "Epoch: 98, Batch number: 28, Loss: 13877.796875\n",
      "Epoch: 99, Batch number: 52, Loss: 14101.82421875\n",
      "Epoch: 101, Batch number: 0, Loss: 14166.6767578125\n",
      "Epoch: 102, Batch number: 24, Loss: 14209.578125\n",
      "Epoch: 103, Batch number: 48, Loss: 14286.58984375\n",
      "Epoch: 104, Batch number: 72, Loss: 14401.3671875\n",
      "Epoch: 106, Batch number: 20, Loss: 13863.55078125\n",
      "Epoch: 107, Batch number: 44, Loss: 14207.1474609375\n",
      "Epoch: 108, Batch number: 68, Loss: 14035.6015625\n",
      "Epoch: 110, Batch number: 16, Loss: 13745.5341796875\n",
      "Epoch: 111, Batch number: 40, Loss: 13664.4501953125\n",
      "Epoch: 112, Batch number: 64, Loss: 14267.6357421875\n",
      "Epoch: 114, Batch number: 12, Loss: 13678.86328125\n",
      "Epoch: 115, Batch number: 36, Loss: 14080.0615234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116, Batch number: 60, Loss: 13986.1533203125\n",
      "Epoch: 118, Batch number: 8, Loss: 14093.9365234375\n",
      "Epoch: 119, Batch number: 32, Loss: 13874.2470703125\n",
      "Epoch: 120, Batch number: 56, Loss: 14105.8427734375\n",
      "Epoch: 122, Batch number: 4, Loss: 13747.775390625\n",
      "Epoch: 123, Batch number: 28, Loss: 14035.0458984375\n",
      "Epoch: 124, Batch number: 52, Loss: 13491.0048828125\n",
      "Epoch: 126, Batch number: 0, Loss: 13697.974609375\n",
      "Epoch: 127, Batch number: 24, Loss: 14305.349609375\n",
      "Epoch: 128, Batch number: 48, Loss: 13749.0927734375\n",
      "Epoch: 129, Batch number: 72, Loss: 13714.6162109375\n",
      "Epoch: 131, Batch number: 20, Loss: 13856.96484375\n",
      "Epoch: 132, Batch number: 44, Loss: 14128.2998046875\n",
      "Epoch: 133, Batch number: 68, Loss: 13912.5244140625\n",
      "Epoch: 135, Batch number: 16, Loss: 13731.01171875\n",
      "Epoch: 136, Batch number: 40, Loss: 13410.1953125\n",
      "Epoch: 137, Batch number: 64, Loss: 13508.7470703125\n",
      "Epoch: 139, Batch number: 12, Loss: 13828.42578125\n",
      "Epoch: 140, Batch number: 36, Loss: 13949.0068359375\n",
      "Epoch: 141, Batch number: 60, Loss: 13579.3671875\n",
      "Epoch: 143, Batch number: 8, Loss: 13360.8798828125\n",
      "Epoch: 144, Batch number: 32, Loss: 14082.59765625\n",
      "Epoch: 145, Batch number: 56, Loss: 13515.990234375\n",
      "Epoch: 147, Batch number: 4, Loss: 13419.3310546875\n",
      "Epoch: 148, Batch number: 28, Loss: 13862.8203125\n",
      "Epoch: 149, Batch number: 52, Loss: 13651.5712890625\n",
      "Epoch: 151, Batch number: 0, Loss: 13786.7998046875\n",
      "Epoch: 152, Batch number: 24, Loss: 13577.0830078125\n",
      "Epoch: 153, Batch number: 48, Loss: 13641.037109375\n",
      "Epoch: 154, Batch number: 72, Loss: 13855.1513671875\n",
      "Epoch: 156, Batch number: 20, Loss: 13917.07421875\n",
      "Epoch: 157, Batch number: 44, Loss: 13782.7001953125\n",
      "Epoch: 158, Batch number: 68, Loss: 13909.267578125\n",
      "Epoch: 160, Batch number: 16, Loss: 13497.73828125\n",
      "Epoch: 161, Batch number: 40, Loss: 13719.578125\n",
      "Epoch: 162, Batch number: 64, Loss: 14129.1826171875\n",
      "Epoch: 164, Batch number: 12, Loss: 13410.3740234375\n",
      "Epoch: 165, Batch number: 36, Loss: 13997.6865234375\n",
      "Epoch: 166, Batch number: 60, Loss: 13894.6650390625\n",
      "Epoch: 168, Batch number: 8, Loss: 13969.8232421875\n",
      "Epoch: 169, Batch number: 32, Loss: 13593.7841796875\n",
      "Epoch: 170, Batch number: 56, Loss: 14359.2646484375\n",
      "Epoch: 172, Batch number: 4, Loss: 13473.9443359375\n",
      "Epoch: 173, Batch number: 28, Loss: 13279.3095703125\n",
      "Epoch: 174, Batch number: 52, Loss: 13863.5693359375\n",
      "Epoch: 176, Batch number: 0, Loss: 13081.0595703125\n",
      "Epoch: 177, Batch number: 24, Loss: 13715.0673828125\n",
      "Epoch: 178, Batch number: 48, Loss: 13963.78125\n",
      "Epoch: 179, Batch number: 72, Loss: 13876.5673828125\n",
      "Epoch: 181, Batch number: 20, Loss: 14054.439453125\n",
      "Epoch: 182, Batch number: 44, Loss: 13631.7490234375\n",
      "Epoch: 183, Batch number: 68, Loss: 13355.408203125\n",
      "Epoch: 185, Batch number: 16, Loss: 13648.8095703125\n",
      "Epoch: 186, Batch number: 40, Loss: 13816.7099609375\n",
      "Epoch: 187, Batch number: 64, Loss: 14114.8798828125\n",
      "Epoch: 189, Batch number: 12, Loss: 13326.2060546875\n",
      "Epoch: 190, Batch number: 36, Loss: 13705.66015625\n",
      "Epoch: 191, Batch number: 60, Loss: 14130.2001953125\n",
      "Epoch: 193, Batch number: 8, Loss: 13345.47265625\n",
      "Epoch: 194, Batch number: 32, Loss: 13434.197265625\n",
      "Epoch: 195, Batch number: 56, Loss: 13524.1123046875\n",
      "Epoch: 197, Batch number: 4, Loss: 13921.001953125\n",
      "Epoch: 198, Batch number: 28, Loss: 13264.681640625\n",
      "Epoch: 199, Batch number: 52, Loss: 13460.060546875\n",
      "Epoch: 201, Batch number: 0, Loss: 13651.5390625\n",
      "Epoch: 202, Batch number: 24, Loss: 13829.142578125\n",
      "Epoch: 203, Batch number: 48, Loss: 13311.6953125\n",
      "Epoch: 204, Batch number: 72, Loss: 13170.306640625\n",
      "Epoch: 206, Batch number: 20, Loss: 13610.93359375\n",
      "Epoch: 207, Batch number: 44, Loss: 13910.1904296875\n",
      "Epoch: 208, Batch number: 68, Loss: 13692.6240234375\n",
      "Epoch: 210, Batch number: 16, Loss: 13148.73828125\n",
      "Epoch: 211, Batch number: 40, Loss: 13894.16796875\n",
      "Epoch: 212, Batch number: 64, Loss: 13280.0\n",
      "Epoch: 214, Batch number: 12, Loss: 13436.2900390625\n",
      "Epoch: 215, Batch number: 36, Loss: 13264.365234375\n",
      "Epoch: 216, Batch number: 60, Loss: 13638.0859375\n",
      "Epoch: 218, Batch number: 8, Loss: 13625.4755859375\n",
      "Epoch: 219, Batch number: 32, Loss: 13673.376953125\n",
      "Epoch: 220, Batch number: 56, Loss: 13492.1767578125\n",
      "Epoch: 222, Batch number: 4, Loss: 13280.4453125\n",
      "Epoch: 223, Batch number: 28, Loss: 13517.1240234375\n",
      "Epoch: 224, Batch number: 52, Loss: 13926.3603515625\n",
      "Epoch: 226, Batch number: 0, Loss: 13250.166015625\n",
      "Epoch: 227, Batch number: 24, Loss: 13383.21875\n",
      "Epoch: 228, Batch number: 48, Loss: 13722.6650390625\n",
      "Epoch: 229, Batch number: 72, Loss: 13851.583984375\n",
      "Epoch: 231, Batch number: 20, Loss: 13340.8251953125\n",
      "Epoch: 232, Batch number: 44, Loss: 13439.5\n",
      "Epoch: 233, Batch number: 68, Loss: 13898.4072265625\n",
      "Epoch: 235, Batch number: 16, Loss: 13456.5634765625\n",
      "Epoch: 236, Batch number: 40, Loss: 13531.5283203125\n",
      "Epoch: 237, Batch number: 64, Loss: 13692.056640625\n",
      "Epoch: 239, Batch number: 12, Loss: 13661.375\n",
      "Epoch: 240, Batch number: 36, Loss: 13541.7880859375\n",
      "Epoch: 241, Batch number: 60, Loss: 13739.1318359375\n",
      "Epoch: 243, Batch number: 8, Loss: 13426.8916015625\n",
      "Epoch: 244, Batch number: 32, Loss: 13290.5166015625\n",
      "Epoch: 245, Batch number: 56, Loss: 13711.0166015625\n",
      "Epoch: 247, Batch number: 4, Loss: 13945.69921875\n",
      "Epoch: 248, Batch number: 28, Loss: 13227.037109375\n",
      "Epoch: 249, Batch number: 52, Loss: 13340.7294921875\n",
      "Epoch: 251, Batch number: 0, Loss: 13574.0869140625\n",
      "Epoch: 252, Batch number: 24, Loss: 13938.3974609375\n",
      "Epoch: 253, Batch number: 48, Loss: 14126.083984375\n",
      "Epoch: 254, Batch number: 72, Loss: 13506.0478515625\n",
      "Epoch: 256, Batch number: 20, Loss: 13150.81640625\n",
      "Epoch: 257, Batch number: 44, Loss: 13803.82421875\n",
      "Epoch: 258, Batch number: 68, Loss: 13628.6708984375\n",
      "Epoch: 260, Batch number: 16, Loss: 13639.7607421875\n",
      "Epoch: 261, Batch number: 40, Loss: 13453.36328125\n",
      "Epoch: 262, Batch number: 64, Loss: 13373.6513671875\n",
      "Epoch: 264, Batch number: 12, Loss: 13602.8857421875\n",
      "Epoch: 265, Batch number: 36, Loss: 13200.970703125\n",
      "Epoch: 266, Batch number: 60, Loss: 13574.7080078125\n",
      "Epoch: 268, Batch number: 8, Loss: 13372.5712890625\n",
      "Epoch: 269, Batch number: 32, Loss: 13791.791015625\n",
      "Epoch: 270, Batch number: 56, Loss: 13601.7021484375\n",
      "Epoch: 272, Batch number: 4, Loss: 13657.583984375\n",
      "Epoch: 273, Batch number: 28, Loss: 13793.3125\n",
      "Epoch: 274, Batch number: 52, Loss: 13842.9052734375\n",
      "Epoch: 276, Batch number: 0, Loss: 13326.6396484375\n",
      "Epoch: 277, Batch number: 24, Loss: 13098.236328125\n",
      "Epoch: 278, Batch number: 48, Loss: 13435.373046875\n",
      "Epoch: 279, Batch number: 72, Loss: 13353.388671875\n",
      "Epoch: 281, Batch number: 20, Loss: 13613.16796875\n",
      "Epoch: 282, Batch number: 44, Loss: 13219.9794921875\n",
      "Epoch: 283, Batch number: 68, Loss: 13566.947265625\n",
      "Epoch: 285, Batch number: 16, Loss: 13658.8681640625\n",
      "Epoch: 286, Batch number: 40, Loss: 13525.20703125\n",
      "Epoch: 287, Batch number: 64, Loss: 13893.166015625\n",
      "Epoch: 289, Batch number: 12, Loss: 13661.03515625\n",
      "Epoch: 290, Batch number: 36, Loss: 12873.4638671875\n",
      "Epoch: 291, Batch number: 60, Loss: 13767.2568359375\n",
      "Epoch: 293, Batch number: 8, Loss: 13392.0283203125\n",
      "Epoch: 294, Batch number: 32, Loss: 13417.2890625\n",
      "Epoch: 295, Batch number: 56, Loss: 13954.8740234375\n",
      "Epoch: 297, Batch number: 4, Loss: 13128.2236328125\n",
      "Epoch: 298, Batch number: 28, Loss: 13767.6962890625\n",
      "Epoch: 299, Batch number: 52, Loss: 13792.4873046875\n",
      "Training finished\n",
      "\n",
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0005\n",
      "Number of epochs: 300\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 26964.8515625\n",
      "Epoch: 2, Batch number: 24, Loss: 24349.87109375\n",
      "Epoch: 3, Batch number: 48, Loss: 22843.26171875\n",
      "Epoch: 4, Batch number: 72, Loss: 21870.65625\n",
      "Epoch: 6, Batch number: 20, Loss: 20361.875\n",
      "Epoch: 7, Batch number: 44, Loss: 19884.900390625\n",
      "Epoch: 8, Batch number: 68, Loss: 19499.068359375\n",
      "Epoch: 10, Batch number: 16, Loss: 17781.658203125\n",
      "Epoch: 11, Batch number: 40, Loss: 17697.7265625\n",
      "Epoch: 12, Batch number: 64, Loss: 17400.599609375\n",
      "Epoch: 14, Batch number: 12, Loss: 16583.44921875\n",
      "Epoch: 15, Batch number: 36, Loss: 16825.767578125\n",
      "Epoch: 16, Batch number: 60, Loss: 16738.478515625\n",
      "Epoch: 18, Batch number: 8, Loss: 16499.345703125\n",
      "Epoch: 19, Batch number: 32, Loss: 16058.798828125\n",
      "Epoch: 20, Batch number: 56, Loss: 16012.3203125\n",
      "Epoch: 22, Batch number: 4, Loss: 15518.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Batch number: 28, Loss: 15423.177734375\n",
      "Epoch: 24, Batch number: 52, Loss: 15577.7978515625\n",
      "Epoch: 26, Batch number: 0, Loss: 15248.087890625\n",
      "Epoch: 27, Batch number: 24, Loss: 15080.1474609375\n",
      "Epoch: 28, Batch number: 48, Loss: 15128.162109375\n",
      "Epoch: 29, Batch number: 72, Loss: 15399.85546875\n",
      "Epoch: 31, Batch number: 20, Loss: 14881.3017578125\n",
      "Epoch: 32, Batch number: 44, Loss: 15112.5302734375\n",
      "Epoch: 33, Batch number: 68, Loss: 14794.3408203125\n",
      "Epoch: 35, Batch number: 16, Loss: 15195.9267578125\n",
      "Epoch: 36, Batch number: 40, Loss: 14920.0419921875\n",
      "Epoch: 37, Batch number: 64, Loss: 15060.4833984375\n",
      "Epoch: 39, Batch number: 12, Loss: 14538.0712890625\n",
      "Epoch: 40, Batch number: 36, Loss: 14823.2626953125\n",
      "Epoch: 41, Batch number: 60, Loss: 14857.0322265625\n",
      "Epoch: 43, Batch number: 8, Loss: 14619.412109375\n",
      "Epoch: 44, Batch number: 32, Loss: 14642.869140625\n",
      "Epoch: 45, Batch number: 56, Loss: 14484.060546875\n",
      "Epoch: 47, Batch number: 4, Loss: 14670.509765625\n",
      "Epoch: 48, Batch number: 28, Loss: 14447.390625\n",
      "Epoch: 49, Batch number: 52, Loss: 14928.5439453125\n",
      "Epoch: 51, Batch number: 0, Loss: 14209.7080078125\n",
      "Epoch: 52, Batch number: 24, Loss: 14746.998046875\n",
      "Epoch: 53, Batch number: 48, Loss: 14283.783203125\n",
      "Epoch: 54, Batch number: 72, Loss: 14671.59375\n",
      "Epoch: 56, Batch number: 20, Loss: 14521.5517578125\n",
      "Epoch: 57, Batch number: 44, Loss: 14290.3037109375\n",
      "Epoch: 58, Batch number: 68, Loss: 14678.560546875\n",
      "Epoch: 60, Batch number: 16, Loss: 14124.3115234375\n",
      "Epoch: 61, Batch number: 40, Loss: 14049.8798828125\n",
      "Epoch: 62, Batch number: 64, Loss: 14266.3203125\n",
      "Epoch: 64, Batch number: 12, Loss: 13965.3115234375\n",
      "Epoch: 65, Batch number: 36, Loss: 13953.9375\n",
      "Epoch: 66, Batch number: 60, Loss: 14130.087890625\n",
      "Epoch: 68, Batch number: 8, Loss: 13824.2138671875\n",
      "Epoch: 69, Batch number: 32, Loss: 13675.8017578125\n",
      "Epoch: 70, Batch number: 56, Loss: 13650.3955078125\n",
      "Epoch: 72, Batch number: 4, Loss: 13635.2265625\n",
      "Epoch: 73, Batch number: 28, Loss: 14128.7822265625\n",
      "Epoch: 74, Batch number: 52, Loss: 14033.7041015625\n",
      "Epoch: 76, Batch number: 0, Loss: 13888.6884765625\n",
      "Epoch: 77, Batch number: 24, Loss: 13836.703125\n",
      "Epoch: 78, Batch number: 48, Loss: 13795.400390625\n",
      "Epoch: 79, Batch number: 72, Loss: 14104.060546875\n",
      "Epoch: 81, Batch number: 20, Loss: 13645.3583984375\n",
      "Epoch: 82, Batch number: 44, Loss: 13851.7734375\n",
      "Epoch: 83, Batch number: 68, Loss: 14026.1552734375\n",
      "Epoch: 85, Batch number: 16, Loss: 14129.16015625\n",
      "Epoch: 86, Batch number: 40, Loss: 14117.2861328125\n",
      "Epoch: 87, Batch number: 64, Loss: 13897.0224609375\n",
      "Epoch: 89, Batch number: 12, Loss: 13988.73046875\n",
      "Epoch: 90, Batch number: 36, Loss: 13498.365234375\n",
      "Epoch: 91, Batch number: 60, Loss: 14234.4306640625\n",
      "Epoch: 93, Batch number: 8, Loss: 14105.154296875\n",
      "Epoch: 94, Batch number: 32, Loss: 13940.677734375\n",
      "Epoch: 95, Batch number: 56, Loss: 13509.9814453125\n",
      "Epoch: 97, Batch number: 4, Loss: 13541.7744140625\n",
      "Epoch: 98, Batch number: 28, Loss: 13712.697265625\n",
      "Epoch: 99, Batch number: 52, Loss: 13810.0078125\n",
      "Epoch: 101, Batch number: 0, Loss: 14000.986328125\n",
      "Epoch: 102, Batch number: 24, Loss: 13583.78125\n",
      "Epoch: 103, Batch number: 48, Loss: 13544.4375\n",
      "Epoch: 104, Batch number: 72, Loss: 13928.05078125\n",
      "Epoch: 106, Batch number: 20, Loss: 13629.3251953125\n",
      "Epoch: 107, Batch number: 44, Loss: 14027.0078125\n",
      "Epoch: 108, Batch number: 68, Loss: 13816.78125\n",
      "Epoch: 110, Batch number: 16, Loss: 13607.9873046875\n",
      "Epoch: 111, Batch number: 40, Loss: 13929.287109375\n",
      "Epoch: 112, Batch number: 64, Loss: 13775.44140625\n",
      "Epoch: 114, Batch number: 12, Loss: 13750.7109375\n",
      "Epoch: 115, Batch number: 36, Loss: 13980.560546875\n"
     ]
    }
   ],
   "source": [
    "algorithm = 'Adam'\n",
    "epochs = 300\n",
    "sample_loss_every = 100\n",
    "learning_rate = 5e-4\n",
    "\n",
    "for trainer_list in sk_trainers:\n",
    "    for trainer in trainer_list:\n",
    "        trainer.Train(algorithm=algorithm, epochs=epochs, sample_loss_every=sample_loss_every, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (5., 4.)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for trainer_list in sk_trainers:\n",
    "    for trainer in trainer_list:\n",
    "        ax.plot(trainer.loss_history['iter'],trainer.loss_history['loss'],\n",
    "                label='ws={}, ed={}'.format(trainer.window_size, trainer.embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW trainer created:\n",
      "Window size: 2\n",
      "Number of samples: 10\n",
      "Vocabulary Size: 2\n",
      "Number of batches: 1\n",
      "Number of samples per batch: 64\n",
      "\n",
      "Dispositivo seleccionado: cpu\n",
      "Dimensión del espacio de los embeddings: 10\n"
     ]
    }
   ],
   "source": [
    "#corpus = [['w1', 'w2', 'w3', 'w4'], ['w1', 'w3', 'w3', 'w3'], ['w1'], ['w1', 'w2', 'w3', 'w4', 'w1', 'w2', 'w3', 'w4']]\n",
    "corpus = GetTrainCorpus('./promptsl40.train')\n",
    "cutoff_freq = 0\n",
    "window_size_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "batch_size = 512\n",
    "\n",
    "state_dict = None\n",
    "device = 'cuda:0'\n",
    "paralelize = False\n",
    "embedding_dim_list = [50, 100, 150, 200, 300, 400]\n",
    "\n",
    "cbow_trainers = []\n",
    "for window_size in window_size_list:\n",
    "    embedding_dim_trainers = []\n",
    "    for embedding_dim in embedding_dim_list:\n",
    "        cbow_trainer = CBOWTrainer(corpus, cutoff_freq, window_size, batch_size)\n",
    "        cbow_trainer.InitModel(state_dict=state_dict, device=device, paralelize=paralelize, embedding_dim=embedding_dim)\n",
    "        embedding_dim_trainers.append(cbow_trainer)\n",
    "    cbow_trainers.append(embedding_dim_trainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Optimization method: SGD\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 1\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 7.79583740234375\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algorithm = 'Adam'\n",
    "epochs = 300\n",
    "sample_loss_every = 100\n",
    "learning_rate = 5e-4\n",
    "\n",
    "for trainer_list in cbow_trainers:\n",
    "    for trainer in trainer_list:\n",
    "        trainer.Train(algorithm=algorithm, epochs=epochs, sample_loss_every=sample_loss_every, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
