{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NM-grams\n",
    "\n",
    "La tarea de modelo de lenguaje consiste en obtener la probabilidad $P(w_1,\\ldots,w_T)$ de obtener una secuencia de palabras $w_1,\\ldots,w_T$ pertenecientes a un vocabulario $V$. Esta probabilidad suele desglosarse de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_1,\\ldots,w_T) &= P(w_T|w_{T-1},\\ldots,w_1)P(w_{T-1},\\ldots,w_1) \\\\\n",
    "&= P(w_T|w_{T-1},\\ldots,w_1)P(w_{T-1}|w_{T-2},\\ldots,w_1)P(w_{T-2},\\ldots,w_1) \\\\\n",
    "& \\cdots \\\\\n",
    "&= P(w_1) \\prod_{i=0}^{T-2} P(w_{T-i}|w_{T-i-1}\\ldots w_{1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "De esta manera, hacer un modelo de N-gramas consiste en aproximar las probabilidades \n",
    "\n",
    "$$\n",
    "P(w_{T-i}|w_{T-i-1}\\ldots w_{1}) \\approx P(w_{T-i}|w_{T-i-1}\\ldots w_{T-i-N+1})\n",
    "$$\n",
    "\n",
    "bajo la suposición de que ésta depende unicamente de los N-1 instantes de tiempo anteriores. Luego, es posible obtener un método (ya sea frecuentista, con redes neuronales o lo que sea) con el cual estimar estas probabilidades entrenándolas con un corpus de texto cualquiera. \n",
    "\n",
    "Los modelos de N-gramas se usan en muchos lugares, directa o indirectamente. Inclusive, en modelos para hacer word vectors con métodos distribucionales frecuentistas o neuronales también se hace esta aproximación. El modelo Skipgram usa una aproximación de bigramas entrenada a partir de muestras de palabras centrales y sus contextos. El modelo de PPMI cuenta la cantidad de palabras que aparecieron en determinados contextos y obtiene una probabilidad de ocurrencia $P(w_c|w_o)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a hacer una generalización de este modelo y vamos a comparar los diferentes métodos con los que es posible estimar las probabilidades de este nuevo modelo.\n",
    "\n",
    "Definimos un modelo de NM-gramas de manera similar al anterior, con la salvedad que ahora vamos a querer desglosar la probabilidad $P(w_1,\\ldots,w_T)$ anterior de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_1,\\ldots,w_T) &= P(w_T,\\ldots,w_{T-M}|w_{T-M-1},\\ldots,w_{1}) P(w_{T-M-1},\\ldots,w_{1}) \\\\\n",
    "&= P(w_T,\\ldots,w_{T-M}|w_{T-M-1},\\ldots,w_{1}) P(w_{T-M-1},\\ldots,w_{T-2M-1}|w_{T-2M-2},\\ldots,w_{1})\n",
    "P(w_{T-2M-2},\\ldots,w_{1})\\\\\n",
    "\\cdots \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "De hecho, este modelo se puede generalizar para un desgloce no regular de los NM-gramas. Por ejemplo, supongamos que $T=10$. Una posibilidad es aproximar $P(w_1,\\ldots,w_{10})$ como sigue:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_1,\\ldots,w_{10}) &= P(w_{10}|w_9,\\ldots,w_1)P(w_9,\\ldots,w_1) \\\\\n",
    "&\\approx P(w_{10}|w_9)P(w_9,w_8|w_7,\\ldots,w_1)P(w_7,\\ldots,w_1) \\\\\n",
    "&\\approx P(w_{10}|w_9)P(w_9,w_8|w_7,w_6)P(w_7,w_6|w_5\\ldots,w_1)P(w_5\\ldots,w_1) \\\\\n",
    "&\\approx P(w_{10}|w_9)P(w_9,w_8|w_7,w_6)P(w_7|w_5)P(w_6|w_5)P(w_5|w_4,w_3)P(w_4,\\ldots,w_1) \\\\\n",
    "&\\approx P(w_{10}|w_9)P(w_9,w_8|w_7,w_6)P(w_7|w_5)P(w_6|w_5)P(w_5|w_4,w_3)P(w_4,w_3|w_1)P(w_2|w_1)P(w_1) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Esto se basa en que podemos utilizar las siguientes igualdades o aproximaciones según convenga:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_t,\\ldots,w_1) &= P(w_t|w_{t-1},\\ldots,w_1)P(w_{t-1},\\ldots,w_1) \\\\\n",
    "P(w_t|w_{t-1},\\ldots,w_1) &\\approx P(w_t|w_{t-1},\\ldots,w_{t-1-N}) \\\\\n",
    "P(w_t,\\ldots,w_1) &= P(w_t,\\ldots,w_{t-M}|w_{t-M-1},\\ldots,w_1)P(w_{t-M-1},\\ldots,w_1) \\\\\n",
    "P(w_t,\\ldots,w_{t-M}|w_{t-M-1},\\ldots,w_1) &\\approx P(w_t,\\ldots,w_{t-M}|w_{t-M-1},\\ldots,w_{t-M-N-1}) \\\\\n",
    "P(w_t,\\ldots,w_{t-M}|w_{t-M-1},\\ldots,w_1) &\\approx P(w_t|w_{t-M-1},\\ldots,w_1),\\ldots,P(w_{t-M}|w_{t-M-1},\\ldots,w_1) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notamos que en algunas hemos utilizado la aproximación de N-gramas, en otras independencia condicional y en otras una combinación de ambas. El ojetivo, a continuación, sería obtener las muestras para entrenar estas probabilidades a partir de un corpus de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def NgramWindowBasedDataset(corpus,left_window=2,right_window=1,unk_token='<UNK>'):\n",
    "    pass\n",
    "\n",
    "def NMgramWindowBasedDataset(corpus,m_gram=1,left_window=2,right_window=1,unk_token='<UNK>'):\n",
    "    pass\n",
    "\n",
    "class NgramWindowBasedDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Muestras de N-gramas obtenidas de un corpus de texto con una \n",
    "    ventana fija.\n",
    "    \n",
    "    Argumentos:\n",
    "        * corpus_idx: lista de lista de enteros que contienen los \n",
    "        índices de cada palabra del vocabulario.\n",
    "        * left_n / right_n: Cantidad de palabras a la izquierda /\n",
    "        derecha de la palabra central.\n",
    "        * unk_idx: valor del ínidice a ser tomado por el modelo\n",
    "        como índice desconocido.\n",
    "        \n",
    "    Devuelve:\n",
    "        * __getitem__(self,idx): devuelve la muestra idx del dataset.\n",
    "        * __len__(self): devuelve la cantidad de muestras del dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,corpus_idx,left_n,right_n, unk_idx):\n",
    "        self.words, self.contexts = self.get_samples(corpus_idx,left_n,right_n, unk_idx)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.words[idx], self.contexts[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def get_samples(self, corpus_idx, left_n, right_n, unk_idx=-1):\n",
    "        unk_token_idx = unk_idx\n",
    "        context_size = left_n + right_n\n",
    "        words = []\n",
    "        contexts = []\n",
    "        for doc in corpus_idx:\n",
    "            for i in range(left_n):\n",
    "                doc.insert(0,unk_token_idx)\n",
    "            for i in range(right_n):\n",
    "                doc.append(unk_token_idx)\n",
    "            for i, idx in enumerate(doc[left_n:-right_n],left_n):\n",
    "                words.append(idx)\n",
    "                contexts.append(doc[i-left_n:i] + doc[i+1:i+right_n+1])\n",
    "            for i in range(left_n):\n",
    "                doc.pop(0)\n",
    "            for i in range(right_n):\n",
    "                doc.pop(-1)\n",
    "        words = torch.tensor(words).view(-1,1).repeat(1,context_size).view(-1)\n",
    "        contexts = torch.tensor(contexts).view(-1)\n",
    "        mask = (words != unk_token_idx) * (contexts != unk_token_idx)\n",
    "        return words[mask], contexts[mask]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
