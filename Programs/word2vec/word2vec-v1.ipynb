{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import re\n",
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link de ayuda: [Word2Vec](https://rguigoures.github.io/word2vec_pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo.** Nuestro objetivo es utilizar un corpus de texto para entender el significado de los gramas que aparecen en el mismo. Para hacer eso, hay que definir un **modelo de lenguaje**, es decir una serie de reglas que nos van a conducir a entender el texto. \n",
    "\n",
    "El algoritmo *word2vec* define dos modelos de lenguaje distintos, pero basados en el hecho de que el significado de las palabras puede deducirse de su contexto (**hipótesis distribucional**). Esto se hace para entender el texto a través de su segmentación en palabras y, posteriormente, de la asignación de un vector de $\\mathbb{R}^{n}$ a cada palabra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "\n",
    "El primer algoritmo *word2vec* propone un modelo de lenguaje de la siguiente manera:\n",
    "\n",
    "* El corpus de texto con el que se trabaja consiste en una sucesión ordenada de palabras y/o signos de puntuación, tabulación, fin de página, etc., que para generalizar, llamaremos \"gramas\". \n",
    "\n",
    "* Los gramas que aparecen en el texto pertenecen a un vocabulario $V = \\{ w_1, w_2, \\ldots, w_{|V|} \\}$. Este vocabulario contiene a los gramas codificadss con *one-hot* vectors, es decir, \n",
    "\n",
    "$$\n",
    "w_{i_j} = \n",
    "\\begin{cases} \n",
    "  1 & \\mbox{si} & j = i \\\\\n",
    "  0 & \\mbox{si} & j \\neq i \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "* Se definen las variables aleatorias $\\mathbf{X}_{-m}, \\mathbf{X}_{-m+1}, \\ldots, \\mathbf{X}_{-1}, \\mathbf{X}_0, \\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{m}$, todas ellas con realizaciones en $V$, y para cada sucesión contigua de gramas encontrada en el texto, se tiene una probabilidad conjunta \n",
    "\n",
    "$$\n",
    "P(\\mathbf{X}_{-m} = x_{-m}, \\mathbf{X}_{-m+1} = x_{-m+1}, \\ldots, \\mathbf{X}_{-1} = x_{-1}, \\mathbf{X}_0 = x_{0}, \\mathbf{X}_{1} = x_1, \\ldots, \\mathbf{X}_{m-1} =x_{m-1}, \\mathbf{X}_{m} = x_{m})\n",
    "$$\n",
    "\n",
    "* Se desea estimar la probabilidad \n",
    "$$\n",
    "P(\\mathbf{X}_0 = w_i | \\mathbf{X}_{-m} = x_{-m}, \\mathbf{X}_{-m+1} = x_{-m+1}, \\ldots, \\mathbf{X}_{-1} = x_{-1}, \\mathbf{X}_{1} = x_1, \\ldots, \\mathbf{X}_{m-1} =x_{m-1}, \\mathbf{X}_{m} = x_{m})\n",
    "$$\n",
    "para todo $i=1,\\ldots,|V|$ y para cada conjunto posible de $x_{-m},\\ldots,x_{m}$ de vectores pertenecientes a $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "El modelo *Skip-gram* propone algo similar, aunque esta vez se busca estimar\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X}_{-m} = x_{-m}, \\mathbf{X}_{-m+1} = x_{-m+1}, \\ldots, \\mathbf{X}_{-1} = x_{-1}, \\mathbf{X}_{1} = x_1, \\ldots, \\mathbf{X}_{m-1} =x_{m-1}, \\mathbf{X}_{m} = x_{m} | \\mathbf{X}_0 = x_0)\n",
    "$$\n",
    "\n",
    "Si se asume idependencia condicional, esta probabilidad es igual a\n",
    "\n",
    "$$\n",
    "\\prod_{i=-m\\\\i\\neq 0}^{m} P(\\mathbf{X}_{i} = x_i | \\mathbf{X}_0 = x_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulación\n",
    "\n",
    "El corpus de texto *Brown* es un conjunto de archivos de texto divididos por categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/anaconda3/envs/TorchEnv/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown', download_dir='/home/lestien/anaconda3/envs/TorchEnv/nltk_data')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus categories:\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Corpus categories:')\n",
    "print(brown.categories())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un corpus para cada categoría. Un corpus es un conjunto de frases que aparecen en el conjunto de textos en la categoría. Un corpus es una lista de listas de palabras. Por ejemplo, para la categoría \"news\" se tiene el siguiente corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de frases en estas categorías:  4623\n",
      "\n",
      "Algunos ejemplos:\n",
      "\n",
      "Frase 1:\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "Frase 2:\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "\n",
      "Frase 3:\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "\n",
      "Frase 4:\n",
      "['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "\n",
      "Frase 5:\n",
      "['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['news']\n",
    "corpus_unpreproceced = brown.sents(categories=categories)\n",
    "print('Cantidad de frases en estas categorías: ',len(corpus_unpreproceced))\n",
    "print()\n",
    "\n",
    "print('Algunos ejemplos:')\n",
    "print()\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    print('Frase {}:'.format(i+1))\n",
    "    print(corpus_unpreproceced[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando texto...\n",
      "\n",
      "Nuevo texto:\n",
      "\n",
      "Frase 1:\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "\n",
      "Frase 2:\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "\n",
      "Frase 3:\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "\n",
      "Frase 4:\n",
      "['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "\n",
      "Frase 5:\n",
      "['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "print('Procesando texto...')\n",
    "for sentence in corpus_unpreproceced:\n",
    "    text = ' '.join(sentence)\n",
    "    text = text.lower()\n",
    "    text.replace('\\n', ' ')\n",
    "    text = re.sub('[^a-z ]+', '', text)\n",
    "    corpus.append([w for w in text.split() if w != ''])\n",
    "    \n",
    "print()\n",
    "print('Nuevo texto:')\n",
    "print()\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    print('Frase {}:'.format(i+1))\n",
    "    print(corpus_unpreproceced[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el vocabulario para este corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las muestras en forma de tupla `(contexto, palabra central)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de muestras disponibles:  87004\n"
     ]
    }
   ],
   "source": [
    "m = 2\n",
    "samples = []\n",
    "for sentence in corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        first_context_word_index = max(0,i-m)\n",
    "        last_context_word_index = min(i+m+1, len(sentence))\n",
    "        samples.append((sentence[first_context_word_index:i] + sentence[i+1:last_context_word_index], word))\n",
    "        \n",
    "print('Cantidad de muestras disponibles: ', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/lestien/anaconda3/envs/TorchEnv/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de muestas: 87004\n",
      "\n",
      "context:\n",
      "fulton\n",
      "county\n",
      "\n",
      "Palabra central:\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "class BrownDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, categories, root='./', preprocessing=None, context_size=2):\n",
    "        nltk.download('brown', download_dir=root)\n",
    "        from nltk.corpus import brown\n",
    "        self.corpus_unpreproceced = brown.sents(categories=categories)\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "        if self.preprocessing:\n",
    "            self.corpus = self.preprocessing(self.corpus_unpreproceced)\n",
    "        else:\n",
    "            self.corpus = self.corpus_unpreproceced\n",
    "        \n",
    "        self.vocabulary = set(itertools.chain.from_iterable(self.corpus))\n",
    "\n",
    "        self.word_to_index = {w: idx for (idx, w) in enumerate(self.vocabulary)}\n",
    "        self.index_to_word = {idx: w for (idx, w) in enumerate(self.vocabulary)}\n",
    "        \n",
    "        samples = []\n",
    "        for sentence in self.corpus:\n",
    "            for i, word in enumerate(sentence):\n",
    "                first_context_word_index = max(0,i-m)\n",
    "                last_context_word_index = min(i+m+1, len(sentence))\n",
    "                samples.append((sentence[first_context_word_index:i] + sentence[i+1:last_context_word_index], word))\n",
    "        \n",
    "        self.samples = samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        context, word = self.samples[idx]\n",
    "        idx_context = torch.empty(len(context), dtype=torch.float32)\n",
    "        idx_word = torch.tensor(self.word_to_index[word], dtype=torch.float32)\n",
    "        for i, w in enumerate(context):\n",
    "            idx_context[i] = self.word_to_index[w]\n",
    "\n",
    "        return idx_context, idx_word\n",
    "       \n",
    "\n",
    "        \n",
    "class PreprocessBrown(object):\n",
    "    \n",
    "    def __call__(self,corpus_unpreproceced):\n",
    "        corpus = []\n",
    "        for sentence in corpus_unpreproceced:\n",
    "            text = ' '.join(sentence)\n",
    "            text = text.lower()\n",
    "            text.replace('\\n', ' ')\n",
    "            text = re.sub('[^a-z ]+', '', text)\n",
    "            corpus.append([w for w in text.split() if w != ''])\n",
    "        return corpus\n",
    "\n",
    "\n",
    "categories = ['news']\n",
    "context_size = 2\n",
    "train_dataset = BrownDataset(categories=categories,\n",
    "                             root='/home/lestien/anaconda3/envs/TorchEnv/nltk_data',\n",
    "                             preprocessing=PreprocessBrown(),\n",
    "                             context_size=context_size)\n",
    "\n",
    "print('Cantidad de muestas:', len(train_dataset))\n",
    "print()\n",
    "c, w = train_dataset[0]\n",
    "print('context:')\n",
    "for i in c:\n",
    "    print(train_dataset.index_to_word[i.tolist()])\n",
    "print()\n",
    "print('Palabra central:')\n",
    "print(train_dataset.index_to_word[w.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora falta hacer el resto del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "\n",
    "\n",
    "val_size = .02\n",
    "batch_size = 64\n",
    "\n",
    "NUM_TRAIN = int((1 - val_size) * len(train_dataset))\n",
    "NUM_VAL = len(train_dataset) - NUM_TRAIN\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler=SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "val_dataloader = DataLoader(train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def CheckAccuracy(loader, model, device):  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        \n",
    "        return num_correct, num_samples\n",
    "        \n",
    "\n",
    "def train(model, data, epochs=1, learning_rate=1e-2, sample_loss_every=100):\n",
    "    \n",
    "    input_dtype = data['input_dtype'] \n",
    "    target_dtype = data['target_dtype']\n",
    "    device = data['device']\n",
    "    train_dataloader = data['train_dataloader']\n",
    "    val_dataloader = data['val_dataloader']\n",
    "    \n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for e in range(epochs):\n",
    "        for t, (x,y) in enumerate(train_dataloader):\n",
    "            x = x.to(device=device, dtype=input_dtype)\n",
    "            y = y.to(device=device, dtype=target_dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "\n",
    "            # Backward pass\n",
    "            loss = model.loss(scores,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % sample_loss_every == 0:\n",
    "                num_correct, num_samples = CheckAccuracy(val_dataloader, model, device)\n",
    "                performance_history['iter'].append(t)\n",
    "                performance_history['loss'].append(loss.item())\n",
    "                performance_history['accuracy'].append(float(num_correct) / num_samples)\n",
    "                print('Epoch: %d, Iteration: %d, Accuracy: %d/%d ' % (e, t, num_correct, num_samples))\n",
    "                \n",
    "    num_correct, num_samples = CheckAccuracy(val_dataloader, model, device)\n",
    "    print('Final accuracy: %.2f%%' % (100 * float(num_correct) / num_samples) )\n",
    "    \n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word)\n",
    "        hidden = self.linear(emb)\n",
    "        return out\n",
    "    \n",
    "    def loss(self, scores, target):\n",
    "        m = nn.CrossEntropyLoss()\n",
    "        return m(scores,target)\n",
    "    \n",
    "vocab_size = len(train_dataset.vocabulary)\n",
    "embedding_size = 50\n",
    "model = Word2Vec(vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 4 and 2 in dimension 1 at /opt/conda/conda-bld/pytorch_1570910687650/work/aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-f4cd2675b60b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Entrenamiento:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mperformance_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_loss_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-143-92961262cf53>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, learning_rate, sample_loss_every)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TorchEnv/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 4 and 2 in dimension 1 at /opt/conda/conda-bld/pytorch_1570910687650/work/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Especificaciones de cómo adquirir los datos para entrenamiento:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "data = {\n",
    "    'device': device,\n",
    "    'input_dtype': torch.float32, \n",
    "    'target_dtype': torch.long,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'val_dataloader': val_dataloader\n",
    "}\n",
    "\n",
    "# Hiperparámetros del modelo y otros:\n",
    "epochs = 1 # Cantidad de epochs\n",
    "sample_loss_every = 50 # Cantidad de iteraciones para calcular la cantidad de aciertos\n",
    "learning_rate = 1e-2 # Tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento:\n",
    "performance_history = train(model, data, epochs, learning_rate, sample_loss_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
