{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0].split('BecaNLP')[0],'BecaNLP/Utils'))\n",
    "\n",
    "import NLPUtils as nlp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from NLPUtils.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.126231670379639\n",
      "12.097833156585693\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "for comment, rate in imdb.read_train_it():\n",
    "    pass\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "\n",
    "tic = time.time()\n",
    "df = imdb.read_train()\n",
    "for idx, row in df.iterrows():\n",
    "    row.comment, row.rate\n",
    "toc = time.time()\n",
    "\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 s ± 160 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "12.2 s ± 87.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def func1():\n",
    "    for comment, rate in imdb.read_train_it():\n",
    "        pass\n",
    "    \n",
    "def func2():\n",
    "    df = imdb.read_train()\n",
    "    for idx, row in df.iterrows():\n",
    "        row.comment, row.rate\n",
    "        \n",
    "%timeit func1()\n",
    "%timeit func2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "corpus = 'No quiero que la palabra casa esté en mi vocabulario'\n",
    "vocab = list(set(corpus.split(' ')))\n",
    "vocab.pop(vocab.index('casa'))\n",
    "pattern = re.compile(r'^(?!.*).*$')\n",
    "\n",
    "for match in pattern.finditer(corpus):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7bdaae5c1e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \"\"\"\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [['esto', 'es', 'una', 'prueba'],\n",
    "          ['esto', 'es', 'otra', 'prueba']]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'esto': 3,\n",
       "         'es': 2,\n",
       "         'una': 1,\n",
       "         'prueba': 2,\n",
       "         'y': 2,\n",
       "         'otra': 1,\n",
       "         'no': 1,\n",
       "         ('esto', 'es'): 2,\n",
       "         ('es', 'una'): 1,\n",
       "         ('una', 'prueba'): 1,\n",
       "         ('prueba', 'y'): 2,\n",
       "         ('y', 'esto'): 2,\n",
       "         ('es', 'otra'): 1,\n",
       "         ('otra', 'prueba'): 1,\n",
       "         ('esto', 'no'): 1,\n",
       "         ('esto', 'es', 'una'): 1,\n",
       "         ('es', 'una', 'prueba'): 1,\n",
       "         ('una', 'prueba', 'y'): 1,\n",
       "         ('prueba', 'y', 'esto'): 2,\n",
       "         ('y', 'esto', 'es'): 1,\n",
       "         ('esto', 'es', 'otra'): 1,\n",
       "         ('es', 'otra', 'prueba'): 1,\n",
       "         ('otra', 'prueba', 'y'): 1,\n",
       "         ('y', 'esto', 'no'): 1})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import tee, islice, chain\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(lst, n):\n",
    "    tlst = lst\n",
    "    while True:\n",
    "        a, b = tee(tlst)\n",
    "        l = tuple(islice(a, n))\n",
    "        if len(l) == n:\n",
    "            yield l\n",
    "            next(b)\n",
    "            tlst = b\n",
    "        else:\n",
    "            break\n",
    "\n",
    "corpus = 'esto es una prueba y esto es otra prueba y esto no'\n",
    "words = corpus.split(' ')\n",
    "Counter(chain(words, ngrams(words, 2),ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 7, 6, 5, 4, 3, 2, 1] 10\n",
      "[10, 8, 7, 6, 5, 4, 3, 2, 1] 9\n",
      "[10, 9, 7, 6, 5, 4, 3, 2, 1] 8\n",
      "[10, 9, 8, 6, 5, 4, 3, 2, 1] 7\n",
      "[10, 9, 8, 7, 5, 4, 3, 2, 1] 6\n",
      "[10, 9, 8, 7, 6, 4, 3, 2, 1] 5\n",
      "[10, 9, 8, 7, 6, 5, 3, 2, 1] 4\n",
      "[10, 9, 8, 7, 6, 5, 4, 2, 1] 3\n",
      "[10, 9, 8, 7, 6, 5, 4, 3, 1] 2\n",
      "[10, 9, 8, 7, 6, 5, 4, 3, 2] 1\n"
     ]
    }
   ],
   "source": [
    "arr = [10,9,8,7,6,5,4,3,2,1]\n",
    "for i in range(1,11):\n",
    "    train = (arr[:i-1] + arr[i:])\n",
    "    dev = arr[i-1]\n",
    "    print(train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 8,  2, 10,  5, 11, 14,  1,  0,  4, 13,  9]),\n",
       "  array([ 3,  7, 12,  6])),\n",
       " (array([ 3,  7, 12,  6, 11, 14,  1,  0,  4, 13,  9]),\n",
       "  array([ 8,  2, 10,  5])),\n",
       " (array([ 3,  7, 12,  6,  8,  2, 10,  5,  4, 13,  9]),\n",
       "  array([11, 14,  1,  0])),\n",
       " (array([ 3,  7, 12,  6,  8,  2, 10,  5, 11, 14,  1,  0]),\n",
       "  array([ 4, 13,  9]))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(15)\n",
    "nlp.datasets.utils.split_dev_kfolds(len(arr),dev_size=.25,k_folds=4,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 3\n",
      "7 3\n",
      "8 2\n",
      "8 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'train': (<7x645 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 877 stored elements in Compressed Sparse Row format>,\n",
       "   array([ 1, 10,  1,  8,  8, 10,  1])),\n",
       "  'dev': (<3x645 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 173 stored elements in Compressed Sparse Row format>,\n",
       "   array([10,  1,  3]))},\n",
       " 1: {'train': (<7x597 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 815 stored elements in Compressed Sparse Row format>,\n",
       "   array([10,  1,  3,  8,  8, 10,  1])),\n",
       "  'dev': (<3x597 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 187 stored elements in Compressed Sparse Row format>,\n",
       "   array([ 1, 10,  1]))},\n",
       " 2: {'train': (<8x794 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 1105 stored elements in Compressed Sparse Row format>,\n",
       "   array([10,  1,  3,  1, 10,  1, 10,  1])),\n",
       "  'dev': (<2x794 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 101 stored elements in Compressed Sparse Row format>, array([8, 8]))},\n",
       " 3: {'train': (<8x833 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 1136 stored elements in Compressed Sparse Row format>,\n",
       "   array([10,  1,  3,  1, 10,  1,  8,  8])),\n",
       "  'dev': (<2x833 sparse matrix of type '<class 'numpy.int64'>'\n",
       "   \twith 109 stored elements in Compressed Sparse Row format>,\n",
       "   array([10,  1]))}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = imdb.train_reader().sample(n=10,replace=False,random_state=123)\n",
    "data = imdb.count_ngrams_and_vectorize(df,dev_size=.2,k_folds=4,\n",
    "                           random_state=0,labels_func=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and this', 'document', 'document is', 'first', 'first document', 'is', 'is the', 'is this', 'one', 'second', 'second document', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re \n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "\n",
    "def my_tokenizer(string,tokenizer_pattern,tk_to_idx):\n",
    "    tokens = re.split(tokenizer_pattern,string)\n",
    "    return [tk_to_idx.get(tk,'<UNK>') for tk in tokens]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'document',\n",
       " 'this is',\n",
       " 'is the',\n",
       " 'the first',\n",
       " 'first document']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "analyzer('This is the first document.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola is the first document.\n",
      "['This', 'is', 'the', 'first', 'document']\n"
     ]
    }
   ],
   "source": [
    "prep = vectorizer.build_preprocessor()\n",
    "print(prep('This is the first document.'))\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "print(tokenizer('This is the first document.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'(This)')\n",
    "\n",
    "pattern.findall('This is the first document.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Primero, tengo que definir cómo voy a repartir las muestras entre train y dev, o si voy a hacer k-folds\n",
    "\n",
    "```Python\n",
    "\n",
    "def split():\n",
    "    return list_of_indeces\n",
    "\n",
    "```\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_train_dataset(reader,vocab,ngram_range,tokenizer_pattern,labels_func):\n",
    "    dataset = {'X': X_train, 'y': y_train, 'vectorizer':vectorizer}\n",
    "    return dataset\n",
    "\n",
    "def build_ngram_test_dataset(reader,feature_vectorizer,labels_func):\n",
    "    feat_dicts = []\n",
    "    labels = []\n",
    "    for text, label in reader():\n",
    "        feat_dicts.append(re.split(tokenizer_pattern,text))\n",
    "        labels.append(labels_func(label))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_vectorizer(reader,vocab,ngram_range,tokenizer_pattern):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab,\n",
    "     ngram_range=ngram_range,tokenizer_pattern=tokenizer_pattern)\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    for text in reader():\n",
    "        tokens = re.split(tokenizer_pattern,text)\n",
    "        feat_dicts.append(vocab.get(tk,'<UNK>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
