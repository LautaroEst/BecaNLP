{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html\n",
    "\n",
    "Para instalarlo, se ejecut√≥:\n",
    "\n",
    "```\n",
    "pip install -U gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings pre-entrenados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpora': ['semeval-2016-2017-task3-subtaskBC',\n",
       "  'semeval-2016-2017-task3-subtaskA-unannotated',\n",
       "  'patent-2017',\n",
       "  'quora-duplicate-questions',\n",
       "  'wiki-english-20171001',\n",
       "  'text8',\n",
       "  'fake-news',\n",
       "  '20-newsgroups',\n",
       "  '__testing_matrix-synopsis',\n",
       "  '__testing_multipart-matrix-synopsis'],\n",
       " 'models': ['fasttext-wiki-news-subwords-300',\n",
       "  'conceptnet-numberbatch-17-06-300',\n",
       "  'word2vec-ruscorpora-300',\n",
       "  'word2vec-google-news-300',\n",
       "  'glove-wiki-gigaword-50',\n",
       "  'glove-wiki-gigaword-100',\n",
       "  'glove-wiki-gigaword-200',\n",
       "  'glove-wiki-gigaword-300',\n",
       "  'glove-twitter-25',\n",
       "  'glove-twitter-50',\n",
       "  'glove-twitter-100',\n",
       "  'glove-twitter-200',\n",
       "  '__testing_word2vec-matrix-synopsis']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info(name_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n",
      "semeval-2016-2017-task3-subtaskBC\n",
      "semeval-2016-2017-task3-subtaskA-unannotated\n",
      "patent-2017\n",
      "quora-duplicate-questions\n",
      "wiki-english-20171001\n",
      "text8\n",
      "fake-news\n",
      "20-newsgroups\n",
      "__testing_matrix-synopsis\n",
      "__testing_multipart-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "for key in api.info()['models'].keys():\n",
    "    print(key)\n",
    "for key in api.info()['corpora'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.1% 13.8/13.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model1 = api.load('20-newsgroups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model2 = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Word2VecKeyedVectors(vector_size)\n",
      " |  \n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method work only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7f9306cbeea0>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lestien/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "help(model2.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: db7n+@andrew.cmu.edu (D. Andrew Byler)\n",
      "Subject: Re: Serbian genocide Work of God?\n",
      "Organization: Freshman, Civil Engineering, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 61\n",
      "\n",
      "Vera Shanti Noyes writes;\n",
      "\n",
      ">this is what indicates to me that you may believe in predestination.\n",
      ">am i correct?  i do not believe in predestination -- i believe we all\n",
      ">choose whether or not we will accept God's gift of salvation to us.\n",
      ">again, fundamental difference which can't really be resolved.\n",
      "\n",
      "Of course I believe in Predestination.  It's a very biblical doctrine as\n",
      "Romans 8.28-30 shows (among other passages).  Furthermore, the Church\n",
      "has always taught predestination, from the very beginning.  But to say\n",
      "that I believe in Predestination does not mean I do not believe in free\n",
      "will.  Men freely choose the course of their life, which is also\n",
      "affected by the grace of God.  However, unlike the Calvinists and\n",
      "Jansenists, I hold that grace is resistable, otherwise you end up with\n",
      "the idiocy of denying the universal saving will of God (1 Timothy 2.4). \n",
      "For God must give enough grace to all to be saved.  But only the elect,\n",
      "who he foreknew, are predestined and receive the grace of final\n",
      "perserverance, which guarantees heaven.  This does not mean that those\n",
      "without that grace can't be saved, it just means that god foreknew their\n",
      "obstinacy and chose not to give it to them, knowing they would not need\n",
      "it, as they had freely chosen hell.\n",
      "\t\t\t\t\t\t\t  ^^^^^^^^^^^\n",
      "People who are saved are saved by the grace of God, and not by their own\n",
      "effort, for it was God who disposed them to Himself, and predestined\n",
      "them to become saints.  But those who perish in everlasting fire perish\n",
      "because they hardened their heart and chose to perish.  Thus, they were\n",
      "deserving of God;s punishment, as they had rejected their Creator, and\n",
      "sinned against the working of the Holy Spirit.\n",
      "\n",
      ">yes, it is up to God to judge.  but he will only mete out that\n",
      ">punishment at the last judgement. \n",
      "\n",
      "Well, I would hold that as God most certainly gives everybody some\n",
      "blessing for what good they have done (even if it was only a little),\n",
      "for those He can't bless in the next life, He blesses in this one.  And\n",
      "those He will not punish in the next life, will be chastised in this one\n",
      "or in Purgatory for their sins.  Every sin incurs some temporal\n",
      "punishment, thus, God will punish it unless satisfaction is made for it\n",
      "(cf. 2 Samuel 12.13-14, David's sin of Adultery and Murder were\n",
      "forgiven, but he was still punished with the death of his child.)  And I\n",
      "need not point out the idea of punishment because of God's judgement is\n",
      "quite prevelant in the Bible.  Sodom and Gommorrah, Moses barred from\n",
      "the Holy Land, the slaughter of the Cannanites, Annias and Saphira,\n",
      "Jerusalem in 70 AD, etc.\n",
      "\n",
      "> if jesus stopped the stoning of an adulterous woman (perhaps this is\n",
      "not a >good parallel, but i'm going to go with it anyway), why should we\n",
      "not >stop the murder and violation of people who may (or may not) be more\n",
      ">innocent?\n",
      "\n",
      "We should stop the slaughter of the innocent (cf Proverbs 24.11-12), but\n",
      "does that mean that Christians should support a war in Bosnia with the\n",
      "U.S. or even the U.N. involved?  I do not think so, but I am an\n",
      "isolationist, and disagree with foreign adventures in general.  But in\n",
      "the case of Bosnia, I frankly see no excuse for us getting militarily\n",
      "involved, it would not be a \"just war.\"  \"Blessed\" after all, \"are the\n",
      "peacemakers\" was what Our Lord said, not the interventionists.  Our\n",
      "actions in Bosnia must be for peace, and not for a war which is\n",
      "unrelated to anything to justify it for us.\n",
      "\n",
      "Andy Byler\n",
      "\n",
      "From: mpaul@unl.edu (marxhausen paul)\n",
      "Subject: Re: \"National repentance\"\n",
      "Organization: University of Nebraska--Lincoln\n",
      "Lines: 37\n",
      "\n",
      "mcovingt@aisun3.ai.uga.edu (Michael Covington) writes:\n",
      "\n",
      ">I heard on the radio today about a Christian student conference where\n",
      ">Christians were called to \"repent\" of America's \"national\" sins, such\n",
      ">as sexual promiscuity.\n",
      "\n",
      ">How can I repent of _someone else's_ sin?   I can't.\n",
      "\n",
      ">And when I claim to \"repent\" of someone else's sin, am I not in fact\n",
      ">_judging_ him?  Jesus equipped us to judge activities but warned us\n",
      ">not to judge people. \"Judge not that ye be not judged.\"\n",
      "\n",
      "Strictly speaking, you're right - we can't repent _for_ somebody else,\n",
      "for what they've done.  I guess I don't think it's out of line to talk\n",
      "about a generalized repentence for our contribution to or participation\n",
      "in \"The sins of society\" , or for our tacit approval (by our silence) of\n",
      "sinful attitudes or practices....it may be that we're also just plain\n",
      "begging for mercy, hoping God will withhold his hand of judgement on\n",
      "our whole country for the sake of a few, much as Abraham sought to do\n",
      "for the sake of Lot.  (Hmmm, the results there were pretty cautionary...)\n",
      "\n",
      "A few times lately when I've observed some either out-and-out sinful \n",
      "activity, or just some self-destructive activity, I've gotten a strong\n",
      "impression that many folks really don't know any better.  Christ's pity\n",
      "on the crowds as being \"like sheep without a shepherd\" rings true to me.\n",
      "If these folks don't have a clue, do I bear _any_ responsibility for my\n",
      "not having communicated a better way?  Worse still; have I expressed \n",
      "judgement and disgust at their doings, and thus alienated them from any \n",
      "positive relationship whereby I might pass along anything positive?  \n",
      "I _know_ I've got something to repent about on that score.\n",
      "\n",
      "Anyway, it's a real interesting question.  \n",
      "--\n",
      "paul marxhausen .... ....... ............. ............ ............ .......... \n",
      " .. . .  . . . university of nebraska - lincoln .  . . .. . .  .. . . . . . . .\n",
      " .     .    .  .   .     .   .  .    .   .  .   .    .   .  grace .   .    .  . \n",
      "   .         .       .      .        .        .      .        .   happens .     \n",
      "\n",
      "From: eggertj@moses.atc.ll.mit.edu (Jim Eggert x6127 g41)\n",
      "Subject: Re: DID HE REALLY RISE???\n",
      "Reply-To: eggertj@atc.ll.mit.edu\n",
      "Organization: MIT Lincoln Lab - Group 41\n",
      "Lines: 13\n",
      "\n",
      "In article <Apr.12.03.44.39.1993.18842@athos.rutgers.edu> reedr@cgsvax.claremont.edu writes:\n",
      ">   The real problem was\n",
      ">   that Christians were pacifist and preached there was only one god.  When the\n",
      ">   state operates by a system of divinitation of the emperor -  monotheism \n",
      ">   becomes a capital offense.  The Jews were able to get exemption from this,\n",
      ">   and were also not evangelistic.\n",
      "\n",
      "I disagree with your claim that Jews were not evangelistic (except in\n",
      "the narrow sense of the word).  Jewish proselytism was widespread.\n",
      "There are numerous accounts of Jewish proselytism, both in the New\n",
      "Testament and in Roman and Greek documents of the day.\n",
      "--\n",
      "=Jim  eggertj@atc.ll.mit.edu (Jim Eggert)\n",
      "\n",
      "From: tas@pegasus.com (Len Howard)\n",
      "Subject: Re: Can sin \"block\" our prayers?\n",
      "Organization: Pegasus,  Honolulu\n",
      "Lines: 24\n",
      "\n",
      "In article <Apr.12.03.45.11.1993.18872@athos.rutgers.edu> jayne@mmalt.guild.org (Jayne Kulikauskas) writes:\n",
      ">mike@boulder.snsc.unr.edu (Mike McCormick) writes:\n",
      ">\n",
      ">> Not honoring our wives can cause our prayers to be hindered:\n",
      ">>         prayers may not be hindered.  I Peter 3:7\n",
      ">\n",
      ">One interpretation I've heard of this verse is that it refers to the sin \n",
      ">of physically abusing one's wife.  The husband is usually physically \n",
      ">stronger than his wife but is not permitted to use this to dominate her.  \n",
      ">He must honor her as his sister in Christ.  This would therefore be an \n",
      ">example of a specific sin that blocks prayer.\n",
      ">Jayne Kulikauskas/ jayne@mmalt.guild.org\n",
      "\n",
      "I would be a bit more specific in looking at this verse in regard to\n",
      "'blocking' prayer.  I have trouble thinking that God would allow\n",
      "anything to block our access to him in prayer, especially if we have\n",
      "sinned and are praying for forgivenenss.\n",
      "    I can see, however, how our prayer life might be hindered by our\n",
      "sin, if we are concentrating on what is causing the sin or what has\n",
      "happened, we may not be thinking about prayer, thus our prayers are\n",
      "'hindered' by our own actions.\n",
      "    But I don't think anything can 'block' the transmission, or\n",
      "reception of prayer to God.\n",
      "Shalom,                                      Len Howard\n",
      "\n",
      "From: db7n+@andrew.cmu.edu (D. Andrew Byler)\n",
      "Subject: Re: Question about Virgin Mary\n",
      "Organization: Freshman, Civil Engineering, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 22\n",
      "\n",
      "D. Andrew Kille writes:\n",
      "\n",
      ">Just an observation- although the bodily assumption has no basis in\n",
      ">the Bible, Carl Jung declared it to be one of the most important\n",
      ">pronouncements\n",
      ">of the church in recent years, in that it implied the inclusion of the \n",
      ">feminine into the Godhead.\n",
      "\n",
      "Which means he has absolutely no idea about what the Assumption is.\n",
      "\n",
      "However greatly we extoll Mary, it is quite obvious that she is in no\n",
      "way God or even part of God or equal to God.  The Assumption of our\n",
      "Blessed Mother, meant that because of her close identification with the\n",
      "redemptive work of Christ, she was Assumed (note that she did not\n",
      "ASCEND) body and soul into Heaven, and is thus one of the few, along\n",
      "with Elijah, Enoch, Moses (maybe????) who are already perfected in\n",
      "Heaven.  Obviously, the Virgin Mary is far superior in glorification to\n",
      "any of the previously mentioned personages.\n",
      "\n",
      "Jung should stick to Psychology rather than getting into Theology.\n",
      "\n",
      "Andy Byler\n",
      "\n",
      "From: tas@pegasus.com (Len Howard)\n",
      "Subject: Re: Question from an agnostic\n",
      "Organization: Pegasus,  Honolulu\n",
      "Lines: 18\n",
      "\n",
      "Hi Damon,  No matter what system or explanation of creation you wish\n",
      "to accept, you always have to start with one of two premises, creation\n",
      "from nothing, or creation from something.  There are no other\n",
      "alternatives.  And if we accept one or the other of those two\n",
      "premises, then again there are two alternatives, either creation was\n",
      "random, or was according to some plan.\n",
      "   If it was random, I am unable to accept that the complex nature of\n",
      "our world with interrelated interdependent organisms and creatures\n",
      "could exist as they do.  Therefore I am left with creation under the\n",
      "control of an intelligence capable of devising such a scheme.  I call\n",
      "that intelligence God.\n",
      "   I also prefer the \"Creatio ex nihilo\" rather than from chaos, as it\n",
      "is cleaner.\n",
      "   There is obviously no way to prove either or neither.  We are and\n",
      "we must have come from somewhere.  Choose whatever explanation you\n",
      "feel most comfortable with, Damon.  You are the one who has to live\n",
      "with your choice.\n",
      "Shalom,                                  Len Howard\n",
      "\n",
      "From: sdixon@andy.bgsu.edu (Sherlette Dixon)\n",
      "Subject: Re:  My original post (Was Jesus Black?)\n",
      "Organization: Bowling Green State University Student\n",
      "Lines: 11\n",
      "\n",
      "My, my, my.  I knew that I would receive a response to my post, but not\n",
      "THIS extensive.  Thank you to all who responded; it at least showed that\n",
      "people were willing to think about it, even though the general response was\n",
      "a return to the same old \"Why should it matter?\" question.  To those of you\n",
      "who were a part of this response, I suggest that you read the articles\n",
      "covering this same question in soc.culture.african.american, for you are in\n",
      "DIRE need of some cultural enlightenment.\n",
      "\n",
      "Hasta luego\n",
      "\n",
      "Sherlette\n",
      "\n",
      "From: mdw33310@uxa.cso.uiuc.edu (Michael D. Walker)\n",
      "Subject: Re: The doctrine of Original Sin\n",
      "Organization: University of Illinois at Urbana\n",
      "Lines: 24\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\tJust a quick reminder:  \n",
      "\n",
      "\tThe way you are interpreting those passages is your opinion.  You make\n",
      "\tit sound as if your opinion is somehow an undisputable fact.\n",
      "\n",
      "\tMany would interpret the passages you cite very differently.\n",
      "\n",
      "\t(Many have--several of the great theologians you mentioned do that \n",
      "\tvery thing.  These were people who had much more expertise in the\n",
      "\tinterpretation of scripture than you or me or probably anyone reading\n",
      "\tthis newsgroup.  To say that all of them are wrong and you are right\n",
      "\tis, in my opinion, (notice those last three words) coming pretty darn\n",
      "\tclose to the sin of pride.  \n",
      "\n",
      "\tIn the future I would suggest you not be so absolutist in your \n",
      "\tinterpretations, especially when contradicting highly respected\n",
      "\tdoctors of Christianity.\n",
      "\n",
      "\t\t\t\t\t- Mike Walker\n",
      "\t\t\t\t\t  mdw33310@uxa.cso.uiuc.edu\n",
      "\t\t\t\t\t  (Univ. of Illinois)\n",
      "\n",
      "From: parkin@Eng.Sun.COM (Michael Parkin)\n",
      "Subject: Re: DID HE REALLY RISE???\n",
      "Reply-To: parkin@Eng.Sun.COM\n",
      "Organization: Sun Microsystems Inc., Mountain View, CA\n",
      "Lines: 57\n",
      "\n",
      "Another issue of importance.  Was the crucification the will of God or\n",
      "a tragic mistake.  I believe it was a tragic mistake.  God's will can\n",
      "never be accomplished through the disbelief of man.  Jesus came to\n",
      "this world to build the kingdom of heaven on the earth.  He\n",
      "desperately wanted the Jewish people to accept him as the Messiah.  If\n",
      "the crucification was the will of God how could Jesus pray that this\n",
      "cup pass from him.  Was this out of weakness.  NEVER.  Many men and\n",
      "women have given their lives for their country or other noble causes.\n",
      "Is Jesus less than these.  No he is not.  He knew the crucification\n",
      "was NOT the will of GOD.  God's will was that the Jewish people accept\n",
      "Jesus as the Messiah and that the kingdom of Heaven be established on\n",
      "the earth with Jesus as it's head. (Just like the Jewish people\n",
      "expected). If this had happened 2000 years ago can you imagine what\n",
      "kind of world we would live in today.  It would be a very different\n",
      "world.  And that is eactly what GOD wanted.  Men and women of that age\n",
      "could have been saved by following the living Messiah while he was on\n",
      "the earth.  Jesus could have established a sinless lineage that would\n",
      "have continued his reign after his ascension to the spiritual world to\n",
      "live with GOD.  Now the kingdom of heaven on the earth will have to\n",
      "wait for Christ's return.  But when he returns will he be recognized\n",
      "and will he find faith on this earth.  Isn't it about time for his\n",
      "return.  It's been almost 2000 years.\n",
      "\n",
      "Mike\n",
      "\n",
      "\n",
      "In article 28885@athos.rutgers.edu, oser@fermi.wustl.edu (Scott Oser) writes:\n",
      "In article <Apr.10.05.33.59.1993.14428@athos.rutgers.edu> mcovingt@aisun3.ai.uga.edu (Michael Covington) writes:\n",
      ">The two historic facts that I think the most important are these:\n",
      ">\n",
      ">(1) If Jesus didn't rise from the dead, then he must have done something\n",
      ">else equally impressive, in order to create the observed amount of impact.\n",
      ">\n",
      ">(2) Nobody ever displayed the dead body of Jesus, even though both the\n",
      ">Jewish and the Roman authorities would have gained a lot by doing so\n",
      ">(it would have discredited the Christians).\n",
      "\n",
      "And the two simplest refutations are these:\n",
      "\n",
      "(1)  What impact?  The only record of impact comes from the New Testament.\n",
      "I have no guarantee that its books are in the least accurate, and that\n",
      "the recorded \"impact\" actually happened.  I find it interesting that no other\n",
      "contemporary source records an eclipse, an earthquake, a temple curtain\n",
      "being torn, etc.  The earliest written claim we have of Jesus' resurrection\n",
      "is from the Pauline epistles, none of which were written sooner than 20 years\n",
      "after the supposed event.\n",
      "\n",
      "(2)  It seems probable that no one displayed the body of Jesus because no\n",
      "one knew where it was.  I personally believe that the most likely\n",
      "explanation was that the body was stolen (by disciples, or by graverobbers).\n",
      "Don't bother with the point about the guards ... it only appears in one\n",
      "gospel, and seems like exactly the sort of thing early Christians might make\n",
      "up in order to counter the grave-robbing charge.  The New Testament does\n",
      "record that Jews believed the body had been stolen.  If there were really\n",
      "guards, they could not have effectively made this claim, as they did.\n",
      "\n",
      "-Scott O.\n",
      "\n",
      "From: dlecoint@garnet.acns.fsu.edu (Darius_Lecointe)\n",
      "Subject: Re: Sabbath Admissions 5of5\n",
      "Organization: Florida State University\n",
      "Lines: 21\n",
      "\n",
      "I find it interesting that cls never answered any of the questions posed. \n",
      "Then he goes on the make statements which make me shudder.  He has\n",
      "established a two-tiered God.  One set of rules for the Jews (his people)\n",
      "and another set for the saved Gentiles (his people).  Why would God\n",
      "discriminate?  Does the Jew who accepts Jesus now have to live under the\n",
      "Gentile rules.\n",
      "\n",
      "God has one set of rules for all his people.  Paul was never against the\n",
      "law.  In fact he says repeatedly that faith establishes rather that annuls\n",
      "the law.  Paul's point is germane to both Jews and Greeks.  The Law can\n",
      "never be used as an instrument of salvation.  And please do not combine\n",
      "the ceremonial and moral laws in one.\n",
      "\n",
      "In Matt 5:14-19 Christ plainly says what He came to do and you say He was\n",
      "only saying that for the Jews's benefit.  Your Christ must be a\n",
      "politician, speaking from both sides of His mouth.  As Paul said, \"I have\n",
      "not so learned Christ.\"  Forget all the theology, just do what Jesus says.\n",
      " Your excuses will not hold up in a court of law on earth, far less in\n",
      "God's judgement hall.\n",
      "\n",
      "Darius\n",
      "\n",
      "From: schnitzi@osceola.cs.ucf.edu (Mark Schnitzius)\n",
      "Subject: Re: Atheists and Hell\n",
      "Organization: University of Central Florida\n",
      "Lines: 33\n",
      "\n",
      "atterlep@vela.acs.oakland.edu (Cardinal Ximenez) writes:\n",
      "\n",
      ">1) Atheists believe that when they die, they die forever.\n",
      "\n",
      ">2) A god who would condemn those who fail to believe in him to eternal death\n",
      ">   is unfair.\n",
      "\n",
      ">  I don't see what the problem is!  To Christians, Hell is, by definition, \n",
      ">eternal death--exactly what atheists are expecting when they die.  \n",
      "\n",
      "Well, I think that most Christians believe that your conciousness will\n",
      "somehow continue on after your 'physical' death, which contradicts what\n",
      "most atheists (myself included) believe, namely that your conciousness,\n",
      "being contained in your brain, dies when your brain dies.\n",
      "\n",
      ">There's no\n",
      ">reason Hell has to be especially awful--to most people, eternal death is bad\n",
      ">enough.\n",
      "\n",
      "I fear the pain that often comes with the process of dying, but since I\n",
      "won't be around to worry about it, I don't fear eternal death.\n",
      "\n",
      ">  Literal interpreters of the Bible will have a problem with this view, since\n",
      ">the Bible talks about the fires of Hell and such.  \n",
      "\n",
      "This is something I've always found confusing.  If all your nerve endings\n",
      "die with your physical body, why would flame hurt you?  How can one \"wail\n",
      "and gnash teeth\" with no lungs and no teeth?\n",
      "\n",
      "\n",
      "Mark Schnitzius\n",
      "schnitzi@eola.cs.ucf.edu\n",
      "University of Central Florida\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(model):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(v['data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
