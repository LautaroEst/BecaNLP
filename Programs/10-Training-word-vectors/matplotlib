{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        #target[target==self.vocab_size] = 1\n",
    "        print(target.size(), target.max(), self.vocab_size)\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        #target[target==self.vocab_size] = 1\n",
    "        print(target.size(), target.max(), self.vocab_size)\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=False,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        #arget[target==self.vocab_size] = 1\n",
    "        print(target.size(), target.max(), self.vocab_size)\n",
    "        print(scores.size())\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        target[target==self.vocab_size] = 1\n",
    "        if target.size() != torch.Size([2]):\n",
    "            context_size = target.size(1)\n",
    "            scores = scores.view(-1,self.vocab_size,1).repeat(1,1,context_size)\n",
    "            print(target.size(), target.max(), self.vocab_size)\n",
    "            print(scores.size())\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        target[target==self.vocab_size] = 1\n",
    "        if target.size() != torch.Size([2]):\n",
    "            context_size = target.size(1)\n",
    "            scores = scores.view(-1,self.vocab_size,1).repeat(1,1,context_size)\n",
    "            print(target.size(), target.max(), self.vocab_size)\n",
    "            print(scores.size())\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        #target[target==self.vocab_size] = 1\n",
    "        if target.size() != torch.Size([2]):\n",
    "            context_size = target.size(1)\n",
    "            scores = scores.view(-1,self.vocab_size,1).repeat(1,1,context_size)\n",
    "            print(target.size(), target.max(), self.vocab_size)\n",
    "            print(scores.size())\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._token_to_idx = {}\n",
    "        self._idx_to_token = {}\n",
    "        self._idx_to_freq = {}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "            self._idx_to_freq[index] += 1\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            self._idx_to_freq[index] = 1\n",
    "        return index\n",
    "    \n",
    "    def index_to_token(self, index):\n",
    "        \n",
    "        if not isinstance(index, list):\n",
    "            if not isinstance(index, int):\n",
    "                raise NameError(\"'index' must be an integer or list of integers\")\n",
    "            if index not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(index))\n",
    "            return self._idx_to_token[index]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in index:\n",
    "            if not isinstance(idx, int):\n",
    "                raise NameError(\"{} is not an integer\".format(idx))\n",
    "            if idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(idx))\n",
    "            tokens.append(self._idx_to_token[idx])\n",
    "        return tokens\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        \n",
    "        if not isinstance(token, list):\n",
    "            if not isinstance(token, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if token not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(token))\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "        indeces = []\n",
    "        for tk in token:\n",
    "            if not isinstance(tk, str):\n",
    "                raise NameError(\"'token' must be a string or list of strings\")\n",
    "            if tk not in self._token_to_idx:\n",
    "                raise KeyError('the token {} is not in the Vocabulary'.format(tk))\n",
    "            indeces.append(self._token_to_idx[tk])\n",
    "        return indeces\n",
    "    \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            if tk_or_idx not in self._idx_to_token:\n",
    "                raise KeyError('the index {} exeeds the Vocabulary lenght'.format(tk_or_idx))\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            if tk_or_idx not in self._token_to_idx:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = self._idx_to_freq[self._token_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    paragraphs = re.findall(r'.+\\n',corpus)\n",
    "    return [re.findall(r'\\b[\\w\\-,\"]+\\b', par) for par in paragraphs]\n",
    "\n",
    "\n",
    "class Word2VecSamples(Dataset):\n",
    "    \n",
    "    no_token = '<NT>'\n",
    "    \n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        \n",
    "        # Obtengo el vocabulario a partir del corpus ya tokenizado:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = Vocabulary()\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                self.vocabulary.add_token(token)\n",
    "                \n",
    "        # Obtengo el contexto a partir del corpus:\n",
    "        self.window_size = window_size\n",
    "        self.data = pd.DataFrame({'word': [token for doc in corpus for token in doc],\n",
    "                                  'context': [[self.no_token for j in range(i-window_size, max(0,i-window_size))] + \\\n",
    "                                              doc[max(0,i-window_size):i] + \\\n",
    "                                              doc[i+1:min(i+window_size+1, len(doc))] + \\\n",
    "                                              [self.no_token for j in range(min(i+window_size+1, len(doc)),i+window_size+1)] \\\n",
    "                                              for doc in corpus for i in range(len(doc))]\n",
    "                                 })\n",
    "        self.padding_idx = len(self.vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        word_vector = torch.tensor(self.vocabulary.token_to_index(self.data['word'].iloc[idx]), dtype=torch.long)\n",
    "        context_vector = torch.zeros(2 * self.window_size, dtype=torch.long)\n",
    "        for i, token in enumerate(self.data['context'].iloc[idx]):\n",
    "            if token == self.no_token:\n",
    "                context_vector[i] = self.padding_idx\n",
    "            else:\n",
    "                context_vector[i] = self.vocabulary.token_to_index(token)\n",
    "            \n",
    "        return word_vector, context_vector        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "class CBOWModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.emb(x).mean(dim=1)\n",
    "        return self.out(embedding)\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "        return lf(scores,target)\n",
    "        \n",
    "        \n",
    "class SkipGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=vocab_size)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.out(self.emb(x))\n",
    "    \n",
    "    def loss(self,scores,target):\n",
    "        lf = nn.CrossEntropyLoss(ignore_index=self.vocab_size)\n",
    "        if target.size() != torch.Size([2]):\n",
    "            context_size = target.size(1)\n",
    "            scores = scores.view(-1,self.vocab_size,1).repeat(1,1,context_size)\n",
    "        return lf(scores,target)\n",
    "\n",
    "\n",
    "def TrainWordVectors(data, \n",
    "                     lm='CBOW', \n",
    "                     window_size=2,\n",
    "                     batch_size=64,\n",
    "                     embedding_dim=100,\n",
    "                     use_gpu=True,\n",
    "                     epochs=1,\n",
    "                     learning_rate=1e-2,\n",
    "                     sample_loss_every=100):\n",
    "    \n",
    "    # Chequeo que se haya pasado bien el corpus:\n",
    "    if isinstance(data,str):\n",
    "        corpus = tokenize(data)\n",
    "    elif isinstance(data,list):\n",
    "        for doc in data:\n",
    "            if isinstance(doc,list):\n",
    "                for token in doc:\n",
    "                    if isinstance(token,str):\n",
    "                        corpus = data\n",
    "    else:\n",
    "        raise TypeError('data debe ser una lista de listas de tokens o un texto plano (string)')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Obtengo los batches de muestras:\n",
    "    dataset = Word2VecSamples(corpus, window_size=window_size)\n",
    "    samples_idx = torch.randperm(len(dataset))\n",
    "    my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=my_sampler(samples_idx))\n",
    "    \n",
    "    vocab_size = len(dataset.vocabulary)    \n",
    "    \n",
    "    # Defino el modelo:\n",
    "    if lm == 'CBOW':\n",
    "        model = CBOWModel(vocab_size, embedding_dim)\n",
    "    elif lm == 'SkipGram':\n",
    "        model = SkipGramModel(vocab_size, embedding_dim)\n",
    "    else:\n",
    "        raise TypeError('El modelo de entrenamiento no es v√°lido.')\n",
    "    \n",
    "    \n",
    "    print('Starting training...')\n",
    "    performance_history = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() and use_gpu else torch.device('cpu')\n",
    "    model = model.to(device=device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    batch_len = len(dataloader)\n",
    "    \n",
    "    try:\n",
    "        for e in range(epochs):\n",
    "            for t, (x,y) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "                if lm == 'CBOW':\n",
    "                    scores = model(y)\n",
    "                    loss = model.loss(scores,x)\n",
    "                elif lm == 'SkipGram':\n",
    "                    scores = model(x)\n",
    "                    loss = model.loss(scores,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (e * batch_len + t) % sample_loss_every == 0:\n",
    "                    performance_history['iter'].append(e * batch_size + t)\n",
    "                    performance_history['loss'].append(loss.item())\n",
    "                    print('Epoch: {}, Batch number: {}, Loss: {}'.format(e+1, t,loss.item()))\n",
    "                    \n",
    "        return performance_history\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print('Exiting training...')\n",
    "        return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='CBOW', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history = TrainWordVectors(corpus,\n",
    "                                       lm='CBOW', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp1_v2.txt','r') as file:\n",
    "    corpus = file.read()\n",
    "    \n",
    "performance_history_sk = TrainWordVectors(corpus,\n",
    "                                       lm='SkipGram', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=10,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=200)\n",
    "\n",
    "performance_history_cbow = TrainWordVectors(corpus,\n",
    "                                       lm='CBOW', \n",
    "                                       window_size=2,\n",
    "                                       batch_size=64,\n",
    "                                       embedding_dim=100,\n",
    "                                       use_gpu=True,\n",
    "                                       epochs=10,\n",
    "                                       learning_rate=1e-2,\n",
    "                                       sample_loss_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(performance_history_sk['iter'],performance_history_sk['loss'],label='SkipGram')\n",
    "ax.plot(performance_history_cbow['iter'],performance_history_cbow['loss'],label='CBOW')\n",
    "    \n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
