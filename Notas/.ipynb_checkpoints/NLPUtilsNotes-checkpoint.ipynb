{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas sobre los programas y eficiencia\n",
    "\n",
    "[Acá](https://www.codementor.io/@satwikkansal/python-practices-for-efficient-code-performance-memory-and-usability-aze6oiq65) hay un link muy interesante sobre eficiencia en python.\n",
    "\n",
    "Me gustaría pensar en cosas para acelerar mis programas, y para eso tendría que saber cosas de programación como eficiencia y esas cosas de informáticos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El corpus\n",
    "\n",
    "Un corpus de texto es un conjunto de documentos que, a su vez, son una secuencia de tokens. Por ejemplo:\n",
    "\n",
    "```Python\n",
    "[['This', 'is', 'the', 'pencil', 'of', 'Esther', 'Píscore', '.'],\n",
    " ['<START>','Duerma', ',', 'Don', 'Rodrigo','<END>']]\n",
    "```\n",
    "\n",
    "Siempre puedo representar al corpus como una lista de listas de strings.\n",
    "\n",
    "Un corpus puede ser obtenido de un archivo csv, una string, un dataframe, un archivo txt, o de donde fuere, pero siempre va a tener la misma forma. En general, para obtener un corpus hay que hacer un proceso de \"limpieza\" de texto (tokenization, lemmatization, stemmization y esas cosas) que no viene al caso ahora, pero que siempre me termina dando un objeto con esas características. \n",
    "\n",
    "Un corpus puede ser un conjunto de ejemplos de un campo, como cuando hago text classification, o un conjunto de documentos preparados para entrenar word vectors, entre otras cosas.\n",
    "\n",
    "Por otra parte, yo voy a querer, junto con el corpus, un conjunto de instrucciones para transformar el corpus en un conjunto de muestras vectorizadas que pueda ingresar a mi modelo para entrenarlo. Para eso necesito en el 99.9% de los casos un vocabulario, es decir, un mapeo de tokens a índices que me permite identificar, posteriormente, cada token (único) con un vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    token_sep = '<TS>'\n",
    "    doc_sep = '<DS>'\n",
    "    default_tokenizer = lambda s: s.split(' ')\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.vocabulary = Vocabulary.from_list_corpus(data)\n",
    "        self.docs_num = len(data)\n",
    "        self.tokens_num = sum([len(doc) for doc in data])\n",
    "        self.data = [[self.vocabulary.token_to_index(tk) for tk in doc] for doc in data]\n",
    "        \n",
    "        self.max_idx = self.tokens_num\n",
    "    \n",
    "    @classmethod\n",
    "    def from_binary_files(cls, filenames, decode='utf-8', tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.default_tokenizer\n",
    "        texts_list = []\n",
    "        if isinstance(filenames, list):\n",
    "            for filename in filenames:\n",
    "                with open(filename, 'rb') as file:\n",
    "                    texts_list.append(self.token_sep.join(tokenizer(file.read().decode(decode))))\n",
    "        elif isinstance(filenames, str):\n",
    "            with open(filenames, 'rb') as file:\n",
    "                texts_list.append(self.token_sep.join(tokenizer(file.read().decode(decode))))\n",
    "        data = self.doc_sep.join(texts_list)\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text_files(cls, filenames, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.default_tokenizer\n",
    "        texts_list = []\n",
    "        if isinstance(filenames, list):\n",
    "            for filename in filenames:\n",
    "                with open(filename, 'r') as file:\n",
    "                    texts_list.append(self.token_sep.join(tokenizer(file.read().decode(decode))))\n",
    "        elif isinstance(filenames, str):\n",
    "            with open(filenames, 'r') as file:\n",
    "                texts_list.append(self.token_sep.join(tokenizer(file.read().decode(decode))))\n",
    "        data = self.doc_sep.join(texts_list)\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_strings(cls, texts, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.default_tokenizer\n",
    "        texts_list = []\n",
    "        if isinstance(filenames, list):\n",
    "            for text in texts:\n",
    "                texts_list.append(self.token_sep.join(tokenizer(text)))\n",
    "        elif isinstance(filenames, str):\n",
    "            texts_list.append(self.token_sep.join(tokenizer(text)))\n",
    "        data = self.doc_sep.join(texts_list)\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv_file(cls, filename, fieldname, tokenizer=None, **kwargs):\n",
    "        ds = pd.read_csv(filename, **kwargs)[fieldname]\n",
    "        data = self.doc_sep.join(ds.str.apply(tokenizer).str.join(self.token_sep).tolist())\n",
    "        return cls(data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Corpus object\\nNumber of docs = {}\\nNumber of tokens = {}\".format(self.docs_num, self.tokens_num)\n",
    "    \n",
    "    def __str__(self):\n",
    "        printed_text = ''\n",
    "        num_print_docs = min(self.docs_num,5)\n",
    "        unk_token_idx = self.vocabulary.max_idx\n",
    "        for i in range(num_print_docs):\n",
    "            doc = self.data[i]\n",
    "            if len(doc) <= 5:\n",
    "                printed_text += repr([self.vocabulary.index_to_token(idx) if idx != unk_token_idx \\\n",
    "                                      else self.vocabulary.unk_token for idx in doc]) \n",
    "            else:\n",
    "                printed_text += repr([self.vocabulary.index_to_token(idx) if idx != unk_token_idx \\\n",
    "                                      else self.vocabulary.unk_token for idx in doc[:4]])[:-1] + ', ...]'\n",
    "            if i < num_print_docs:\n",
    "                printed_text += '\\n'\n",
    "        if num_print_docs != self.docs_num:\n",
    "            printed_text += '...'\n",
    "        return printed_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokens_num\n",
    "    \n",
    "    def __getitem__(self,tk_or_idx):\n",
    "        if isinstance(tk_or_idx, int):\n",
    "            return self.data[tk_or_idx]\n",
    "        if isinstance(tk_or_idx, str):\n",
    "            return [i for doc in self.data for i, tk in enumerate(doc)]\n",
    "        raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (self.vocabulary.index_to_token(idx) for doc in self.data for idx in doc)\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        return key in self.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word_context(corpus, vocabulary, left_window=2, right_window=2, split_contexts=0):\n",
    "    unk_token_idx = len(vocabulary)\n",
    "    context_size = left_window + right_window\n",
    "    words = []\n",
    "    contexts = []\n",
    "    for doc in corpus:\n",
    "        for i in range(left_window):\n",
    "            doc.insert(0,unk_token_idx)\n",
    "        for i in range(right_window):\n",
    "            doc.append(unk_token_idx)\n",
    "        for i, idx in enumerate(doc[left_window:-right_window],left_window):\n",
    "            words.append(idx)\n",
    "            contexts.append(doc[i-left_window:i] + doc[i+1:i+right_window+1])\n",
    "        for i in range(left_window):\n",
    "            doc.pop(0)\n",
    "        for i in range(right_window):\n",
    "            doc.pop(-1)\n",
    "\n",
    "    print(words)\n",
    "    print(contexts)\n",
    "    if split_contexts == 0:\n",
    "        words = torch.tensor(words)\n",
    "        contexts = torch.tensor(contexts)\n",
    "        mask = (words != unk_token_idx) * (contexts != unk_token_idx).any(dim=1)\n",
    "    elif split_contexts == -1:\n",
    "        words = torch.tensor(words).view(-1,1).repeat(1,context_size).view(-1)\n",
    "        contexts = torch.tensor(contexts).view(-1)\n",
    "        mask = (words != unk_token_idx) * (contexts != unk_token_idx)\n",
    "    elif split_contexts < 0:\n",
    "        raise RuntimeError('El tamaño del contexto debe ser positivo o igual a -1')\n",
    "    elif context_size % split_contexts == 0:\n",
    "        words = torch.tensor(words).view(-1,1).repeat(1,context_size // split_contexts).view(-1)\n",
    "        contexts = torch.tensor(contexts).view(-1,split_contexts)\n",
    "        mask = (words != unk_token_idx) * (contexts != unk_token_idx).any(dim=1)\n",
    "    else:\n",
    "        raise RuntimeError('Los tamaños de los contextos deben ser iguales')\n",
    "\n",
    "    words = words[mask]\n",
    "    contexts = contexts[mask]\n",
    "    return words, contexts\n",
    "    \n",
    "corpus = [['w1', 'w2', 'w3'],['w1', 'w2', 'w3', 'w1', 'w2'], ['w4', 'w4'], ['w5'],\n",
    "          ['w1', 'w2', 'w2', 'w3'],['w1', 'w2', 'w3', 'w1', 'w2'], ['w4', 'w4'], ['w5']]\n",
    "\n",
    "from collections import Counter\n",
    "vocabulary = Counter(itertools.chain.from_iterable(corpus))\n",
    "vectorize_word_context(corpus, vocabulary, left_window=2, right_window=2, split_contexts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    token_sep = '<TS>'\n",
    "    doc_sep = '<DS>'\n",
    "    default_tokenizer = lambda s: s.split(' ')\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    @classmethod\n",
    "    def from_binary_files(cls, filenames, decode='utf-8', tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = cls.default_tokenizer\n",
    "        texts_list = []\n",
    "        if isinstance(filenames, list):\n",
    "            for filename in filenames:\n",
    "                with open(filename, 'rb') as file:\n",
    "                    texts_list.append(cls.token_sep.join(tokenizer(file.read().decode(decode))))\n",
    "        elif isinstance(filenames, str):\n",
    "            with open(filenames, 'rb') as file:\n",
    "                texts_list.append(cls.token_sep.join(tokenizer(file.read().decode(decode))))\n",
    "        data = cls.doc_sep.join(texts_list)\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text_files(cls, filenames, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = cls.default_tokenizer\n",
    "        texts_list = []\n",
    "        if isinstance(filenames, list):\n",
    "            for filename in filenames:\n",
    "                with open(filename, 'r') as file:\n",
    "                    texts_list.append(cls.token_sep.join(tokenizer(file.read())))\n",
    "        elif isinstance(filenames, str):\n",
    "            with open(filenames, 'r') as file:\n",
    "                texts_list.append(cls.token_sep.join(tokenizer(file.read())))\n",
    "        data = cls.doc_sep.join(texts_list)\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_strings(cls, texts, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = cls.default_tokenizer\n",
    "        texts_list = []\n",
    "        if isinstance(filenames, list):\n",
    "            for text in texts:\n",
    "                texts_list.append(cls.token_sep.join(tokenizer(text)))\n",
    "        elif isinstance(filenames, str):\n",
    "            texts_list.append(cls.token_sep.join(tokenizer(text)))\n",
    "        data = self.doc_sep.join(texts_list)\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv_file(cls, filename, fieldname, tokenizer=None, **kwargs):\n",
    "        ds = pd.read_csv(filename, **kwargs)[fieldname]\n",
    "        data = cls.doc_sep.join(ds.str.apply(tokenizer).str.join(cls.token_sep).tolist())\n",
    "        return cls(data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Corpus object\\nNumber of docs = {}\\nNumber of tokens = {}\".format(self.docs_num, self.tokens_num)\n",
    "    \n",
    "    def __str__(self):\n",
    "        printed_text = ''\n",
    "        num_print_docs = min(self.docs_num,5)\n",
    "        unk_token_idx = self.vocabulary.max_idx\n",
    "        for i in range(num_print_docs):\n",
    "            doc = self.data[i]\n",
    "            if len(doc) <= 5:\n",
    "                printed_text += repr([self.vocabulary.index_to_token(idx) if idx != unk_token_idx \\\n",
    "                                      else self.vocabulary.unk_token for idx in doc]) \n",
    "            else:\n",
    "                printed_text += repr([self.vocabulary.index_to_token(idx) if idx != unk_token_idx \\\n",
    "                                      else self.vocabulary.unk_token for idx in doc[:4]])[:-1] + ', ...]'\n",
    "            if i < num_print_docs:\n",
    "                printed_text += '\\n'\n",
    "        if num_print_docs != self.docs_num:\n",
    "            printed_text += '...'\n",
    "        return printed_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokens_num\n",
    "    \n",
    "    def __getitem__(self,tk_or_idx):\n",
    "        if isinstance(tk_or_idx, int):\n",
    "            return self.data[tk_or_idx]\n",
    "        if isinstance(tk_or_idx, str):\n",
    "            return [i for doc in self.data for i, tk in enumerate(doc)]\n",
    "        raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (doc.split(self.doc_sep) for doc in self.data.split(self.token_sep) )\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        return key in self.vocabulary\n",
    "    \n",
    "\n",
    "corpus = Corpus.from_text_files('../Utils/Datasets/wiki2018/parts/xaa')\n",
    "for i in map(str.split,corpus.data):\n",
    "    print(i[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intento 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, filename, preprocessing=None, **kwargs):\n",
    "        df = pd.read_csv(filename, **kwargs)\n",
    "        if preprocessing is not None:\n",
    "            df = preprocessing(df)\n",
    "        return cls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
