{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_utils_v03 import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas Pytorch (versión 3)\n",
    "\n",
    "El procedimiento para hacer un modelo paramétrico supervisado en Pytorch es siempre el mismo:\n",
    "\n",
    "1. Programar una función `GetDataLoaders()` que devuelva los dataloaders de los conjuntos de entrenamiento, validación y testeo de un dataset específico.\n",
    "\n",
    "2. Definir la forma del modelo neuronal y hacer una subclase de `torch.nn.Module` que lo represente.\n",
    "\n",
    "3. Entrenar el modelo. Para esto, se hace una función `ModelTrain()` que obtenga los parámetros del modelo anterior a partir del conjunto de entrenamiento y calcule la performance del mismo sobre el conjunto de validación. Este proceso es iterativo y se repite hasta encontrar la mejor performance sobre el conjunto de validación. Luego, se ven los resultados en el conjunto de testeo. \n",
    "\n",
    "Vamos a explicar con más detalle cada uno de estos pasos.\n",
    "\n",
    "## El Dataset\n",
    "\n",
    "El primer paso es organizar el dataset, es decir, definir cómo le vamos a pasar la información de las muestras a la función de entrenamiento. \n",
    "\n",
    "En los programas de Pytorch, toda la información del dataset queda incluída en dos objetos del módulo `torch.utils.data`: `torch.utils.data.Dataset` y `torch.utils.data.DataLoader`. El primero es el objeto que representa a las muestras, mientras que el segundo es el que se usa para pasarle las muestras a la función de entrenamiento. En general, los datasets disponibles se componen de un conjunto de entrenamiento y otro de testeo. Además, se desea destinar parte de las muestras de entrenamiento a un proceso de validación mientras se está entrenando (para no usar las muestras de testeo). \n",
    "\n",
    "Por lo tanto, se desea implementar una función `GetData()` (que funciona para un dataset específico) que permita acceder a las muestras de entrenamiento y de testeo, y además devuelva los dataloaders necesarios para utilizar en la función de entrenamiento del modelo.\n",
    "\n",
    "```Python\n",
    "def GetData(transform, val_size, batch_size):\n",
    "    \n",
    "    # Proceso de carga de datos\n",
    "    # ...\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_tadaloader\n",
    "```\n",
    "\n",
    "**Ejemplo.** Se dispone del dataset CIFAR10, que tiene las siguientes características:\n",
    "\n",
    "* Contiene un conjunto de 50000 muestras de entrenamiento y 10000 muestras de testeo.\n",
    "* Cada muestra consiste en una imagen de 32x32 píxels en RGB y un número que representa el índice correspondiente al contenido de la imagen en la lista `['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']`.\n",
    "\n",
    "A continuación mostramos una implementación de la carga de este dataset con las siguientes características:\n",
    "\n",
    "* Las imágenes se modifican para tener un tamaño 224x244 y se cargan en memoria en tensores de tamaño 3x224x24.\n",
    "* Las imágenes se representan en punto flotante y se normalizan en media y varianza.\n",
    "* Para el proceso de entrenamiento se utilizará un *batch* de tamaño 64 (es decir, cada *batch* de datos contiene 64 muestras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9c80d8f6b29a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Obtenemos los dataloaders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetCIFAR10Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disco.lautaro/home/lestien/Documents/Python-Notes-Utils/Pytorch/pytorch_utils_v02/TorchvisionDatasets.py\u001b[0m in \u001b[0;36mGetCIFAR10Dataset\u001b[0;34m(transform, val_size, batch_size)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# Se descarga el dataset y se definen los dataloaders de train / validation / test:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     cifar10_train = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n\u001b[0;32m--> 190\u001b[0;31m                                  transform=transform)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     cifar10_val = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n",
      "\u001b[0;32m/mnt/disco.lautaro/home/lestien/Documents/Python-Notes-Utils/Pytorch/pytorch_utils_v02/TorchvisionDatasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         super(CIFAR10, self).__init__(root, transform=transform,\n\u001b[0;32m---> 55\u001b[0;31m                                       target_transform=target_transform)\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m  \u001b[0;31m# training set or test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "val_size = .02 # Proporción de muestras del training set destinadas a validación.\n",
    "batch_size = 64 # Tamaño del batch\n",
    "transform = T.Compose([T.Resize(224),\n",
    "                       T.ToTensor(),\n",
    "                       T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]) # Transformación de las imágenes\n",
    "\n",
    "\n",
    "# Obtenemos los dataloaders:\n",
    "cifar10_train = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n",
    "                        transform=transform)\n",
    "\n",
    "cifar10_val = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n",
    "                      transform=transform)\n",
    "\n",
    "cifar10_test = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/test/', train=False, download=True, \n",
    "                       transform=transform)\n",
    "\n",
    "NUM_TRAIN = int((1 - val_size) * len(cifar10_train))\n",
    "NUM_VAL = len(cifar10_train) - NUM_TRAIN\n",
    "\n",
    "train_dataloader = DataLoader(cifar10_train, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "val_dataloader = DataLoader(cifar10_val, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))\n",
    "\n",
    "test_dataloader = DataLoader(cifar10_test, batch_size=batch_size)\n",
    "\n",
    "print()\n",
    "print('CIFAR10 Dataset:')\n",
    "print()\n",
    "print('Cantidad de muestras de entrenamiento disponibles: %d' % len(train_dataloader.dataset) )\n",
    "print('Cantidad de muestras de testeo disponibles: %d' % len(test_dataloader.dataset) )\n",
    "print('Cantidad de muestras de entrenamiento destinadas a validación: %d' % (len(train_dataloader.dataset) * val_size // 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para definir las transformaciones realizadas a las imágenes antes de cargarlas se utiliza el módulo `torch.transforms` cargado como `T`. También pueden definirse transformaciones propias como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "MNIST Dataset:\n",
      "\n",
      "Cantidad de muestras de entrenamiento disponibles: 50000\n",
      "Cantidad de muestras de testeo disponibles: 10000\n",
      "Cantidad de muestras de entrenamiento destinadas a validación: 1000\n"
     ]
    }
   ],
   "source": [
    "class MNISTReshape(object):\n",
    "    def __call__(self, sample):\n",
    "        return sample[0,:,:]\n",
    "    \n",
    "val_size = .02 # Proporción de muestras del training set destinadas a validación.\n",
    "batch_size = 64 # Tamaño del batch\n",
    "transform = T.Compose([MNISTReshape(),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "# Obtenemos los dataloaders:\n",
    "mnist_train = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n",
    "                      transform=transform)\n",
    "\n",
    "mnist_val = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n",
    "                    transform=transform)\n",
    "\n",
    "mnist_test = CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/test/', train=False, download=True, \n",
    "                     transform=transform)\n",
    "\n",
    "NUM_TRAIN = int((1 - val_size) * len(mnist_train))\n",
    "NUM_VAL = len(mnist_train) - NUM_TRAIN\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "val_dataloader = DataLoader(mnist_val, \n",
    "                            batch_size=batch_size, \n",
    "                            sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))\n",
    "\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=batch_size)\n",
    "\n",
    "print()\n",
    "print('MNIST Dataset:')\n",
    "print()\n",
    "print('Cantidad de muestras de entrenamiento disponibles: %d' % len(train_dataloader.dataset) )\n",
    "print('Cantidad de muestras de testeo disponibles: %d' % len(test_dataloader.dataset) )\n",
    "print('Cantidad de muestras de entrenamiento destinadas a validación: %d' % (len(train_dataloader.dataset) * val_size // 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = dset.CIFAR10('Datasets/TorchvisionDatasets/CIFAR10/train', train=True, download=True,\n",
    "                                 transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_integrity', '_format_transform_repr', '_load_meta', '_repr_indent', 'base_folder', 'class_to_idx', 'classes', 'data', 'download', 'extra_repr', 'filename', 'meta', 'root', 'target_transform', 'targets', 'test_list', 'tgz_md5', 'train', 'train_list', 'transform', 'transforms', 'url']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torchvision.datasets.cifar'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(cifar10_train))\n",
    "cifar10_train.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 27171.81lines/s]\n",
      "120000lines [00:08, 14370.02lines/s]\n",
      "7600lines [00:00, 14663.17lines/s]\n"
     ]
    }
   ],
   "source": [
    "val_size = .02 # Proporción de muestras del training set destinadas a validación.\n",
    "batch_size = 64 # Tamaño del batch\n",
    "n_grams = 2 # Modelo de bi-gramas\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = GetAGNewsDataset(val_size, batch_size, n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'data',\n",
       " 'datasets',\n",
       " 'utils',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "dir(torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torchtext.datasets.AG_NEWS is torchtext.datasets.text_classification.DATASETS['AG_NEWS'])\n",
    "dir(torchtext.datasets.AG_NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.datasets.sst.SST'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils.data' has no attribute '__bases__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0f245ebcbfb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSST\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# is torchtext.datasets.text_classification.DATASETS['SST'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__bases__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.utils.data' has no attribute '__bases__'"
     ]
    }
   ],
   "source": [
    "print(torchtext.datasets.SST) # is torchtext.datasets.text_classification.DATASETS['SST'])\n",
    "dir(torchtext.datasets.SST)\n",
    "print(torch.utils.data.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnews_train, agnews_test = torchtext.datasets.text_classification.DATASETS['AG_NEWS'](\n",
    "        root='./Datasets/TextDatasets/AG_NEWS', ngrams=n_grams, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.datasets.text_classification.TextClassificationDataset'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(agnews_train))\n",
    "print(isinstance(agnews_train,torch.utils.data.Dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " tensor([    572,     564,       2,    2326,   49106,     150,      88,       3,\n",
       "            1143,      14,      32,      15,      32,      16,  443749,       4,\n",
       "             572,     499,      17,      10,  741769,       7,  468770,       4,\n",
       "              52,    7019,    1050,     442,       2,   14341,     673,  141447,\n",
       "          326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n",
       "             144,     145,  299709,  443750,   51274,     703,   14312,      23,\n",
       "         1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n",
       "            4052]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agnews_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_data', '_labels', '_vocab'])\n",
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_data', '_labels', '_vocab', 'get_labels', 'get_vocab']\n",
      "['UNK', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_default_unk_index', 'extend', 'freqs', 'itos', 'load_vectors', 'set_vectors', 'stoi', 'unk_index', 'vectors']\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(agnews_train.__dict__.keys())\n",
    "print(dir(agnews_train))\n",
    "print(dir(agnews_train.get_vocab()))\n",
    "vocab = agnews_train.get_vocab()\n",
    "i = 0 \n",
    "for word in vocab:\n",
    "    print(type(word))\n",
    "    if i > 20:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f403adff390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agnews_train._vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
