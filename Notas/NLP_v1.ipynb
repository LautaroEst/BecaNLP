{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "* Las personas usan lenguajes para comunicar sus ideas.\n",
    "\n",
    "* Las palabras son bloques de significado que tienen la ventaja de que se pueden escribir y combinar entre sí para representar una idea. \n",
    "\n",
    "* La primera forma de representar una idea más grande a partir de palabras es hacer manualmente una red de palabras, es decir, listar para cada palabra, las palabras que se relacionan con ella. (*good*, *goodness*, *well*, etc.). Dada una secuencia de palabras, se podría pensar en que todas las combinaciones de frases generadas de reemplazar cada palabra de la frase por sus conectadas podrían dar lugar a un significado más claro. Es más, se podría interpretar como que el significado de la frase son todas esas combinaciones juntas.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "La segunda forma es con una representación distribuída. Es decir, ahora tenemos que las palabras viven en un espacio continuo y no están relacionadas por una red (lo cual hace que vivan en un espacio discreto). Esta idea hace que podamos investigar la similitud entre las palabras y sus significados. Esto depende de cómo se las entrene.\n",
    "\n",
    "La hipótesis distribucional asume que es posible comprender el significado de las palabras a partir de su contexto. Con esto, podemos pensar en que el significado de las palabras se obtiene como consecuencia de aprender la probabilidad $P(w_{n+1}|w_1,\\ldots,w_n)$ de que aparezca una palabra $w_{n+1}$ dado un conjunto de palabras $w_1,\\ldots,w_n$. El algoritmo `word2vec` propone estimar esta probabilidad a partir de las muestras que aparecen en un texto.\n",
    "\n",
    "Un poco más claro. Defino las variables $W_{c-m},W_{c-m+1},\\ldots,W_c,\\ldots,W_{c+m-1},W_{c+m}$ con realizaciones en un vocabulario $V= \\{ w_1, \\ldots, w_{|V|} \\}$. El objetivo es estimar la probabilidad\n",
    "\n",
    "$$\n",
    "P(W_c=w_c|W_{c-m}=w_{c-m},\\ldots,W_{c-1}=w_{c-1},W_{c+1}=w_{c+1},W_{c+m}=w_{c+m})\n",
    "$$\n",
    "\n",
    "o simplemente\n",
    "\n",
    "$$\n",
    "P(w_c|w_{c-m},\\ldots,w_{c-1},w_{c+1},w_{c+m})\n",
    "$$\n",
    "\n",
    "Para eso contamos con las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_dict={}, frequencies_dict={}):\n",
    "        \n",
    "        self._idx_to_tk = tokens_dict\n",
    "        self._tk_to_idx = {tk: idx for idx, tk in tokens_dict.items()}\n",
    "        self._idx_to_freq = frequencies_dict\n",
    "        self.max_idx = len(self)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, cutoff_freq=0):\n",
    "        corpus_words = sorted(list(set([item for sublist in corpus for item in sublist])))\n",
    "        freqs_dict = {word: 0 for word in corpus_words}\n",
    "        for doc in corpus:\n",
    "            for token in doc:\n",
    "                freqs_dict[token] += 1\n",
    "        freqs = np.array(list(freqs_dict.values()))\n",
    "        mask = freqs > cutoff_freq\n",
    "        corpus_words = {idx: tk for idx, tk in enumerate(itertools.compress(corpus_words,mask))}\n",
    "        freqs = {idx: freq for idx, freq in enumerate(freqs[mask])}\n",
    "        return cls(corpus_words, freqs)\n",
    "\n",
    "    def index_to_token(self, index):\n",
    "        return self._idx_to_tk[index]\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        return self._tk_to_idx[token]\n",
    "        \n",
    "    def get_freq(self, tk_or_idx):\n",
    "        \n",
    "        if isinstance(tk_or_idx, int):\n",
    "            freq = self._idx_to_freq[tk_or_idx]\n",
    "        elif isinstance(tk_or_idx, str):\n",
    "            freq = 0 if tk_or_idx not in self._tk_to_idx else self._idx_to_freq[self._tk_to_idx[tk_or_idx]]\n",
    "        else:\n",
    "            raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._idx_to_tk)\n",
    "    \n",
    "    def __getitem__(self,tk_or_idx):\n",
    "        if isinstance(tk_or_idx, int):\n",
    "            return self.index_to_token(tk_or_idx)\n",
    "        if isinstance(tk_or_idx, str):\n",
    "            return self.token_to_index(tk_or_idx)\n",
    "        raise KeyError('{} must be either integer or string'.format(tk_or_idx))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current >= self.max_idx:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            token = self._idx_to_tk[self.current]\n",
    "            self.current += 1\n",
    "            return token\n",
    "\n",
    "    def __contains__(self,key):\n",
    "        return key in self._tk_to_idx\n",
    "    \n",
    "    \n",
    "\n",
    "def samples_generator1(doc, vocabulary, window_size):\n",
    "    for t, token in enumerate(doc):\n",
    "        if token in vocabulary:\n",
    "            len_doc = len(doc)\n",
    "            cond1 = max(-1,t-window_size) == -1\n",
    "            cond2 = min(t+window_size, len_doc) == len_doc\n",
    "            if cond1 and cond2:\n",
    "                context = itertools.chain(doc[:t],doc[t+1:])\n",
    "            if cond1 and not cond2:\n",
    "                context = itertools.chain(doc[:t],doc[t+1:t+window_size+1])\n",
    "            if cond2 and not cond1:\n",
    "                context = itertools.chain(doc[t-window_size:t],doc[t+1:])\n",
    "            if not cond1 and not cond2:\n",
    "                context = itertools.chain(doc[t-window_size:t],doc[t+1:t+window_size+1])\n",
    "\n",
    "            context_list = [vocabulary.token_to_index(tk) for tk in context if tk in vocabulary]\n",
    "            if len(context_list) != 0:\n",
    "                yield (vocabulary.token_to_index(token), context_list)\n",
    "    \n",
    "    \n",
    "def samples_generator2(doc, vocabulary, window_size, padding_idx):\n",
    "    for t, token in enumerate(doc):\n",
    "        if token in vocabulary:\n",
    "            len_doc = len(doc)\n",
    "            cond1 = max(-1,t-window_size) == -1\n",
    "            cond2 = min(t+window_size, len_doc) == len_doc\n",
    "            if cond1 and cond2:\n",
    "                context = itertools.chain(doc[:t],doc[t+1:])\n",
    "            if cond1 and not cond2:\n",
    "                context = itertools.chain(doc[:t],doc[t+1:t+window_size+1])\n",
    "            if cond2 and not cond1:\n",
    "                context = itertools.chain(doc[t-window_size:t],doc[t+1:])\n",
    "            if not cond1 and not cond2:\n",
    "                context = itertools.chain(doc[t-window_size:t],doc[t+1:t+window_size+1])\n",
    "\n",
    "            context_list = [vocabulary.token_to_index(tk) for tk in context if tk in vocabulary]\n",
    "            yield (vocabulary.token_to_index(token), context_list)\n",
    "\n",
    "corpus = [['w1', 'w2', 'w3', 'w4'], ['w1', 'w3', 'w3', 'w3'], ['w1'], ['w1', 'w2', 'w3', 'w4', 'w1', 'w2', 'w3', 'w4']]\n",
    "cutoff_freq = 3\n",
    "window_size = 2\n",
    "vocabulary = Vocabulary.from_corpus(corpus,cutoff_freq=cutoff_freq)\n",
    "padding_idx = len(vocabulary)\n",
    "\n",
    "word_indeces = []\n",
    "word_contexts = []\n",
    "for doc in corpus:\n",
    "    gen = samples_generator1(doc, vocabulary, window_size)\n",
    "    for word_index, word_context in gen:\n",
    "        word_indeces.append(word_index)\n",
    "        padd_num = 2 * window_size - len(word_context)\n",
    "        if padd_num > 0:\n",
    "            word_contexts.append(word_context + [padding_idx for i in range(padd_num)])\n",
    "        else:\n",
    "            word_contexts.append(word_context)\n",
    "\n",
    "word_indeces = torch.tensor(word_indeces,dtype=torch.long)\n",
    "context_indeces = torch.tensor(word_contexts,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5702, 0.0000],\n",
      "        [2.0046, 2.0046],\n",
      "        [1.9594, 2.4536]]) tensor(1.4987)\n",
      "[tensor([0.5702, 2.0046, 1.9594]), tensor([0.0000, 2.0046, 2.4536])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "target = torch.tensor([[1,5],[1,1],[0,3]])\n",
    "scores = torch.randn(3,vocab_size).view(-1,vocab_size,1).repeat(1,1,target.size(1))\n",
    "lf = nn.CrossEntropyLoss(ignore_index=vocab_size,reduction='none')\n",
    "loss1 = lf(scores,target)\n",
    "print(loss1, loss1.mean())\n",
    "\n",
    "loss2 = [lf(scores[:,:,0],target[:,0]), lf(scores[:,:,0],target[:,1])]\n",
    "print(loss2)\n",
    "\n",
    "lf(torch.rand(1,5),torch.tensor([5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
