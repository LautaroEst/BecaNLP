{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IntroUtils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a NLP\n",
    "\n",
    "Todas las tareas de Procesamiento de Languaje Natural se pueden reducir a un proceso que implica recibir un texto y tomar una decisión a partir de él. Por ejemplo, si se quiere deducir si el contenido de un *tweet* tiene una connotación positiva o negativa va a ser necesario hacer una función capaz de recibir el contenido del *tweet* y decidir si pertence a alguna de las clases \"positivo\" y \"negativo\".\n",
    "\n",
    "FOTO DE DOS TWEETS POSITIVOS Y NEGATIVOS\n",
    "\n",
    "Como puede observarse, esta tarea puede resultar muy ambigua a veces, por lo que habría que determinar un criterio bien claro de qué se considera positivo y qué se considera negativo. \n",
    "\n",
    "Otro ejemplo es el de predicción de texto. Dado una secuencia de palabras, se desea decidir la siguiente palabra que completa la frase:\n",
    "\n",
    "EJEMPLO DE PREDICCIÓN DE TEXTO\n",
    "\n",
    "Acá se ve que no hay una única opción para completar la frase y que el texto mantenga una coherencia. Por ahora no vamos a lidiar con el problema de la ambigüedad, puesto que hay que determinar otras cosas más básicas antes. \n",
    "\n",
    "Por eso, lo primero que vamos a hacer va a ser explicar cómo resolver tareas comunes en NLP a partir de un conjunto de datos. Después, vamos a entrar en el tema de modelos de lenguaje, que nos va a permitir entender *word embeddings* tradicionales y contextuales (transformers), que son temas importantes y que mejoran mucho la performance de casi todas las tareas de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Text Classification*\n",
    "\n",
    "Supongamos que tenemos acceso a un conjunto de comentarios y *reviews* sobre una película y queremos identificar cuáles de ellos consideran que la película fue buena y cuáles consideran que fue mala. Supongamos también que las únicas dos opciones son \"la película fue buena\" y \"la película fue mala\", y que cada comentario contiene exactamente una de estas dos calificaciones. Es decir, no hay calificaciones intermedias, y no hay comentarios sin calificación. \n",
    "\n",
    "El problema de clasificar automáticamente un texto entre dos categorías correspondientes a una calificación positiva y una calificación negativa se conoce como **análisis de sentimientos** (*sentiment analysis*). También podemos proponer el caso en que se quiera decidir entre más de dos categorías (por ejemplo, \"muy mala\", \"mala\", \"más o menos\", \"buena\", \"muy buena\" y \"el padrino\"), y el problema seguiría siendo el mismo. \n",
    "\n",
    "En general, el problema de decidir automáticamente a qué categoría pertenecen ciertas muestras de texto se conoce como **clasificación de texto** (*text classification*), y se puede aplicar a diferentes tareas. Algunas variantes comunes, además de análisis de dentimientos, son:\n",
    "\n",
    "* ***Topic label:*** Clasificar un conjunto de notas periodísticas por temática (deportes, internacional, política, etc.)\n",
    "* ***Spam identification:*** Decidir si un mail es un *spam* o no.\n",
    "* ***Language detection:*** Identificar el idioma en que está escrito el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Clasificación de reviews de celulares\n",
    "\n",
    "Vamos a utilizar el corpus `Amazon_Unlocked_Mobile.csv` para clasificar comentarios positivos y negativos sobre modelos y marcas de celulares. Ésta es una base de datos obtenida con *reviews* de Amazon, calificados con un puntaje de 1 a 5 estrellas. A partir de ella, seleccionamos los reviews que tengan una connotación positiva (calificados con 4 o 5 estrellas) y los que tienen una negativa (1 o 2 estellas). De esta manera, se tiene para cada comentario, una etiqueta ($y=1$) si el comentario fue positivo, y otra ($y=0$) si el comentario fue negativo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel so lucky to have found this used (phone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>very pleased</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it works good but it goes slow sometimes but i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great phone to replace my lost phone. the only...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Positively Rated\n",
       "0  i feel so lucky to have found this used (phone...                 1\n",
       "1  nice phone, nice up grade from my pantach revu...                 1\n",
       "2                                       very pleased                 1\n",
       "3  it works good but it goes slow sometimes but i...                 1\n",
       "4  great phone to replace my lost phone. the only...                 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Utils/Datasets/Amazon-cellphone-reviews/Amazon_Unlocked_Mobile.csv')\n",
    "#df = df.sample(frac=0.1, random_state=10)\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['Rating'] != 3]\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df = df[['Reviews','Positively Rated']]\n",
    "df['Reviews'] = df['Reviews'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en entrenamiento, validación y testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de muestras de train: 262035\n",
      "Cantidad de muestras de validación: 15414\n",
      "Cantidad de muestras de test: 30828\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2635147587)\n",
    "\n",
    "data_size = len(df)\n",
    "train_size, val_size = .85, .05\n",
    "perm = np.random.permutation(data_size)\n",
    "train_idx, val_idx, test_idx = perm[:int(train_size*data_size)], perm[int(train_size*data_size):int((train_size+val_size)*data_size)], perm[int((train_size+val_size)*data_size):]\n",
    "train_data, val_data, test_data = df.iloc[train_idx], df.iloc[val_idx], df.iloc[test_idx]\n",
    "\n",
    "print('Cantidad de muestras de train:',len(train_data))\n",
    "print('Cantidad de muestras de validación:',len(val_data))\n",
    "print('Cantidad de muestras de test:',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidas las muestras, es necesario definir el vocabulario de palabras que pueden aparecer en cada muestra, con el objetivo de hacer una codificación que sirva para entrenar nuestro modelo. \n",
    "\n",
    "Dado un vocabulario $V = \\{ w_1, w_2, \\ldots, w_{|V|} \\}$, se le asigna a cada palabra del vocabulario un vector *one-hot*. Es decir que la palabra $w_j \\in V$ queda representada por un vector\n",
    "\n",
    "$$\n",
    "h_j = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{|V|}\n",
    "$$\n",
    "\n",
    "con la j-ésima componente igual a 1, y cero es todas las demás. De esta manera, un comentario conformado por una serie de palabras pertenecientes a $V$ se puede representar como la suma de los vectores *one-hot* de cada una de esas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, vocabulary):\n",
    "        \n",
    "        self.ds_x = df['Reviews'].str.split(' ')\n",
    "        self.y = torch.from_numpy(df['Positively Rated'].to_numpy()).type(torch.float)\n",
    "        self.vocab = vocabulary\n",
    "        self.vocab_size = len(vocabulary)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        x = torch.zeros(self.vocab_size)\n",
    "        comment = self.ds_x.iloc[idx]\n",
    "        for word in comment:\n",
    "            x[self.vocab[word]] += 1\n",
    "        return x, self.y[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "amazon_vocabulary = Vocabulary.from_dataseries(df['Reviews'])\n",
    "train_dataset = AmazonDataset(train_data, amazon_vocabulary)\n",
    "val_dataset = AmazonDataset(val_data, amazon_vocabulary)\n",
    "test_dataset = AmazonDataset(test_data, amazon_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a entrenar un modelo de regresión logística con estas muestras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, train_dataset, val_dataset=None, batch_size=128, device='cpu'):\n",
    "        vocab_size = len(train_dataset[0][0])\n",
    "        model = self.Model(vocab_size)\n",
    "        super().__init__(model,train_dataset,val_dataset,batch_size,device)\n",
    "    \n",
    "    class Model(nn.Module):\n",
    "        \n",
    "        def __init__(self, vocab_size):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(vocab_size,1)\n",
    "        \n",
    "        def forward(self,x):\n",
    "            return self.linear(x).squeeze()\n",
    "        \n",
    "    def Loss(self,scores,targets):\n",
    "        lf = nn.BCEWithLogitsLoss()\n",
    "        return lf(scores,targets)\n",
    "    \n",
    "    def CheckAccuracy(self):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_dataloader:\n",
    "                x = x.to(device=self.device)  \n",
    "                y = y.to(device=self.device)\n",
    "\n",
    "                scores = self.model(x)\n",
    "                preds = (torch.sigmoid(scores) > .5).type(torch.long)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "        self.model.train()\n",
    "        return ', Accuracy: {}/{} ({:.1f}%)'.format(num_correct,num_samples,float(num_correct)/float(num_samples)*100)\n",
    "\n",
    "trainer = LogisticRegressionTrainer(train_dataset, val_dataset=val_dataset, batch_size=128, device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 10\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.6939, Accuracy: 11066/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 100, Loss: 0.5462, Accuracy: 13199/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 200, Loss: 0.4879, Accuracy: 13274/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 300, Loss: 0.4502, Accuracy: 13559/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 400, Loss: 0.4131, Accuracy: 13568/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 500, Loss: 0.3638, Accuracy: 13617/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 600, Loss: 0.3027, Accuracy: 13665/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 700, Loss: 0.3587, Accuracy: 13714/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 800, Loss: 0.3302, Accuracy: 13702/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 900, Loss: 0.3038, Accuracy: 13812/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1000, Loss: 0.3411, Accuracy: 13860/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1100, Loss: 0.2949, Accuracy: 13895/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1200, Loss: 0.3262, Accuracy: 13918/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1300, Loss: 0.3330, Accuracy: 13919/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1400, Loss: 0.2776, Accuracy: 13969/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1500, Loss: 0.2708, Accuracy: 13961/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1600, Loss: 0.3023, Accuracy: 13973/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1700, Loss: 0.2717, Accuracy: 14028/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1800, Loss: 0.3301, Accuracy: 14041/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 1900, Loss: 0.2636, Accuracy: 14060/15414 (0.0%)\n",
      "Epoch: 1, Batch number: 2000, Loss: 0.2453, Accuracy: 14027/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 52, Loss: 0.2204, Accuracy: 14066/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 152, Loss: 0.2783, Accuracy: 14077/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 252, Loss: 0.2640, Accuracy: 14064/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 352, Loss: 0.1736, Accuracy: 14093/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 452, Loss: 0.2284, Accuracy: 14101/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 552, Loss: 0.2017, Accuracy: 14108/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 652, Loss: 0.2524, Accuracy: 14143/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 752, Loss: 0.2823, Accuracy: 14160/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 852, Loss: 0.2248, Accuracy: 14167/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 952, Loss: 0.2158, Accuracy: 14169/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1052, Loss: 0.2718, Accuracy: 14177/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1152, Loss: 0.2049, Accuracy: 14198/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1252, Loss: 0.2025, Accuracy: 14211/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1352, Loss: 0.1849, Accuracy: 14241/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1452, Loss: 0.2652, Accuracy: 14250/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1552, Loss: 0.1664, Accuracy: 14251/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1652, Loss: 0.2052, Accuracy: 14266/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1752, Loss: 0.2227, Accuracy: 14268/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1852, Loss: 0.1332, Accuracy: 14279/15414 (0.0%)\n",
      "Epoch: 2, Batch number: 1952, Loss: 0.1644, Accuracy: 14283/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 4, Loss: 0.2009, Accuracy: 14298/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 104, Loss: 0.1707, Accuracy: 14313/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 204, Loss: 0.1553, Accuracy: 14315/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 304, Loss: 0.1958, Accuracy: 14322/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 404, Loss: 0.1394, Accuracy: 14331/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 504, Loss: 0.1898, Accuracy: 14338/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 604, Loss: 0.1089, Accuracy: 14349/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 704, Loss: 0.1206, Accuracy: 14347/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 804, Loss: 0.1530, Accuracy: 14357/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 904, Loss: 0.1362, Accuracy: 14350/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1004, Loss: 0.1820, Accuracy: 14354/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1104, Loss: 0.1551, Accuracy: 14364/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1204, Loss: 0.1637, Accuracy: 14390/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1304, Loss: 0.1761, Accuracy: 14398/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1404, Loss: 0.2246, Accuracy: 14405/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1504, Loss: 0.1607, Accuracy: 14407/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1604, Loss: 0.1527, Accuracy: 14413/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1704, Loss: 0.1469, Accuracy: 14418/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1804, Loss: 0.1429, Accuracy: 14418/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 1904, Loss: 0.1464, Accuracy: 14425/15414 (0.0%)\n",
      "Epoch: 3, Batch number: 2004, Loss: 0.2028, Accuracy: 14444/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 56, Loss: 0.1948, Accuracy: 14433/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 156, Loss: 0.1989, Accuracy: 14427/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 256, Loss: 0.1229, Accuracy: 14440/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 356, Loss: 0.1485, Accuracy: 14441/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 456, Loss: 0.1198, Accuracy: 14445/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 556, Loss: 0.1973, Accuracy: 14478/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 656, Loss: 0.1923, Accuracy: 14450/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 756, Loss: 0.1683, Accuracy: 14456/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 856, Loss: 0.1443, Accuracy: 14482/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 956, Loss: 0.1679, Accuracy: 14472/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1056, Loss: 0.1881, Accuracy: 14473/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1156, Loss: 0.1114, Accuracy: 14474/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1256, Loss: 0.1769, Accuracy: 14463/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1356, Loss: 0.0988, Accuracy: 14476/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1456, Loss: 0.1066, Accuracy: 14486/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1556, Loss: 0.1167, Accuracy: 14478/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1656, Loss: 0.2314, Accuracy: 14476/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1756, Loss: 0.1334, Accuracy: 14485/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1856, Loss: 0.1282, Accuracy: 14472/15414 (0.0%)\n",
      "Epoch: 4, Batch number: 1956, Loss: 0.1280, Accuracy: 14494/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 8, Loss: 0.1550, Accuracy: 14507/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 108, Loss: 0.2043, Accuracy: 14510/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 208, Loss: 0.1168, Accuracy: 14510/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 308, Loss: 0.1088, Accuracy: 14509/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 408, Loss: 0.1236, Accuracy: 14511/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 508, Loss: 0.1311, Accuracy: 14525/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 608, Loss: 0.1351, Accuracy: 14523/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 708, Loss: 0.1516, Accuracy: 14534/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 808, Loss: 0.1445, Accuracy: 14529/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 908, Loss: 0.1431, Accuracy: 14521/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1008, Loss: 0.1238, Accuracy: 14545/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1108, Loss: 0.1644, Accuracy: 14535/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1208, Loss: 0.1460, Accuracy: 14530/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1308, Loss: 0.1350, Accuracy: 14526/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1408, Loss: 0.1579, Accuracy: 14548/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1508, Loss: 0.1262, Accuracy: 14561/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1608, Loss: 0.1457, Accuracy: 14537/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1708, Loss: 0.0806, Accuracy: 14550/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1808, Loss: 0.1427, Accuracy: 14553/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 1908, Loss: 0.1225, Accuracy: 14548/15414 (0.0%)\n",
      "Epoch: 5, Batch number: 2008, Loss: 0.1364, Accuracy: 14562/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 60, Loss: 0.1366, Accuracy: 14559/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 160, Loss: 0.1351, Accuracy: 14561/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 260, Loss: 0.1075, Accuracy: 14561/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 360, Loss: 0.1306, Accuracy: 14566/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 460, Loss: 0.1020, Accuracy: 14564/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 560, Loss: 0.1085, Accuracy: 14577/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 660, Loss: 0.1102, Accuracy: 14565/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 760, Loss: 0.1664, Accuracy: 14569/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 860, Loss: 0.1082, Accuracy: 14578/15414 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Batch number: 960, Loss: 0.1035, Accuracy: 14576/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1060, Loss: 0.0727, Accuracy: 14574/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1160, Loss: 0.1485, Accuracy: 14576/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1260, Loss: 0.1926, Accuracy: 14581/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1360, Loss: 0.0766, Accuracy: 14581/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1460, Loss: 0.1660, Accuracy: 14580/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1560, Loss: 0.1488, Accuracy: 14581/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1660, Loss: 0.1009, Accuracy: 14584/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1760, Loss: 0.1605, Accuracy: 14594/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1860, Loss: 0.1215, Accuracy: 14585/15414 (0.0%)\n",
      "Epoch: 6, Batch number: 1960, Loss: 0.1613, Accuracy: 14588/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 12, Loss: 0.1195, Accuracy: 14589/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 112, Loss: 0.1037, Accuracy: 14601/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 212, Loss: 0.1206, Accuracy: 14596/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 312, Loss: 0.1156, Accuracy: 14597/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 412, Loss: 0.1452, Accuracy: 14587/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 512, Loss: 0.0849, Accuracy: 14595/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 612, Loss: 0.0874, Accuracy: 14593/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 712, Loss: 0.1106, Accuracy: 14589/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 812, Loss: 0.1329, Accuracy: 14600/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 912, Loss: 0.2036, Accuracy: 14596/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1012, Loss: 0.1227, Accuracy: 14598/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1112, Loss: 0.1029, Accuracy: 14594/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1212, Loss: 0.1606, Accuracy: 14599/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1312, Loss: 0.1264, Accuracy: 14613/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1412, Loss: 0.1214, Accuracy: 14615/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1512, Loss: 0.1130, Accuracy: 14614/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1612, Loss: 0.1563, Accuracy: 14616/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1712, Loss: 0.1631, Accuracy: 14607/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1812, Loss: 0.1433, Accuracy: 14615/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 1912, Loss: 0.1175, Accuracy: 14611/15414 (0.0%)\n",
      "Epoch: 7, Batch number: 2012, Loss: 0.1309, Accuracy: 14622/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 64, Loss: 0.0653, Accuracy: 14641/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 164, Loss: 0.1360, Accuracy: 14632/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 264, Loss: 0.1005, Accuracy: 14630/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 364, Loss: 0.1076, Accuracy: 14633/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 464, Loss: 0.1204, Accuracy: 14642/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 564, Loss: 0.0990, Accuracy: 14645/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 664, Loss: 0.0977, Accuracy: 14645/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 764, Loss: 0.1189, Accuracy: 14628/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 864, Loss: 0.1031, Accuracy: 14630/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 964, Loss: 0.0606, Accuracy: 14637/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1064, Loss: 0.1221, Accuracy: 14628/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1164, Loss: 0.1109, Accuracy: 14636/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1264, Loss: 0.1082, Accuracy: 14644/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1364, Loss: 0.1144, Accuracy: 14645/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1464, Loss: 0.1468, Accuracy: 14646/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1564, Loss: 0.1960, Accuracy: 14649/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1664, Loss: 0.0912, Accuracy: 14644/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1764, Loss: 0.1224, Accuracy: 14653/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1864, Loss: 0.0917, Accuracy: 14649/15414 (0.0%)\n",
      "Epoch: 8, Batch number: 1964, Loss: 0.0911, Accuracy: 14649/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 16, Loss: 0.0822, Accuracy: 14653/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 116, Loss: 0.0898, Accuracy: 14646/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 216, Loss: 0.1221, Accuracy: 14657/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 316, Loss: 0.0663, Accuracy: 14639/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 416, Loss: 0.0971, Accuracy: 14645/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 516, Loss: 0.1413, Accuracy: 14651/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 616, Loss: 0.1212, Accuracy: 14659/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 716, Loss: 0.0799, Accuracy: 14661/15414 (0.0%)\n",
      "Epoch: 9, Batch number: 816, Loss: 0.0935, Accuracy: 14663/15414 (0.0%)\n",
      "Exiting training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algorithm = 'Adam'\n",
    "epochs = 10\n",
    "sample_loss_every = 100\n",
    "lr = 1e-3\n",
    "trainer.Train(algorithm, epochs, sample_loss_every, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f11b4e871d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados en el conjunto de test: 29315/30828 (95.1%)\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = 0\n",
    "\n",
    "samples_idx = torch.randperm(len(test_dataset))\n",
    "my_sampler = lambda indices: sampler.SubsetRandomSampler(indices)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, sampler=my_sampler(samples_idx))\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dataloader:\n",
    "        x = x.to(device=device)  \n",
    "        y = y.to(device=device)\n",
    "\n",
    "        scores = trainer.model(x)\n",
    "        preds = (torch.sigmoid(scores) > .5).type(torch.long)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "trainer.model.train()\n",
    "\n",
    "print('Resultados en el conjunto de test: {}/{} ({:.1f}%)'.format(num_correct,num_samples,float(num_correct)/float(num_samples)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2: Clasificación de noticias con RNN\n",
    "\n",
    "TO DO :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Text Prediction*\n",
    "\n",
    "Ahora veamos el caso de **predicción de texto**. Acá lo que queremos hacer es decidir, a partir de una secuencia de palabras que forman una frase sin el final, cómo se completa dicha frase. Por ejemplo:\n",
    "\n",
    "Cuando el profesor entró, los alumnos abrieron sus ____\n",
    "\n",
    "De cada diez personas que miran televisión, cinco son ____\n",
    "\n",
    "Hoy no voy al gimnasio, pero mañana ____\n",
    "\n",
    "Esta es una tarea que no tiene una única respuesta, dado que hay muchas maneras de completar cada una de las frases anteriores. Sin mencionar, además, que no existen conjuntos de entrenamiento para aprender a completar frases como estas. \n",
    "\n",
    "Para formalizar lo dicho anteriormente, decimos que, dado un vocabulario $V = \\{ w_1, \\ldots, w_{|V|} \\}$, y un conjunto de palabras $v_1, v_2, \\ldots, v_{n-1} \\in V$, queremos obtener la palabra $v_n$ que saldrá a continuación, con el criterio de decisión bayesiano: \n",
    "\n",
    "$$\n",
    "v_n = \\mathrm{argmax}_{w \\in V} P(w|v_1,\\ldots,v_{n-1})\n",
    "$$\n",
    "\n",
    "El objetivo, entonces, será aprender un modelo que permita calcular la probabilidad de que salga cualquier palabra del vocabulario, dado que se observó un conjunto de palabras arbitrario del mismo vocabulario.\n",
    "\n",
    "Una forma de medir el desempeño de esta tarea es a partir de la perplejidad. Dado un texto compuesto por una sucesión de palabras $w_1,\\ldots,w_N$ pertenecientes a un vocabulario $V$ y un modelo de aprendizaje que me permite obtener la probabilidad de cualquier secuencia de palabras pertenecientes a dicho vocabulario, la perplejidad de ese modelo se define como\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{P(w_1,\\ldots,w_N)}\n",
    "$$\n",
    "\n",
    "Ahora, si se considera que la probabilidad de ocurrencia de una palabra depende sólamente de sus $n-1$ palabras anteriores, entonces es razonable la siguiente aproximación:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p &= \\frac{1}{P(w_1,\\ldots,w_N)} \\\\[.5em]\n",
    "p &= \\frac{1}{P(w_1)P(w_2|w_1)\\ldots P(w_n|w_{n-1},\\ldots,w_1)\\ldots P(w_N|w_{N-1},\\ldots,w_{N-n+1})} \\\\[.5em]\n",
    "p &= \\prod_{i=n}^{N} \\frac{1}{P(w_i|,w_{i-1}\\ldots,w_{i-n+1})}\\prod_{i=1}^{n-2}\\frac{1}{P(w_{n-i}|,w_{n-i+1}\\ldots,w_{1})}\\frac{1}{P(w_1)} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 3: Perplejidad sobre el corpus Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown', download_dir='/home/lestien/anaconda3/envs/TorchEnv/nltk_data')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estandarización de las utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.8 ms ± 584 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "62.6 ms ± 360 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "corpus_string  = read_text_file('../../Utils/Datasets/wikitext-2/wiki.test.tokens')\n",
    "def vocab_orddict(corpus_string):\n",
    "    corpus = corpus_string.split(' ')\n",
    "    vocab_words = set(corpus)\n",
    "    freqs_dict = OrderedDict({word: 0 for word in vocab_words})\n",
    "    for word in corpus:\n",
    "        freqs_dict[word] += 1\n",
    "    freqs_dict = OrderedDict(sorted(freqs_dict.items(), key=lambda t: t[1], reverse=True))\n",
    "    tk_to_idx = OrderedDict({tk:idx for idx,tk in enumerate(freqs_dict.keys())})\n",
    "    corpus_idx = [tk_to_idx[tk] for tk in corpus]\n",
    "\n",
    "def vocab_dict(corpus_string):\n",
    "    corpus = corpus_string.split(' ')\n",
    "    vocab_words = set(corpus)\n",
    "    freqs_dict = {word: 0 for word in vocab_words}\n",
    "    for word in corpus:\n",
    "        freqs_dict[word] += 1\n",
    "    tk_to_idx = {tk:idx for idx,tk in enumerate(freqs_dict.keys())}\n",
    "    corpus_idx = [tk_to_idx[tk] for tk in corpus]\n",
    "\n",
    "\n",
    "%timeit vocab_orddict(corpus_string)\n",
    "%timeit vocab_dict(corpus_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    __slots__ = ['_tk_to_freq', '_tk_to_idx']\n",
    "    \n",
    "    def __init__(self, tokens_freqs=None):\n",
    "        self._tk_to_freq = tokens_freqs\n",
    "        self._tk_to_idx = {} if tokens_freqs is None else {tk: idx for idx, tk in enumerate(tokens_freqs.keys())}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        corpus_words = set([tk for doc in corpus for tk in doc])\n",
    "        freqs_dict = {word: 0 for word in corpus_words}\n",
    "        for tk in itertools.chain.from_iterable(corpus):\n",
    "            freqs_dict[tk] += 1\n",
    "        return cls(freqs_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_string(cls, corpus, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = lambda x: x.split(' ')\n",
    "        return cls.from_corpus([tokenizer(corpus)])\n",
    "    \n",
    "    @classmethod\n",
    "    def from_wordlist(cls, corpus):\n",
    "        tokens_dict = {idx: tk for idx, tk in enumerate(corpus)}\n",
    "        return cls(tokens_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataseries(cls, ds, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = lambda x: x.split(' ')\n",
    "        corpus = [doc for doc in ds.apply(tokenizer)]\n",
    "        return cls.from_corpus(corpus)\n",
    "\n",
    "    def token_to_index(self, token):\n",
    "        return self._tk_to_idx[token]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Vocabulary(size={})>\".format(len(self))\n",
    "    \n",
    "    def __str__(self):\n",
    "        text = ''\n",
    "        for i, tk in enumerate(self):\n",
    "            text += tk + '\\n'\n",
    "            if i > 10:\n",
    "                text += '...'\n",
    "                break\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tk_to_idx)\n",
    "    \n",
    "    def __getitem__(self,tk):\n",
    "        self._tk_to_idx[tk]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (tk for tk in self._tk_to_idx.keys())\n",
    "\n",
    "    def __contains__(self,key):\n",
    "        return key in self._tk_to_idx.keys()\n",
    "    \n",
    "    def __add__(self,vocab):\n",
    "        new_corpus_words = set(vocab._idx_to_tk.values()).union(set(self._idx_to_tk.values()))\n",
    "        new_tk_to_freq = {tk: 0 for tk in new_corpus_words}\n",
    "        for idx, tk in enumerate(new_corpus_words):\n",
    "            freq = self._tk_to_freq[tk] if tk in self._idx_to_tk.values() else 0\n",
    "            if tk in vocab._tk_to_idx.keys():\n",
    "                freq += vocab._tk_to_freq[tk]\n",
    "            new_tk_to_freq[tk] = freq\n",
    "        return Vocabulary(new_tk_to_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de muestras de train: 262035\n",
      "Cantidad de muestras de validación: 15414\n",
      "Cantidad de muestras de test: 30828\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../Utils/Datasets/Amazon-cellphone-reviews/Amazon_Unlocked_Mobile.csv')\n",
    "#df = df.sample(frac=0.1, random_state=10)\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['Rating'] != 3]\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df = df[['Reviews','Positively Rated']]\n",
    "df['Reviews'] = df['Reviews'].str.lower()\n",
    "\n",
    "np.random.seed(2635147587)\n",
    "\n",
    "data_size = len(df)\n",
    "train_size, val_size = .85, .05\n",
    "perm = np.random.permutation(data_size)\n",
    "train_idx, val_idx, test_idx = perm[:int(train_size*data_size)], perm[int(train_size*data_size):int((train_size+val_size)*data_size)], perm[int((train_size+val_size)*data_size):]\n",
    "train_data, val_data, test_data = df.iloc[train_idx], df.iloc[val_idx], df.iloc[test_idx]\n",
    "\n",
    "print('Cantidad de muestras de train:',len(train_data))\n",
    "print('Cantidad de muestras de validación:',len(val_data))\n",
    "print('Cantidad de muestras de test:',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, vocabulary):\n",
    "        \n",
    "        self.ds_x = df['Reviews'].str.split(' ')\n",
    "        self.y = torch.from_numpy(df['Positively Rated'].to_numpy()).type(torch.float)\n",
    "        self.vocab = vocabulary\n",
    "        self.vocab_size = len(vocabulary)\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        x = torch.zeros(self.vocab_size)\n",
    "        for word in text:\n",
    "            x[self.vocab[word]] += 1\n",
    "        return x\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        x = self.vectorize(self.ds_x.iloc[idx])\n",
    "        return x, self.y[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "amazon_vocabulary = Vocabulary.from_dataseries(df['Reviews'])\n",
    "train_dataset = AmazonDataset(train_data, amazon_vocabulary)\n",
    "val_dataset = AmazonDataset(val_data, amazon_vocabulary)\n",
    "test_dataset = AmazonDataset(test_data, amazon_vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
