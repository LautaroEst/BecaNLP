{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación de Word Vectors\n",
    "\n",
    "Todos los métodos para obtener una representación del significado de una palabra utilizan como punto de partida un vocabulario $V = \\{ casa, árbol, él, el, la, ellos, ellas, \\ldots \\}$ finito.\n",
    "\n",
    "## Métodos por PCA\n",
    "\n",
    "Explicar.\n",
    "\n",
    "## Algoritmos `word2vec`\n",
    "\n",
    "> **Resumen:** El objetivo acá es obtener una representación del significado de las palabras como producto secundario de un modelo de aprendizaje supervisado. Es decir, se va a diseñar un clasificador que permita predecir una palabra a partir de otra (ambas pertenecientes al vocabulario $V$) y se van a utilizar los parámetros del modelo como representación del significado de la palabra. \n",
    "\n",
    "Dado un vocabulario $V$, de tamaño $|V|$, se define que\n",
    "\n",
    "$$\n",
    "h_j = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{|V|}\n",
    "$$\n",
    "\n",
    "es un vector *one-hot* con su j-ésima coordenada igual a 1, y cero en otro lado. También definimos las variables aleatorias $\\mathbf{w}$ y $\\mathbf{c}$ con realizaciones en $\\mathbb{R}^{|V|}$ que representan la ocurrencia de una palabra central y de su contexto, respectivamente, a través de su correspondiente vector *one-hot*. Por ejemplo, para un vocabulario $V=\\{ casa, conjunto, el, diez, vacío, la, ellos, ellas, \\ldots \\}$, las palabras \"conjunto\" y \"vacío\" se representan por los vectores $h_2$ y $h_5$, respectivamente. Además, la frase \"conjunto vacío\" puede armarse de dos maneras posibles: \n",
    "1. \"conjunto\" es la palabra central y \"vacío\" es su contexto (en cuyo caso $\\mathbf{w}$ habrá adoptado el valor $h_2$ y $\\mathbf{c}$ el valor $h_5$), o bien\n",
    "2. \"vacío\" es la palabra central y \"conjunto\" es su contexto (en cuyo caso $\\mathbf{w}$ habrá adoptado el valor $h_5$ y $\\mathbf{c}$ el valor $h_2$).\n",
    "Para el caso en que se tiene una frase compuesta por más de dos palabras, se suele tomar el promedio de los vectores *one-hot* que conforman la frase para representar el contexto o la palabra central. Por ejemplo, para el caso de la frase \"el conjunto vacío\" en que se toma a \"conjunto\" como palabra central y a $\\{\"\\!el\\!\", \"\\!vacío\\!\"\\}$ como contexto, se podrá considerar que las variables $\\mathbf{w}$ y $\\mathbf{c}$ adoptaron los valores $h_2$ y $\\frac{1}{2}(h_3 + h_5)$ respectivamente.\n",
    "\n",
    "Con estas definiciones, podemos formalizar el funcionamiento básico de los algoritmos `word2vec`. Dado un vocabulario $V$ y un corpus de texto (cuyas palabras estén contenidas en $V$), se buscará estimar la probabilidad de que, dado un contexto $c$ se obtenga una palabra central $w$:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{w}=w | \\mathbf{c}=c)\n",
    "$$\n",
    "\n",
    "Con esta probabilidad, puede definirse un clasificador bayesiano que asigne una palabra $w$ a un determinado contexto $c$ y tomar como medida de la performance el error de predicciones acertadas:\n",
    "\n",
    "![alt text](\"word2vec.png\")\n",
    "\n",
    "Ahora pasamos a estimar la probabilidad $P(w|c)$. Para un problema de clasificación en el cual se estima la probabilidad de cada clase dado el vector de *features* por una softmax, normalmente se tiene que\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(y|x;W) &= softmax(Wx) \\\\[.5em]\n",
    "&= \\frac{e^{Wx}}{\\sum_{i=1}^{K}e^{w_i^Tx}} \\\\[.5em]\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "P(y=1|x) \\\\\n",
    "\\vdots \\\\\n",
    "P(y=K|x)\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "con\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "- w_1^T - \\\\\n",
    "\\vdots \\\\\n",
    "- w_K^T - \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "y la estimación por *maximum likelihood* resulta en encontrar la matriz $W$ que minimice la función\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "NLL(W) &= -log(l(W)) = -log\\left( \\prod_{i=1}^N P(y^{(i)}|x^{(i)};W)\\right)\\\\[.5em]\n",
    "&= - \\sum_{i=1}^N log\\left(P(y^{(i)}|x^{(i)};W)\\right)\\\\[.5em]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Con el fin de obtener un algoritmo iterativo, se puede encontrar el gradiente de $NLL(W)$ e iterar hasta llegar al mínimo:\n",
    "\n",
    "$$\n",
    "W_{new} \\leftarrow W_{old} - \\mu \\nabla_W NLL(W)\n",
    "$$\n",
    "\n",
    "que para el caso de la softmax, el gradiente tiene la forma\n",
    "\n",
    "$$\n",
    "\\nabla_W NLL(W) = \n",
    "\\begin{bmatrix}\n",
    "| & & | \\\\\n",
    "\\sum_{n=1}^N x^{(n)} \\left(softmax(w_{y^{(n)}}^T x^{(n)}) - \\mathbb{1}_{\\{y^{(n)}=1\\}}\\right) & \\ldots & \\sum_{n=1}^N x^{(n)} \\left(softmax(w_{y^{(n)}}^T x^{(n)}) - \\mathbb{1}_{\\{y^{(n)}=K\\}}\\right) \\\\\n",
    "| & & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\newcommand{\\U}{\\mathcal{U}}\n",
    "\\newcommand{\\V}{\\mathcal{V}}\n",
    "$$\n",
    "\n",
    "\n",
    "En este caso, se va a estimar la probabilidad a posteriori mediante una softmax, pero se va a tomar $W=\\U\\V$, donde \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\U = \n",
    "\\begin{bmatrix}\n",
    "- \\;u_1\\; - \\\\\n",
    "\\vdots \\\\\n",
    "- \\;u_{|V|}\\; - \n",
    "\\end{bmatrix} \\in \\mathbb{R}^{|V|\\times n} & \\hspace{2em} &\n",
    "\\V = \n",
    "\\begin{bmatrix}\n",
    "| & & | \\\\\n",
    "v_1 & \\ldots & v_{|V|} \\\\\n",
    "| & & | \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n\\times |V|}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
