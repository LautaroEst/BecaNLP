{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos distribucionales neuronales\n",
    "\n",
    "A continuación se describirán algunos de los métodos para obtener VSM con redes neuronales. Estos métodos también hacen uso de la hipótesis distribucional, pero estimando la probabilidad de ocurrencia $P(w_i|w_j)$ de la salida de una red neuronal a definir.\n",
    "\n",
    "Métodos a definir:\n",
    "\n",
    "* Word2Vec\n",
    "* GloVe\n",
    "* RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `word2vec`\n",
    "\n",
    "El algoritmo `word2vec` presenta dos modelos para estimar la probabilidad de un modelo de lenguaje con redes neuronales. Esto significa que el modelo estima la probabilidad de que, dada una serie de palabras $w_1,\\ldots, w_N$ se obtenga una palabra $w_c$ del vocabulario. \n",
    "\n",
    "El primero de ellos, estima esta probabilidad a partir de muestras de contextos y sus palabras centrales, es decir, $P(w_c|w_{c-n},\\ldots,w_{c-1},w_{c+1},\\ldots,w_{c+n})$ (**CBOW**).\n",
    "\n",
    "El segundo, hace lo opuesto: estima la misma probabilidad a partir de las palabras centrales y les muestra los contextos (**Skip-Gram**).\n",
    "\n",
    "Ambos métodos utilizan como función de costo la función de máxima verosimilud, que en el contexto de clasificación es igual a la cross-entropy entre la distribución resultante y la verdadera. También es posible plantear otras funciones de costo para optimizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c51ab85655eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mROOT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../Utils/Datasets/aclImdb/train/unsup/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0msplit_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcorpus_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_tk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus_and_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_tk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-c51ab85655eb>\u001b[0m in \u001b[0;36mget_corpus_and_vocab\u001b[0;34m(root, split_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcorpus_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdoc_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Definimos el corpus y el vocabulario\n",
    "\n",
    "import os\n",
    "\n",
    "def get_corpus_and_vocab(root, split_fn):\n",
    "    filenames = os.listdir(ROOT_PATH)\n",
    "    corpus_idx = []\n",
    "    tk_to_idx = {}\n",
    "    vocab_len = 0\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(root,filename), 'r') as f:\n",
    "            doc = split_fn(f.read())\n",
    "        corpus_idx.append([])\n",
    "        doc_idx = corpus_idx[-1]\n",
    "        for tk in doc:\n",
    "            if tk not in tk_to_idx:\n",
    "                tk_to_idx[tk] = vocab_len\n",
    "                vocab_len += 1\n",
    "            doc_idx.append(tk_to_idx[tk])\n",
    "            \n",
    "    \n",
    "    idx_to_tk = {idx: tk for tk, idx in tk_to_idx.items()}\n",
    "    return corpus_idx, idx_to_tk\n",
    "\n",
    "#corpus = [['w1', 'w2', 'w3', 'w4', 'w5'], ['w2', 'w2', 'w5', 'w4']]\n",
    "ROOT_PATH = '../../Utils/Datasets/aclImdb/train/unsup/'\n",
    "split_fn = lambda x: x.split(' ')\n",
    "corpus_idx, idx_to_tk = get_corpus_and_vocab(ROOT_PATH, split_fn)\n",
    "get_size(corpus_idx) + get_size(idx_to_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowTextDataset(Dataset):\n",
    "    \n",
    "    def get_samples(self, corpus_idx, left_n, right_n):\n",
    "        unk_token_idx = len(self.tk_to_idx)\n",
    "        context_size = left_n + right_n\n",
    "        words = []\n",
    "        contexts = []\n",
    "        for doc in corpus_idx:\n",
    "            for i in range(left_n):\n",
    "                doc.insert(0,unk_token_idx)\n",
    "            for i in range(right_n):\n",
    "                doc.append(unk_token_idx)\n",
    "            for i, idx in enumerate(doc[left_n:-right_n],left_n):\n",
    "                words.append(idx)\n",
    "                contexts.append(doc[i-left_n:i] + doc[i+1:i+right_n+1])\n",
    "            for i in range(left_n):\n",
    "                doc.pop(0)\n",
    "            for i in range(right_n):\n",
    "                doc.pop(-1)\n",
    "        words = torch.tensor(words).view(-1,1).repeat(1,context_size).view(-1)\n",
    "        contexts = torch.tensor(contexts).view(-1)\n",
    "        mask = (words != unk_token_idx) * (contexts != unk_token_idx)\n",
    "        return words[mask], contexts[mask]\n",
    "    \n",
    "    def __init__(self,corpus_idx,left_n,right_n):\n",
    "        self.words, self.contexts = self.get_samples(corpus_idx,left_n,right_n)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.words[idx], self.contexts[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "types_list = list(vocab.keys())\n",
    "left_n, right_n = 2, 1\n",
    "dataset = WindowTextDataset(corpus,types_list,left_n,right_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
