{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos Distribucionales Frecuentistas\n",
    "\n",
    "Recordemos que el área de lexical semantics se encarga de estudiar el significado de las palabras individualmente, y que la primera estrategia es representar a las palabras como vectores para estudiar sus características comunes.\n",
    "Además, hay dos grandes formas de obtener word vectors: denotacional y distribuída. A continuación se verán los métodos frecuentistas para obtener representaciones distribuídas del significado de las palabras. Para lo que sigue se asume que se dispone de un corpus de texto ya tokenizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de co-ocurrencias\n",
    "\n",
    "La matriz de co-ocurrencias es una forma de representar la estadística del contenido de un corpus de texto. A continuación se explican diferentes ejemplos de cómo definir matrices de co-ocurrencia según un corpus y cómo se utiliza este método para obtener un espacio de características semánticas. Veamos algunos ejemplos.\n",
    "\n",
    "Supongamos que tenemos acceso a un corpus de texto que consiste de un conjunto de documentos ya tokenizados y, por lo tanto, a la lista de types que forman el vocabulario utilizado en dichos documentos. Con esta información puede definirse una matriz de co-ocurrencias de types por documentos como sigue: cada fila de la matriz corresponde con un type y cada columna con un documento, de manera que cada fila se compone de la cantidad de veces que apareció el type correspondiente a esa fila en cada documento. Más formalmente, el índice $w_{ij}$ de la matriz es la cantidad de veces que apareció el type $i$ en el documento $j$ del corpus.\n",
    "\n",
    "COMPLETAR CON EJEMPLO\n",
    "\n",
    "Con el mismo razonamiento, se pueden construir matrices de co-ocurrencias para los diferentes corpus de texto, definiendo diferentes funcionalidades y aplicaciones. Por ejemplo, existen corpus en que se disponen los contextos discursivos de las frases que lo componen. Es decir, para cada frase se tiene si fue una pregunta, una afirmación, una definición, etc. de manera que es posible construir una matriz con el mismo procedimiento que antes. Esta vez, las columnas representan los contextos discursivos y cada fila contiene la cuenta de los contextos discursivos de cada type. \n",
    "\n",
    "COMPLETAR CON EJEMPLO\n",
    "\n",
    "Como estos, existen más ejemplos. Sin embargo, a continuación explicaremos cómo se utiliza este método para obtener un espacio vectorial de características semánticas.\n",
    "\n",
    "Con el objetivo de obtener una representación vectorial de las características semánticas de las palabras, puede definirse, a partir de la hipótesis distribucional, una matriz de co-ocurrencias cuyas filas contenga esta información. La hipótesis distribucional afirma que el significado de las palabras queda definido por el contexto en el que aparecen, por lo que la matriz de co-ocurrencias contendrá tantas columnas como types, y cada fila mostrará la cantidad de veces que apareció el type correspondiente a esa fila en el contexto de cada type. Es decir, dado un corpus con un vocabulario de tamaño $V$, la matriz de co-ocurrencias tiene dimensión $VxV$ y su componente $w_{ij}$ representa la cantidad de veces que apareció el type $i$ en el contexto del type $j$. \n",
    "\n",
    "También podría haberse definido de manera inversa: el índice $w_{ij}$ de la matriz contiene la la cantidad de veces que apareció el type $j$ en el contexto del type $i$, que para el caso en que el peso del contexto (ver la sección siguiente) es el mismo, el resultado es trasponer la matriz anterior.\n",
    "\n",
    "COMPLETAR CON EJEMPLO\n",
    "\n",
    "En general, este proceso puede verse como un esquema de selección de características (**feature selection***), puesto que se está asignando un vector a cada palabra en donde cada índice representa una cantidad (veces que apareció en un contexto, documento o lo que sea), es decir una magnitud de una característica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Code Snippets\n",
    "\n",
    "A continuación utilizamos el [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), que contiene comentarios de películas con su respectiva clasificación, para obtener una representación semántica de las palabras que aparecen en dicho dataset. En este caso no interesa la calificación del comentario, puesto que lo único que vamos a hacer va a ser aprender la representación de las palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus: 3\n",
      "Number of total tokens in corpus: 1143\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el corpus\n",
    "\n",
    "import os\n",
    "\n",
    "ROOT_PATH = '../../Utils/Datasets/aclImdb/train/unsup/'\n",
    "filenames = os.listdir(ROOT_PATH)[:3]\n",
    "corpus = []\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(ROOT_PATH,filename), 'r') as f:\n",
    "        corpus.append(f.read().split(' '))\n",
    "\n",
    "print('Number of documents in corpus: {}'.format(len(corpus)))\n",
    "print('Number of total tokens in corpus: {}'.format(sum([len(doc) for doc in corpus])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d5da6eb61b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vocabulary size: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Definimos el vocabulario para el corpus\n",
    "\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_vocab_from_corpus(corpus):\n",
    "    vocab = {token: 0 for token in list(set(itertools.chain.from_iterable(corpus)))}\n",
    "    for token in itertools.chain.from_iterable(corpus):\n",
    "        vocab[token] += 1\n",
    "    return vocab\n",
    "    \n",
    "vocab = get_vocab_from_corpus(corpus)\n",
    "print('Vocabulary size: {}'.format(len(vocab)))\n",
    "plt.semilogx(np.arange(len(vocab)), sorted(vocab.values(),reverse=True), marker='o', fillstyle='none', linestyle='none')\n",
    "plt.xlabel('words')\n",
    "plt.ylabel('frequencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos dos maneras de contar ocurrencias en un contexto:\n",
    "\n",
    "* Por cada palabra del documento, sus $N$ palabras anteriores y sus $N$ siguientes cuentan como una ocurrencia en la matriz de co-ocurrencias. Es decir, para una palabra $w_c$ de un documento y su respectivo contexto formado por por $w_{c-N}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+N}$, los índices $m_{w_c;w_{c-N}}$,$m_{w_c;w_{c-N-1}}$, etc. de la matriz se incrementan una vez su cuenta.\n",
    "* De manera equivalente, definimos la segunda forma de contar ocurrencias, con la excepción de que las palabras más alejadas pesan menos en la cuenta de la ocurrencia (por ejemplo, caen con 1/n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_token(occurrences_dict, token):\n",
    "    keys_to_remove = []\n",
    "    for key in occurrences_dict.keys():\n",
    "        if key[0] == token or key[1] == token:\n",
    "            keys_to_remove.append(key)\n",
    "    for key in keys_to_remove:\n",
    "        occurrences_dict.pop(key)\n",
    "    return occurrences_dict\n",
    "\n",
    "    \n",
    "def filter_by_freq(occurrences_dict, vocab, min_freq=1, max_freq=np.inf):\n",
    "    tokens_to_remove = []\n",
    "    for token in vocab:\n",
    "        if vocab[token] < min_freq or vocab[token] > max_freq:\n",
    "            tokens_to_remove.append(token)\n",
    "    for token in tokens_to_remove:\n",
    "        occurrences_dict = filter_by_token(occurrences_dict,token)\n",
    "        vocab.pop(token)\n",
    "    return occurrences_dict, vocab\n",
    "\n",
    "    \n",
    "def get_context(corpus, window=None, left_n=2, right_n=2):\n",
    "    occurrences_dict = {}\n",
    "    unk_token='?UNK?'\n",
    "    if window is None:\n",
    "        window1 = [1. for i in range(left_n)]\n",
    "        window2 = [1. for i in range(right_n)]\n",
    "    else:\n",
    "        if len(window) != left_n + right_n:\n",
    "            raise RuntimeError('El tamaño de la ventana tiene que coincidir con el tamaño del contexto')\n",
    "        window1 = window[:left_n]\n",
    "        window2 = window[left_n:]\n",
    "    for doc in corpus:\n",
    "        for i in range(left_n):\n",
    "            doc.insert(0,unk_token)\n",
    "        for i in range(right_n):\n",
    "            doc.append(unk_token)\n",
    "        for i, token in enumerate(doc):\n",
    "            context1 = doc[i-left_n:i] \n",
    "            context2 = doc[i+1:i+right_n+1]\n",
    "            for j, c in zip(window1,context1):\n",
    "                try:\n",
    "                    occurrences_dict[(token, c)] += j\n",
    "                except KeyError:\n",
    "                    occurrences_dict[(token, c)] = j\n",
    "            for j, c in zip(window2,context2):\n",
    "                try:\n",
    "                    occurrences_dict[(token, c)] += j\n",
    "                except KeyError:\n",
    "                    occurrences_dict[(token, c)] = j\n",
    "        for i in range(left_n):\n",
    "            doc.pop(0)\n",
    "        for i in range(right_n):\n",
    "            doc.pop(-1)\n",
    "    occurrences_dict = filter_by_token(occurrences_dict, unk_token)\n",
    "    return occurrences_dict\n",
    "\n",
    "#corpus = [['w1', 'w2', 'w3', 'w4', 'w5'], ['w2', 'w2', 'w5', 'w4']]\n",
    "\n",
    "window = [1/2, 1., 1.]\n",
    "occurrences_dict = get_context(corpus, window=window, left_n=2, right_n=1)\n",
    "occurrences_dict, vocab = filter_by_freq(occurrences_dict, vocab, min_freq=1, max_freq=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a evaluar este método de selección de features, con algunas de las formas que se describen en el notebook introductorio de vsm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos una matriz ya diseñada para hacer las pruebas\n",
    "import pandas as pd\n",
    "import vsm\n",
    "\n",
    "DATA_HOME = os.path.join('/home/lestien/Documents/Cursos/cs224u - Natural Language Understanding/data', 'vsmdata') \n",
    "imdb20 = pd.read_csv(\n",
    "    os.path.join(DATA_HOME, 'imdb_window20-flat.csv.gz'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance function: euclidean. Neighbors: ['good', 'really', 'great', 'well', 'story']\n",
      "Distance function: cosine. Neighbors: ['good', '.', 'pretty', 'acting', 'measure']\n",
      "Distance function: jaccard. Neighbors: ['good', 'like', 'really', 'great', 'see']\n"
     ]
    }
   ],
   "source": [
    "for distfunc in [vsm.euclidean, vsm.cosine, vsm.jaccard]:\n",
    "    print('Distance function: {}. Neighbors: {}'.format(distfunc.__name__,list(vsm.neighbors('good', imdb20, distfunc).head().index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reponderamiento \n",
    "\n",
    "A pesar de que se pueden definir diferentes métricas que sean invariantes ante la diferencia de frecuencias, es posible hacer una modificación de TODOS los vectores del espacio semántico. Es decir, modificar la matriz de co-ocurrencias teniendo en cuenta todos sus valores. Si bien las métricas anteriores pueden verse como un forma de modificar la matriz para que las diferencias de frecuencias no alteren el contenido semántico de los vectores, en este caso se está usando TODOS los vectores al mismo tiempo. Este procese se conoce como **reponderamiento de la matriz de coocurrencias**.\n",
    "\n",
    "### Observed/Expected\n",
    "\n",
    "Reweighting by observed-over-expected values captures one of the central patterns in all of VSMs: we can adjust the actual cell value in a co-occurrence matrix using information from the corresponding row and column. \n",
    "\n",
    "In the case of observed-over-expected, the rows and columns define our expectation about what the cell value would be if the two co-occurring words were independent. In dividing the observed count by this value, we amplify cells whose values are larger than we would expect.\n",
    "\n",
    "So that this doesn't look more complex than it is, for an $m \\times n$ matrix $X$, define\n",
    "\n",
    "$$\\textbf{rowsum}(X, i) = \\sum_{j=1}^{n}X_{ij}$$\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "Then the observed-over-expected value is\n",
    "\n",
    "$$\\textbf{oe}(X, i, j) = \\frac{X_{ij}}{\\textbf{expected}(X, i, j)}$$\n",
    "\n",
    "In many contexts, it is more intuitive to first normalize the count matrix into a joint probability table and then think of $\\textbf{rowsum}$ and $\\textbf{colsum}$ as probabilities. Then it is clear that we are comparing the observed joint probability with what we would expect it to be under a null hypothesis of independence. These normalizations do not affect the final results, though.\n",
    "\n",
    "Let's do a quick worked-out example. Suppose we have the count matrix $X$ = \n",
    "\n",
    "|    .     | a  | b  | rowsum |\n",
    "|----------|----|----|-------|\n",
    "| __x__    | 34 | 11 |  45   |\n",
    "| __y__    | 47 | 7  |  54   |\n",
    "|__colsum__| 81 | 18 |  99   |\n",
    "\n",
    "Then we calculate like this:\n",
    "\n",
    "$$\\textbf{oe}(X, 1, 0) = \\frac{47}{\\frac{54 \\cdot 81}{99}} = 1.06$$\n",
    "\n",
    "And the full table looks like this:\n",
    "\n",
    "|    .   | a    | b    | \n",
    "|--------|------|------|\n",
    "| __x__  | 0.92 | 1.34 | \n",
    "| __y__  | 1.06 | 0.71 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance function: euclidean. Neighbors: ['good', 'movie', 'br', 'great', 'film']\n",
      "Distance function: cosine. Neighbors: ['good', '.', 'movie', 'br', 'film']\n",
      "Distance function: jaccard. Neighbors: ['good', 'great', 'really', 'well', 'better']\n"
     ]
    }
   ],
   "source": [
    "imdb20_oe = vsm.observed_over_expected(imdb20)\n",
    "for distfunc in [vsm.euclidean, vsm.cosine, vsm.jaccard]:\n",
    "    print('Distance function: {}. Neighbors: {}'.format(distfunc.__name__,list(vsm.neighbors('good', imdb20_oe, distfunc).head().index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI y PPMI\n",
    "\n",
    "Pointwise Mutual Information (PMI) is observed-over-expected in log-space:\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "This basic definition runs into a problem for $0$ count cells. The usual response is to set $\\log(0) = 0$, but this is arguably confusing – cell counts that are smaller than expected get negative values, cell counts that are larger than expected get positive values, and 0-count values are placed in the middle of this ranking without real justification.\n",
    "\n",
    "For this reason, it is more typical to use __Positive PMI__, which maps all negative PMI values to $0$:\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance function: euclidean. Neighbors: ['good', 'movie', 'br', 'great', 'film']\n",
      "Distance function: cosine. Neighbors: ['good', '.', 'movie', 'br', 'film']\n",
      "Distance function: jaccard. Neighbors: ['good', 'great', 'really', 'well', 'better']\n"
     ]
    }
   ],
   "source": [
    "imdb20_ppmi = vsm.observed_over_expected(imdb20)\n",
    "for distfunc in [vsm.euclidean, vsm.cosine, vsm.jaccard]:\n",
    "    print('Distance function: {}. Neighbors: {}'.format(distfunc.__name__,list(vsm.neighbors('good', imdb20_ppmi, distfunc).head().index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Perhaps the best known reweighting schemes is __Term Frequency–Inverse Document Frequency (TF-IDF)__, which is, I believe, still the backbone of today's Web search technologies. As the name suggests, it is built from TF and IDF measures:\n",
    "\n",
    "For an $m \\times n$ matrix $X$:\n",
    "\n",
    "$$\\textbf{TF}(X, i, j) = \\frac{X_{ij}}{\\textbf{colsum}(X, i, j)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, i, j) = \\log\\left(\\frac{n}{|\\{k : X_{ik} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, i, j) = \\textbf{TF}(X, i, j) \\cdot \\textbf{IDF}(X, i, j)$$\n",
    "\n",
    "\n",
    "TF-IDF generally performs best with sparse matrices. It severely punishes words that appear in many documents; if a word appears in every document, then its IDF value is 0. As a result, it can even be problematic with verb dense word $\\times$ word matrices like ours, where most words appear with most other words.\n",
    "\n",
    "There is an implementation of TF-IDF for dense matrices in `vsm.tfidf`.\n",
    "\n",
    "__Important__: `sklearn`'s version, [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer), assumes that term frequency (TF) is defined row-wise and document frequency is defined column-wise. That is, it assumes `sklearn`'s document $\\times$ word basic design, which makes sense for classification tasks, where the design is example $\\times$ features. This is the transpose of the way we've been thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de la dimensionalidad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
