{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión losgística para análisis de sentimientos\n",
    "\n",
    "En este notebook se va a estudiar el modelo de Regresión Logística aplicado a análisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "                \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema general\n",
    "\n",
    "Vamos a formular el problema de análisis de sentimientos como un problema de clasificación binaria. Se dispondrá de un conjunto de datos de entrenamiento que consisten en una serie de comentarios conformados por una secuencia de palabras, y su respectiva connotación (positiva o negativa). El objetivo de esta parte será estudiar el modelo de **regresión logística** utilizado para resolver este problema.\n",
    "\n",
    "Antes de continuar, detallamos la notación que vamos a utilizar:\n",
    "\n",
    "* El texto de entrada va a ser mapeado a un ejemplo de un vector aleatorio $\\mathbf{x}=\\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix}^T$ que representa el conjunto de features de ese texto.\n",
    "\n",
    "* La clase a la que pertenece cada comentario se denota con un ejemplo de una variable aleatoria $y$, con realizaciones en el conjunto $\\{ 0,1 \\}$. De esta manera, hacemos corresponder a $y=1$ cuando el texto tiene una connotación positiva y a $y=0$ cuando ésta es negativa.\n",
    "\n",
    "* El conjunto de $N$ muestras de entrenamiento se representa por $\\left\\{ \\left(\\mathbf{x}^{(i)}, y^{(i)}\\right) \\right\\}_{i=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modelo\n",
    "\n",
    "El modelo de regresión logística es un modelo discriminativo que forma parte de la familia de modelos discriminativos paramétricos conocida como **Modelos Lineales Generalizados** (*Generalized Linear Models*, GLM). Un modelo de este tipo puede construirse de la siguiente manera:\n",
    "\n",
    "1. Se define que la variable aleatoria $y|\\mathbf{x}$ que relaciona la entrada con la salida depende de un conjunto de parámetros $\\theta$ y pertenece a la familia de exponenciales de parámetro $\\eta$ de tal manera que $\\eta = \\theta^T \\mathbf{x}$. Esto es:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y|\\mathbf{x} &\\sim \\mathrm{ExpFamily}\\left(\\eta\\right) \\\\[.5em]\n",
    "P(y|\\mathbf{x}) &= b(y)\\exp\\left( \\eta^T y - a\\left(\\eta\\right)\\right) \\\\[.5em]\n",
    "P(y|\\mathbf{x};\\theta) &= b(y)\\exp\\left( \\mathbf{x}^T \\theta y - a(\\theta^T \\mathbf{x})\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. La salida del modelo se obtiene por medio de\n",
    "\n",
    "$$\n",
    "h_\\theta(\\mathbf{x}) = E\\left[ y|\\mathbf{x}\\right]\n",
    "$$\n",
    "\n",
    "El modelo de regresión logistica consiste en definir la probabilidad a posteriori \n",
    "\n",
    "$$\n",
    "P(y|\\mathbf{x};\\theta) = \\sigma\\left( \\theta^T \\mathbf{x} \\right)^y \\left( 1 - \\sigma\\left( \\theta^T \\mathbf{x} \\right) \\right)^{(1-y)}\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+ e^{-z}}\n",
    "$$ \n",
    "\n",
    "y puede mostrarse que forma parte de los modelos GLM para el caso en que $T(y)=y$, $b(y)=1$ y $a(\\theta^T\\mathbf{x})=\\log\\left(1+e^{\\theta^T\\mathbf{x}}\\right)$. \n",
    "\n",
    "También puede verse que \n",
    "\n",
    "$$\n",
    "h_\\theta(\\mathbf{x}) = E\\left[ y|\\mathbf{x}\\right] = P(y=1|\\mathbf{x};\\theta)\\cdot 1 + P(y=0|\\mathbf{x};\\theta)\\cdot 0 = \\sigma\\left( \\theta^T \\mathbf{x} \\right)\n",
    "$$\n",
    "\n",
    "por lo que la salida del modelo da la probabilidad de pertenecer a la clase $y=1$, y por lo tanto, aporta todo lo necesario para realizar una nueva predicción.\n",
    "\n",
    "En lo que sigue, adoptaremos el criterio de **máxima verosimilitud** para estimar los parámetros del modelo. De esta forma:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\theta} &= \\mathrm{argmax}_\\theta \\prod_{i=1}^N P\\left(y^{(i)}|\\mathbf{x}^{(i)};\\theta\\right) \\\\\n",
    "&= \\mathrm{argmax}_\\theta \\prod_{i=1}^N \\sigma\\left( \\theta^T \\mathbf{x}^{(i)} \\right)^{y^{(i)}} \\left( 1 - \\sigma\\left( \\theta^T \\mathbf{x}^{(i)} \\right) \\right)^{(1-y^{(i)})}\\\\\n",
    "&= \\mathrm{argmax}_\\theta \\log\\left(\\prod_{i=1}^N \\sigma\\left( \\theta^T \\mathbf{x}^{(i)} \\right)^{y^{(i)}} \\left( 1 - \\sigma\\left( \\theta^T \\mathbf{x}^{(i)} \\right) \\right)^{(1-y^{(i)})}\\right)\\\\\n",
    "&= \\mathrm{argmax}_\\theta \\sum_{i=1}^N y^{(i)} \\log\\left(\\sigma\\left( \\theta^T \\mathbf{x}^{(i)} \\right)\\right) + \\left(1 - y^{(i)}\\right) \\log\\left(1 - \\sigma\\left( \\theta^T \\mathbf{x}^{(i)} \\right)\\right)\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de features\n",
    "\n",
    "Si bien extraer características es el principal problema en procesamiento de lenguaje, en este caso sólo se experimentará con una serie de variantes en la tarea de representar un texto como una serie de características o *features*. Sin embargo, la parte en la que se comprende lo que se está diciendo en el texto está incluída, sin duda, en la extracción de features del texto. \n",
    "\n",
    "### Bolsa de Palabras\n",
    "\n",
    "Una forma muy común de representar un texto es con una **Bolsa de Palabras** (*Bag of Words*, BOW). Este método consiste en definir un vocabulario de palabras $V=\\{ w_1, w_2,\\ldots, w_n\\}$ y contar la cantidad de veces que apareció cada una de estas palabras en el texto. De esta manera, la $i$-ésima coordenada del vector $\\mathbf{x}$ corresponde a la cantidad de veces que apareció la palabra $w_i$ en el texto.\n",
    "\n",
    "Por ejemplo, supongamos que se tiene el siguiente texto:\n",
    "\n",
    "```\n",
    "<START> I am Sam. Sam I am. I do not like green eggs and ham. <END>\n",
    "```\n",
    "\n",
    "y se define un vocabulario `V = ['I', 'am', 'Sam', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', '<START>', '<END>']`. Entonces, el vector de features que representa el texto anterior es\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "count(I) \\\\\n",
    "count(am) \\\\\n",
    "count(Sam) \\\\\n",
    "\\vdots \\\\\n",
    "count(.) \\\\\n",
    "count(<START>) \\\\\n",
    "count(<END>) \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 \\\\ 2 \\\\ 2 \\\\ \\vdots \\\\ 3 \\\\ 1 \\\\ 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notemos varias cosas:\n",
    "\n",
    "* Esta representación es equivalente a representar un histograma de las palabras de que aparecen en cada muestra.\n",
    "\n",
    "* No se tiene en cuenta el orden en el que aparecen las palabras, por lo que se está perdiendo información (¡y muy valiosa!)\n",
    "\n",
    "* Existen elementos del vocabulario, como el punto y los signos de comienzo y fin del texto, que no son palabras pero que, sin embargo, forma parte del texto. Esto se hace porque aportan información valiosa sobre el texto, y descartarlos, muchas veces disminuye el desempeño del algoritmo. Por otro lado, se verá que tener muchos componentes en el vocabulario también puede jugar en contra de la cantidad de aciertos. Es común denominar a los integrantes del vocabulario ***tokens*** cuando se refiere a los ejemplos de estos elementos en el texto, y ***types*** cuando se refieren a los elementos en sí.\n",
    "\n",
    "* Existe la posibilidad de que en el texto aparezcan tokens (como por ejemplo `green`) que no forman parte del vocabulario. En este caso se suelen ignorar estas apariciones, aunque a veces se suele incorporar un token especial de tipo desconocido (representado como `<UNK>`) que contabilice las palabras que están fuera del vocabulario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos y separamos en train / dev / test:\n",
    "df_train, df_dev, df_test, vocab = read_and_split_dataset()\n",
    "idx_to_tk = {idx: tk for idx, tk in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Tokenizamos:\n",
    "samples = {}\n",
    "unk_tk = '<UNK>'\n",
    "unk_idx = vocab_size\n",
    "for data, df in zip(['train', 'dev'],[df_train, df_dev]):\n",
    "    samples[data] = tokenize_dataframe(df, vocab, unk_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el dataset y el batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class IMDbBOWDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, samples, vocab_size):\n",
    "        \n",
    "        num_samples = len(samples)\n",
    "        self.x = torch.zeros(num_samples,vocab_size, dtype=torch.float)\n",
    "        for i, sample in enumerate(samples):\n",
    "            for j in sample[0]:\n",
    "                if j!= vocab_size:\n",
    "                    self.x[i,j] += 1.\n",
    "            \n",
    "        self.y = torch.tensor([sample[1] for sample in samples], dtype=torch.float).view(-1,1)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "padding_idx = vocab_size\n",
    "train_dataset = IMDbBOWDataset(samples['train'],padding_idx)\n",
    "dev_dataset = IMDbBOWDataset(samples['dev'], padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(vocab_size,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "model = LogisticRegression(vocab_size)\n",
    "device = 'cuda:1'\n",
    "LRTrainer = Trainer(train_loader,dev_loader,model,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training...\n",
      "Loss function: BCE\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0001\n",
      "Number of epochs: 100\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.12122833728790283\n",
      "Accuracy on validation dataset: 2541/5000 (50.82%)\n",
      "Accuracy on training dataset: 10008/20000 (50.04%)\n",
      "\n",
      "Epoch: 1, Batch number: 20, Loss: 0.1729176640510559\n",
      "Accuracy on validation dataset: 2540/5000 (50.80%)\n",
      "Accuracy on training dataset: 10025/20000 (50.12%)\n",
      "\n",
      "Epoch: 2, Batch number: 0, Loss: -0.12073114514350891\n",
      "Accuracy on validation dataset: 2544/5000 (50.88%)\n",
      "Accuracy on training dataset: 10040/20000 (50.20%)\n",
      "\n",
      "Epoch: 2, Batch number: 20, Loss: -0.5830476880073547\n",
      "Accuracy on validation dataset: 2545/5000 (50.90%)\n",
      "Accuracy on training dataset: 10048/20000 (50.24%)\n",
      "\n",
      "Epoch: 3, Batch number: 0, Loss: -0.5701159834861755\n",
      "Accuracy on validation dataset: 2545/5000 (50.90%)\n",
      "Accuracy on training dataset: 10055/20000 (50.27%)\n",
      "\n",
      "Epoch: 3, Batch number: 20, Loss: 0.6103765964508057\n",
      "Accuracy on validation dataset: 2548/5000 (50.96%)\n",
      "Accuracy on training dataset: 10069/20000 (50.34%)\n",
      "\n",
      "Epoch: 4, Batch number: 0, Loss: 0.40630289912223816\n",
      "Accuracy on validation dataset: 2554/5000 (51.08%)\n",
      "Accuracy on training dataset: 10071/20000 (50.35%)\n",
      "\n",
      "Epoch: 4, Batch number: 20, Loss: -0.00574088841676712\n",
      "Accuracy on validation dataset: 2555/5000 (51.10%)\n",
      "Accuracy on training dataset: 10081/20000 (50.41%)\n",
      "\n",
      "Epoch: 5, Batch number: 0, Loss: 0.33104562759399414\n",
      "Accuracy on validation dataset: 2558/5000 (51.16%)\n",
      "Accuracy on training dataset: 10087/20000 (50.44%)\n",
      "\n",
      "Epoch: 5, Batch number: 20, Loss: -0.34462839365005493\n",
      "Accuracy on validation dataset: 2558/5000 (51.16%)\n",
      "Accuracy on training dataset: 10103/20000 (50.52%)\n",
      "\n",
      "Epoch: 6, Batch number: 0, Loss: -0.246038556098938\n",
      "Accuracy on validation dataset: 2562/5000 (51.24%)\n",
      "Accuracy on training dataset: 10112/20000 (50.56%)\n",
      "\n",
      "Epoch: 6, Batch number: 20, Loss: 0.25862592458724976\n",
      "Accuracy on validation dataset: 2564/5000 (51.28%)\n",
      "Accuracy on training dataset: 10129/20000 (50.65%)\n",
      "\n",
      "Epoch: 7, Batch number: 0, Loss: 0.12167805433273315\n",
      "Accuracy on validation dataset: 2565/5000 (51.30%)\n",
      "Accuracy on training dataset: 10140/20000 (50.70%)\n",
      "\n",
      "Epoch: 7, Batch number: 20, Loss: -0.2494291067123413\n",
      "Accuracy on validation dataset: 2569/5000 (51.38%)\n",
      "Accuracy on training dataset: 10143/20000 (50.72%)\n",
      "\n",
      "Epoch: 8, Batch number: 0, Loss: -0.30615848302841187\n",
      "Accuracy on validation dataset: 2569/5000 (51.38%)\n",
      "Accuracy on training dataset: 10147/20000 (50.73%)\n",
      "\n",
      "Epoch: 8, Batch number: 20, Loss: 0.2642941474914551\n",
      "Accuracy on validation dataset: 2572/5000 (51.44%)\n",
      "Accuracy on training dataset: 10156/20000 (50.78%)\n",
      "\n",
      "Epoch: 9, Batch number: 0, Loss: 0.04528224468231201\n",
      "Accuracy on validation dataset: 2574/5000 (51.48%)\n",
      "Accuracy on training dataset: 10165/20000 (50.83%)\n",
      "\n",
      "Epoch: 9, Batch number: 20, Loss: -1.0950729846954346\n",
      "Accuracy on validation dataset: 2573/5000 (51.46%)\n",
      "Accuracy on training dataset: 10171/20000 (50.85%)\n",
      "\n",
      "Exiting training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entrenamiento:\n",
    "loss_fn = 'BCE'\n",
    "optim_algorithm = 'Adam'\n",
    "epochs = 100\n",
    "sample_loss_every = 20\n",
    "check_on_train = True\n",
    "learning_rate = 1e-4\n",
    "\n",
    "LRTrainer.train(loss_fn,optim_algorithm,epochs,sample_loss_every,check_on_train,lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9aZgkV3Utuk5MOVZV19Sz1N1qDaAJyWomAWYwg7i+Bh4YG/tywX5w8bXBvt/lgQ3PfuCJz/h6tsFmsI1tDJYBGyRjQGCQDEgg1EJjS0hqtdTquWuuysohpvN+nLNPnIiMzIqsyqyupmJ9X39dlZUxZGTE2Wettfc+jHOOHDly5MixeWGc6xPIkSNHjhznFnkgyJEjR45NjjwQ5MiRI8cmRx4IcuTIkWOTIw8EOXLkyLHJYZ3rE1gNJiYm+N69e8/1aeTIkSPHeYW77757mnM+mXz9vAwEe/fuxcGDB8/1aeTIkSPHeQXG2NG013NpKEeOHDk2OfJAkCNHjhybHHkgyJEjR45NjjwQ5MiRI8cmRx4IcuTIkWOTIw8EOXLkyLHJkQeCHDly5NjkyANBjhw5BoIzi0187aEz5/o0cmRAHghy5MgxENz4vWP4hU8ehB+E5/pUcqyAPBDkyJFjIGj6AUIO1L3gXJ9KjhWQB4IcOXIMBMQEllv+OT6THCshDwQ5cuQYCLxALIO73MoZwUZHHghy5MgxEAShCAR1N2cEGx19CQSMsRsYY48wxg4zxt6T8vc/YYzdK/89yhib1/4WaH+7uR/nkyNHjnMPPyRpKGcEGx1rbkPNGDMBfBjAywAcB3AXY+xmzvlD9B7O+f/W3v/LAK7VdtHgnF+z1vPIkSPHxkIkDeWMYKOjH4zgWQAOc86PcM5dADcCeHWX9/8MgH/qw3Fz5MixgaHM4lwa2vDoRyDYBeCY9vtx+VobGGN7AOwD8A3t5SJj7CBj7LuMsdd0Oghj7G3yfQenpqb6cNo5cuQYJDzlEeTS0EZHPwIBS3mNd3jvGwB8jnOu3xkXcs4PAPhZAH/KGNuftiHn/GOc8wOc8wOTk20rreXIkWODIcilofMG/QgExwFcoP2+G8DJDu99AxKyEOf8pPz/CIDbEPcP+oqDT87iW4/lbCJHjvXAoM3i2w9P43N3Hx/Ivjcb+hEI7gJwCWNsH2PMgRjs27J/GGOXARgF8B3ttVHGWEH+PAHgeQAeSm7bL/zlbY/j97/yg0HtPkcOAMDssot//X4+QJFZPKj00U/deRQf+sZjA9n3ZsOaAwHn3AfwDgC3AHgYwGc454cYY7/NGHuV9tafAXAj51yXjZ4O4CBj7D4AtwL4oJ5t1G9sKdmYr3uD2n2OHACAm+89gXd+5j7MLbvn+lTOKRQj6FMgeHJ6GdO1lvrd9TlcP+9j1A+sOX0UADjnXwLwpcRr70v8/psp290B4Kp+nEMWjJRtLOSBIMeAsSzN0dYmH6R8YgR9kobe8vd34Vn7xvF7rxVDhh+GcPOGdn3BpqosHi07WGr58PKbJ8cA0ZJN1jb7febLrKFan8zik/PN2L68INz0wbZf2FSBYEvZBgAsNHJWkGNwaMrBabPPVqmOoB/po3XXR8MLEITRNfWCXBrqFzZVIBgpiUCQ+wQ5BomGmzMCQKss7oNHMFMTfgvJTWL/QhqK2445VoNNFQhGyw4AYL6+uU28HINFk6Qhf3MPUGQW98MjmJXGOzWyA0RQ4DySoHKsHpsqEJA0lDOCHINELg0J9NMjUIGAxxkBgFwe6gM2VyAoSUaQewQ5BohmbhYD0LKG+iENpTACur6b/Tr3A5srEFSIEeTSUI7BIQ8EAlHTuX5IQy25T00akkEhZwRrx6YKBEMFC6bBcmkox0CRBwIBTxuo13otyCyOMQIZADZ6CmnDDXB2sXmuT6MrNlUgYIxhpGRjvpEzghyDQ9Mj7Xpzm5i+Nviv1TAmacjX00cp0GzwgPvhWw/jdR+541yfRldsqkAACMN4LmcEA8G9x+bxzA/8x6aX3nJGIOCHHGXHBLD2FNK0rKH1Mos55/jpj34Htxw6vartZ+suTsw1EG7g7KbNFwhK53ebiS/efxIf/PLGbJz3+NkappZaOLPYWvnNP8Ro5IEAgNDzqXZnrYZxxAji6aPA4AOBH3Lc+cQsDp1cXNX2YcgRcqC2gRfo2XyBoOyc19LQLYfObNjOlkTRdfq+GUHS0KYPBGGoAsFaW1GTWawzArrfBi0NUcAJVnlf0zlv5AnoJgwENuaWN+4XshIWG15fzbFjs3VVCbtW0MxMz+zYjKBeQ+4mvg6cc3gBx7AKBGtkBLU0RrA+0pAnA8Bq4w3VPmzk1jabLxCUnA39hayExaaHlt+/hT5+4kPfxt/d8WRf9qUCwWZnBD5VFm/e60CzYMUI1jDZaHqB6ldEOnsg5RZgHaShNTICOufFDTzubL5AULZRO487kBIj6Ed/Fc455uueot1rhZKGNvFM2A9C1WPnfL3H+gGauQ8X1+4RzGjrOtB+9Ws76PRRf82MQPy/kSegmzIQAOdvm4nFpg/Oo4Zea4HXZ7MtYgSbNxA0tWuZBwL0xSOYlbLQSMlWTEO/tuvlEYSrnHwRI9jIHQ02YSA4vxvPEb3shzxEM51+PUhuXvKvUkeBze0RkH5fLYq1r9Zyv1Jyx3jVUfeszjp7ncgcmarhziMzmd9Px1qt5KnM4h/2QMAYu4Ex9ghj7DBj7D0pf/85xtgUY+xe+e+t2t/ezBh7TP57cz/Opxt2jhQBAEdn6oM+VN/R9AJFg/tBh4kR9Ita52ZxPBBs5oBI91ZF1hGshXXSthXHSmcEPe77N77wIN79ufszvz83izOAMWYC+DCAVwK4HMDPMMYuT3nrP3POr5H//lpuOwbg/QCeDeBZAN7PGBtd6zl1w9N2DIMx4KFTq8sJPpdYakY6az/kHJq19T0QrIM09MjpJdzz1NzAj9MrYoEgN4tVQVk/AkHJMSOPIKWwLAtml13c+cRsT1lMQdgfs/iHOhBADOCHOedHOOcugBsBvDrjtq8A8DXO+SznfA7A1wDc0Idz6ohqwcLe8QoOnVxAGPK+dEZcLyw2oxupH4N3v5t2rWfW0B9+9RG876ZDAz9Or6AaAmCzMwLx2Qu2CcbWJj/StiXbREBGvL86RvAfD59BEPJYwF4J9FlyRtAduwAc034/Ll9L4nWMsfsZY59jjF3Q47ZgjL2NMXaQMXZwampqTSd8+Y5hPHRqER/55uN4we/fGuuJspGhp5/1wyPod4n+emYN1Zp+Tw/zeiH3CARokmGbDI5pZJ64tPyg7Xmk+7OsMQJ9stFLkLnlQdEmotnDPa+nj84tu/jxP/8Wjs4sZ96eyMsPe/ooS3kt+QT8G4C9nPOrAfwHgL/vYVvxIucf45wf4JwfmJycXPXJAsDlO4dxbLaBv7v9ScwsuzizdH60RNCloZbXD2moz4wgWD9pqOkHG3LGnTMCARrMLcOAYxmZ77E3fOy7+KOvPRp7jfyGkm0qmUZv6Jc1yHhBiG8dnoZlMAQhz/z9qPRRDhydrePQyUU8fGop07bA5pGGjgO4QPt9N4CT+hs45zOccxptPw7guqzbDgJX7BwGAJyVAeDEXGPQh+wL+i8N9TlrSJnFgx8AG27QlxTaleAFIV76x/+Jr2ZsONbIzWIA0eBtmwwFy8x8v56cb+Cp2XgihyvZb8kxlcwSYwQZ973Y8OD6IbbLhJGsjFKlj4Zc+QS9yJ+bJWvoLgCXMMb2McYcAG8AcLP+BsbYDu3XVwF4WP58C4CXM8ZGpUn8cvnaQHG5DARM8pHjc+dHBtFiQ2MEfZGGBuMReOvACFp+uC7th5dbPg6freHBEwuZ3k+DC2O9BYJHzyz1JDesBpzzdVvonQY/yzBQ6IER+AFXLToISUYg2lf0HghoycyJagFAnL11PSdNjlKppD1MQjaFR8A59wG8A2IAfxjAZzjnhxhjv80Ye5V8268wxg4xxu4D8CsAfk5uOwvgdyCCyV0Aflu+NlBsHSpi73gZr712NwDg+PnICDaiNCT3E6wbI+jvcWotH1MJmZAGi6zFQBQIqgWrp/UIfu1f7scH/v3hld+4Bvzt7U/i5X/yzYEeg0Apl6bJhDSU8bvygjDGqgDNLJYZSELW0ZvPZZsUkbQaBYJs2+lmcVr66krQW0xs1FbUVj92wjn/EoAvJV57n/bzewG8t8O2fwvgb/txHr3g5l9+PoqWiW8+NnX+SEON/kpD3oAKytbNI+hzeuZv3nwIXz10Gp/7xetx6bYhABHzylqJTibkcNHuabCotwIsWoOdMT4+VWuTXQYFmmTYhgHHNJS8s+J2IW+bqdMEo2ib6j1rYQSTQ6KoNCur1s3iiB30zgioFTW13dhI2HSVxYThog3HMrBrSwnH5zeuNPSuz96Hv/j6YwDijCDrLKgb+s0IosXEBx8IBuERPDG9jMWmjzf/7fdUy2AKuFkZAckaQ0Wrp0AgZsKDZVINN1i39h/KLCZG0IM0lOyG6wYhbJPBNoWWG4R8VZXFtTZG0Js0FPC1MQJg47ai3rSBgLB7tLQqaYhzji8/cGrgrSq+98Qs7jsu9OnFho+CJb6y/khDgykoW23hTVaEIVceQT8179MLTWwfLuLUQhP3n5gHEMkHCxm/ZxrEeg0EbhCi2ad24J1Qd33RtXMdgoGnp49a2dNHvTBU3VvVa34I2zRgGuLe90MeY7FZGW27R5CVpUT3dRQIemcEwMb1CfJAMFrGyfnel5G75dBp/OKnvo8v3HMCnHPc9eTsioPS3Ufn8D8/eXdscY2VUNe08MWmh8khcRP3RxoiRtDf9QgGzQj0z96vGW4QcpxebKpEAmqS1isjaPoBTIOh5Fg91RH4AW/TxrPgwRML+MHpbFXy1MrZW4eCP5oM9GIWCyMYbQHRDUI4lgHLaGcElsEyM4Kl1ZrFShrikTTUQ5APQmBY9lzKA8EGxa7REryAq1TSLLOEhhvgd74ojL1ay8ddT87h9R/5Du49Nt91u9sPT+Mrh05jppa9bqHhRi2zFxueuon70nSuzys80aA56MriQfTzma61EIQc+ycrAKK2ycS8sj7ATS9E0RK6eC8eRppJmgW/9W+H8HtfyrZ0KbGV9Sj4o8mAaWQ3i+m7TBZ7eQExAhEI/DBU760UrMyTIpKGxqvCI+jVLA41s7iXCUgYcoxWxDE3alHZpg8Eu0dLAEQK6U33nsCV778Fh892Lxb5zMFjODEv5KSGF6iFtU8tNLtuR4PJXEadkHOOuqczAj9iBH2QhvqePrpOlcWxXP2UzJwjU9nTPQn03V28tQogWkhFSUMZMz6aXoCibcKxWGqQ8oIQh8/W2l53g3BVldK1VpC5TUp9HQOBMotNMotXvsdocE16BC0/hGPGGYEKBI7Zg1nswTQYRmUH4qQE1QnR4B+qSU4vE5CAc5Qdq6djrjc2fSC4QAaCf/n+cfzGFx6EH3Lcfrh7i9r7js9j23ABQwULdTdAwxMP4vQKM32aDWT1FcQCNFGrgqWmh7GyA9NgfS0oC3l/isDo4Ri0IRlv49B+3n/41UfwKzfe09M+T8nAvn9SBIK6lBHoOnMer+zuhIYMBLZppA4W/3bfSbzyz77ZZhr6QW/9bwgtL8g8EFIAXQ9piO6tNLP4nZ+5Fzd+76n2bRQjCGIyqxdwOFbECHSJplLI7sXUmj6qBQtFWwx7WaUhrw9mMTXf68cEbhDY9IFg30QVL7t8G/7pe8fgBxyjZRt3H+3e1fKxMzVcum0IRceMLaM3Xes+wFPWT1ZGoDRdn6QhH8MlCwXL6JM0pOdir/0GXa/K4pWqd5dbAZ6aqffkxRAj2DdRAWPtjACI+uJ3Q8sLUbQNGQjaj392qQUv4LEMMPocXpC97YE6nh9m9iKIOSQZwc33nVyRBfcKT08fTZjFXz10Bnc92f6M0eDOedwHcv0AjhkPBHSdygUr87271KJAIAbl7JXFkVkceQS9mcUUCDZifywgDwQwDYaPv+kAPvs/n4t/fOuzcf3+ia6BIAg5Hju7hMu2DaHsmGi4gaKyK2n/VBm8kGFAAaIHVwwSQkOuFmwZCPo3gwf6Iw+tV2XxSv18vEA8sCfns2eDnV5somAZGKs4KNumalOsX+cstQRNjRGkDVDENPRgxnk0w+11oGh6QWazX00stPPinONdn70Pn77zWKfNVoVAKyjT71cvCDsuFasPrq3Yd8xhW0zzCKKCsmqhB2mo6WOoaKFo9RoIyCzG6rKGdEawQVuTb/pAQHjm3jFct2cUP7JnFCfmGzjdQe8/NltH0wtx6fYhlGxTSEMqEHQf4Hv1CBragxsV1fTWxKsbdAlnrfsLNboeDFiDXsksps/SS/HUqYUmdowUwRhDpWBFZrGvM4IMgcCXHoGZ7hEQ09B1cH1Q6dUw7qXVhjKLte99qeXD9cOemchKiBgBixWUUTBNu9/0c9Cvg6s8AjFcxRiBY/VUUFYtWChIaSjroKzu61hBWa/SkPQIcmno/MB1e8S6ON/XFj35g1t+gPf8i1jR6JEzgkJfum0IJcdEwwtQlzftTMoi8At1D//rxnuwUPeUHJCcWR4+W0sNPNEMjqub3bGMnpp4dYMu4ax2f2HI8dH/fDzmjwxCg6YeM0DCI0gxi2lg7GUVulPzDewYEX5RpWCp9FH9wc2SOdRwAxRtA1aHrCFiGnU3PZg13V6loWwegeuHqamPNHnpd6ZXVFBmoGCb6jshfyyVEYTpAdFNZg0FPFoKs9BjICgKaZUxtPU0Wumz6DUYvdYRUEFcPyTdQSAPBAlcvmMYBcvA96U8tFD38DfffgL//sApcM7x6GkRCC7ZWkXJTkpD7Yzg3uPzuOnek7jrydlUs7jW8vGTH7kDv/fl9j4zNFi4QTTrE4GgPx6B1weP4PGpGn7vyz/Avz9wSr02iKyU13z4dvzFNw4DWNkjoIHh6Gz2Jm7ECADR9z6NEWQpKmt6IUrKLG6/DmneQywQpHyvS00vdUUt6rmTZSDUM4v08yI5s9+1HzSoWyaLZQ0Rq0q73/QA1UwygkQdAfkiJcfMXlAmzWLGhFyVdU0CZRavoY7ANEQX1qYnpLHPHjy2bg0AsyAPBAk4loHdoyWVHvqv9xxH0wux1BQNyR49W8MFYyVUCpbwCLwofS8ta4iCxJmlpipo0RnBp757FPN1T6Wgxrb1Io+AHiTbNFCwjf5UFq+ilW8SVK05p51/LyZtVjw+VcMT02JgX8kjoIHhqYyMIAw5ziw2VXviimOpz9XyQ8jxB/c8NY/nffAbXbuENv0ABSkNuUGI0wtNPHQyKvhK8whi0lBKdfE7Pn0P3v25+9pepyCVLRBE+9W/d0pw6LfBrxamkWYxZabRfZIuDaUzAi8Q0pBpRnUEfhDCMlhPftlSS3gEgOhb1LNZzKM21D1lDXEOgzEUbTGB+8qDp/Huz92/oZpd5oEgBePVAmaWXXDO8ak7n1JGz+GpGh45vYhLt4qGZEXJCOghW2z6bTc4DeZPTC2DJgBzcmbZ9AJ8/FtHAKSnJupZQzS4FfooDcUYwSr3R+c4owWCfuvNQchRdwM1K07KBkl4PUpDi00PfshVsV65YKrP1fQCVBwLFcfEF+8/hRPzDRyZ7hII3ABlyQgA4A9ueQS/+Km71d+X5aSh0UEaSvMIziw2cc9T7cWKFBCT12Cm1sKV778Fdx+NGvnWO3gSNAHpd8ovDZ5UUEbnSZOgdGkoOyOgIrNe/DJiBABQtLIHAprYhGF0nXpJiAhCHmMEaYkI5xp5IEjBeMXB7LKL04tNHD5bw39/7h4AYkb42Nkart69BQAUI9BvqOTMnh7Ax6eiIiJ6GO55ah7TNRdDxWgGmrZtzCMw+ycN9SN9lM5R/9z9HlRo8KRjtTrMpgm6WZyFftN+KeALjyB6WAu2iS1lR12jbmys4QUoOSZsOfidnG+oilYgal2hD/j+CmZx0wtwaqHZlnJK94AXxPsHnV1qodbyce+xqKhODzxxj6DVdg79gL4wjWNG/bFoEpQewKNzSEpn8cpiIYkp2SlDzylfy7oDRNJF5joCWoMgDFUiRC8MKgwFIyhIRqAK+9ahniMr8kCQgrGKg5laC2cWxUNyYM8YqgULN971FDgHnn3RGACorCF9tpWUh+gBPCwDwWjZVvnoS/LBvnCsHBssktu6mjQUeQQbQxoiWWyQjICuTepsOuW8XSnn1Fp+quSWBA2+1O++4pha0AlRsAwMl6LWwd2CcMMLlEcAAFO1Vuza0mfoVBSX1niOBqzHztRSX0/ug2awp7T0Wd0j0AP1zPKAzOIwhGkwMJZgBNIjSKsIj3sE8XvTsQyYLM4IHFm1zPnKkw8KwNXVSEOq6Zy2SE2PZrHOCOh+W48K76zIA0EKxqsFzDc8nF4QD9K24QL2T1ZwbLYBxzRwzQWCERSVRxCgIgeRToGA9MALx8qYq3vgnCsWsH242JUR6D8THe5niwlg9YGAHrBBegQq04Yyefz4bDEJL+C4YKwMADiWQYel76gkC43Kmkcg0kENbNEDQYdrH8pe+pQ+CgBTSy20tHOkz9CLNESf99Ez8aIvPSDpgYD2d3JBCwQdzOnpQZnFAVdSjgoEfpg9a8jVPxuHbRqwEm2oqWqZ9q3jvmPzuEVbXnSpJQLQkJSGCraZ3SzW1iMI1iANkUdAAWgjLWXal0DAGLuBMfYIY+wwY+w9KX9/J2PsIcbY/YyxrzPG9mh/Cxhj98p/Nye3PRcYrzjgHHjktJiBbR0qYr/sQfOMC0ZUZWLZFqlryy1fDTzJzCF6AIm5XjhegeuHKnsAALaPiECQ7GXTcHVJQfwcpY/2r+kcsHq9kmaac1pGjR9wqetn64GzEshkjxhB+kxYveaHSu9famYrAgM0RiA9As65ZAQmtpRttbRpp2tP17DkRIxgQa6TS9KF+gwdpKE0uYICTzIQJGfNBBqsTsxHKclxaUjPGnJj2/QLfsjVNVCt0/0Qc8vxdR50dMqecv0ABStqQ530CMR74vv70K2HYyu+0bOmGIFl9OARROmjyiPo4Xkhs7ggJ3CqwnsDrVa25kDAGDMBfBjAKwFcDuBnGGOXJ952D4ADnPOrAXwOwP/R/tbgnF8j/70KGwBjslPgw6cWwRgwUXVUM7Jn7xtX7ys54vLNLrvYPSoDwXI6IyDskQFjru4qg5jSFmuJgXNZ25YGEJKG+l5QtkaPgIrkLIPBC0N84vYnUpdF/PSdT/XcEC6Zex9nBOIzzC67+PZj0+Bc9KqnxmJpaZedPgMxgkrBQiDXPGj5AQq2gbe+4CL87muuBNA5aNIDrktD+nlyzlc0vJOMgHOuPm9SGmolDFX9WEBSGkpnBHS/9nt26geh0vQLOiNodGYEQUdGIBamsXSPQAYaXXbScXyuEQvYJC9WNUaQvY4gSh8N+SoKyjhgGkKOavmBmsisdM0fPLGAf/jOk5mPsxb0gxE8C8BhzvkRzrkL4EYAr9bfwDm/lXNOKRzfBbC7D8cdGKhN7Q9OL2K84sAyDTxtu8gUeu5+PRCIm2q27mJyqICCZbQzgsTgfuG4CATzdQ+1lg/bZBiXs9ekT6A/vCTBOJQ+uoZA8L9uvAfvv+nBvrSYoABFD3HJEQuMH59r4PhcI9E8LMT7bnoQn05pONYNyiMgucYN1GpV9Bk+fedRvPkT31Oz5LGKkHJqrZUfdhp8ielV5PdadwPRO8gycd2eUbz+ugvE8TsMIMprsCOzmOAGIVp+CBrrOtYRpKzXS5fwkTZpKP37o0Hq7FJLDYY6u9RljaigrM9mccjVd5Q1a0iXp1qJwBZvOhfKxWoiIzp5/56Yq8f2t5TKCHqsI+DROgi9tpgwqXbBCzVpSOwjLcsLEI0wf+eLD61LvUE/AsEuAHqjkuPytU54C4Ava78XGWMHGWPfZYy9pg/ns2aMV8TAfHS2jskhMVt/0aVb8em3PhvX64FADhyci4yTiWoBZxbjFcL68oOMRW2v5+uuSmej3Gairw+eWMBXHjy9gjS0+kDwg1NLePRMDX7AldyxarM4MdBWHAue1klTP8+nZuvwQ55plq5jScvg8QOxghWt+0oDylzdQxBGzdx6YQR0rpQ1RP8vt3zFCACRAcNYZ0ZA+yk6kUdAIAmR0EmqSTJIGqy2DRcwtdSKdS3tFEz0Qf3Mgpjx11OyhoKQK0mv73UEQahaQjimuJ6ur2UNpVxDfZZN1yEIhcxoa22o/ZDDD8X+0xjBUtNrS+WmyQR5BEXbzNwSmq6NWI8gbDvXbiC51zCYYgRR8z+xj9OLTdRaPh44HmfK1IhwPdpS9CMQsJTXUkMYY+yNAA4A+APt5Qs55wcA/CyAP2WM7e+w7dtkwDg4NTW11nPuCmIEnANbZf9/w2C4/uIJMBZ9XBow6OfLtg/hoVPx1aL0wXy4aCvZab7hqZJ3oqskFf3Nt5/Ab3zhgdjDS0FCTx/99J1P4bMHOzcLW2x6uPm+k22vL7u+WPw9DNXst1vjsoW6hx//82+lNuNbTjCesmPCl7Nf8fmj/T4u+/D3Ggj099c9UclNwZMednrQyROghUDSTPgklFmspY8C8jrJrCEAYIyh2CUIE+VPk4ZcP4x9n52qo5PSEM3oqf2FnkKqn0fLD/FTH/0O/vX7x2OBhQzjekrgma+7iqGslhF898gMfv3zD7T5W2TmAhEjaPmBkhDTZtRpabR0bZJtqN2Aw5YyKRAPLFQMqgeHNo/Azu4R6P2Fes0aomUqdUbQSDACOndaHItAmVXJtOFBoB+B4DiAC7TfdwNoG30YYy8F8OsAXsU5V5+Yc35S/n8EwG0Ark07COf8Y5zzA5zzA5OTk3047c4YLTtqpkyBIA3ECAAxw7hq1wgOn63FBy43ULOQ4ZKFLSUxQJFHUC3YbYyg4QaYrrmx3jZJRuAFHH/29Ufx7s/dj0/fGUktLT/A73/lB1hsevj890/gV/7pHjyZKIBquAGaXgg/iLoidvMIHju7hEMnF/EHt7SvhJWcwZI0RA+ZPut6fGo59jmzQpfM6i1x7pTOSQ8T7XNR04JNg2UyrJMeQcQIArRkEzmCqOrOIA21eQRh7CpyxRkAACAASURBVHM3OqR+JgcnMopV4Ovw3pYf4ntPzOKhk4uxNaNPagsoqXORf9dTflebyvjJ7x7Fp+58Cl9+8HTsdd0spkBAxjmtWJaUPNIkMgp2etM56jVkG+lZQydkpljMpJfXnpq/ifTRbDNtxQh4dJ2yempBCiOImv+FsXOndHUCHWM9VjXrRyC4C8AljLF9jDEHwBsAxLJ/GGPXAvgoRBA4q70+yhgryJ8nADwPwEN9OKc1wTSYShfcOtwlECQYwdW7RxByxFhB3Q2ULzBctLGlLPY7t+xiueVjqGCpIhca8GgWqFfGknHsWBEdPrPYQsUx8etfeAAPy2N+/+g8/uq2x3HrD86qmdETiUAgtO8Afhiq2W+3dFRKMfzukVkcfDKuYy4nAkHFseCFHRjBFDGC3jKedBO97vpoeBEj0NdzBqKHxrEMVBwz07GSHgExtLrri4IyTe8vdNGWo3oEoy0QtPwwFpR0ppicCf/51x9ThjoNiHROfgcdnT6/3qIZiNZZqLu+mtzQPuh7Ha84q2oUyDnH954Q98Offf3RGCugOgIgMovPyoGOJldJVkCzbdNg6YzATKksNtsnMic0k5z2SdeKFqXprY6gPakia+Akc9k02hlBMqicXYrLyioQZFgQaa1YcyDgnPsA3gHgFgAPA/gM5/wQY+y3GWOUBfQHAKoAPptIE306gIOMsfsA3Argg5zzcx4IACgDd6v0CNKgM4KyY+Kq3SMAgPs1ra/pBdgxUoJpMIyUbBRtE9WChZllN5KGFCOIp9adXmyqh4lmNLZpxAam//fHn45qwcIfffVRAMApKQUcn2uo2eCTWm+cMOSqGtoLuOrE2G2GMyUNxZJt4hN3PBn7Wz0xuy8XTPgBj0xK7WE7MrU6aSjGCNxAtX0wWDRQ0Gyb5LWCZaBSSK/YTqLpBTBYNGDRrHG5JY5VsDRG0CV1l4IeLVWpQ3gE4u8jJTtVGrIMhqmlFv74a4/ipntPAIjuBQrYnYxlCoBiLYb2QbHeipgp7YOM4m3DxZ4YQd318ZmDx3D4bA1TSy087+JxPHqmhlsfUXM8UfmbqCM4Iwe6KBDE7zm9oyhNTPSKeioo0yuLaWDXJxwntNoR2r7lBWAMylwuyqLMLEZsWlJFVk+FGIHJdI8gHuTo/6k2aSg+yRkkrH7shHP+JQBfSrz2Pu3nl3bY7g4AV/XjHPoN0vK7SUO6R1ByLGwdKmLHSBEPHI/6wtTdANWCiYmqowzOMdnCotbysW+i0uYR6A/4cNHCXD3qPlmwDGVeAsA1F2zB215wEf7oa4/inqfm1Azw+FxD/awzC5JqmtJ4pTVlp2sufvqj38EHX3c19k1UYp9zeqkFxoCrdo3gbMIMT2MEfhgZXPRZOOerl4a09y+3/NTFXyKPIPJS9FYR3dBwRTUw+T+Vgm4Wh2qwASCLgrqbxSXbbMvLd4OIEUxUnbb0SAAYLtk4Iq8RDdJtjCBMZwSUjSNkE/GeLWVbyYJ1N8BwycZi01f7oPYS20eKeOR09hXKbjl0Gr/6ufvVM/LOl12G2w/fEbvP6N4CosE3YgRicuX6ISra40UMYahotTECvcVEwKPKYmLlugdyfD4eCCoFKGZH33HBjhaK0aW/NAQp1zxrQRnFZEMygpBH92jSI0gmmnjnmTT0Q4kJaRh3k4b0G6gsf75q10iMEdTdACXHwq++4ml48/V7AUSBYKmZbhbrD/iIlKjIlBWMIDruvokKfv75++BYBr784Gm1rsGJ+YgR6NIQzUqFNBRVZ957bB53PjGL+4+3NzebrrUwWnZEy99kUz3XVw86IOQyYRZT2mKkRy80PDiW0XMgSHou1OrZMQ1lqEUegaeuU1ZGUJf9gQhlJykNJRnBStJQullMqazj1UJMXvK1AZBm8NPLFAhkkCiSNKQZxNqEgfwkTzM0X/K0rfjOkRkcnVlG3YsyrWgfM8suDFkn00sdARV2zS67mKg6eIZkwvrMle4tIGJadD9Sl9c2RhBGjIACoN5aRTWdC0JlRtN3Rc0d9ePox0gyu16Wq0xr156ZESizGOr49F3R56X7aa7uxdgmHfe8kIZ+WBExgi7SUMIjAIArdo7gyPSyusEaro+yY+J11+1WNQjjFQczNRe1lochaWqWHTPW+phApuhyK4BpMKU1AqIQreyIQLJ3vIwjU8tKGjo2W1czDL1tMs1Em3JVKtsQUtNxuZpXmlcwXWthouq0rT0LCEYwqbEmkTXE1X5ocKSMoct3DKPuBm1ZJt2w1PQxWo4CIrV9sK1ogfhk1pBjGagWzGzpo27cEKbAvNAQKak6Iyh0qUjVW1WkZw1pjCBFGiLfA4hm63SsSBri+Pg3j+An/uLbXRiBeP0tz98Hy2D4xO1PouGK9a5pH4AIBKNl8b32kjWkm9HP2jcGyzRQLVhqKVY6j6Q0RG1Wdm0RGVBJOZLOa7gYSWeuzghMXRoSjKOcwghOzDXUsekaJb2eTgvYP3RyMdaaQhxPl4biGT9JPDG9HJObAs330O8jfR/65EqXh843s/iHEjtGSnBMIzbIJaFLQ0X5M6WeLjREPyFqQqZjvOrg7FITTS8ya4eKVptZDGiMoBXNvOmG1iWcveMVPDmzrOSgJ6aXEXIR0I7PNdRgU/eiArCmF6oOjpSrn5ZbPbXUwkS1oLI9dNRbvmJNjImZlheGaj/0QJ+WQemybaIwL5l2mkSorUi27PrYNlyUxxNZF0XHhC2XgwxDrgxlYlW2aaDiWJnN4ngGmPBNZmU7hBgj6CIN6YzASQaCIFDnMl4ppPYaohk7EElDdCzykfwwxJHpZTx0ajEWkPRZJg3qu7aU8BPP2InPHDyGM4stlB0LlsHUwDZTa2G86sAyjJ4YATGY9//E5XjXyy8DIO7fpRgj0OoI5P16erEZe6aS7JIGzWrRilpsy/cUkm2oQxEIaDKmGjT6Ic4utVTLF/pcoousFgg6rFv88W8dwftuejD189J+9P3qeOD4Al78h7fhjsdn1GtkFhuy6Vx8v/IzavvSU0iTiRCDRB4IOuBNz92Df/2l67vqh0WrnRHQDH6x4alKUp05AMBYpaAWBKHZZ1WTMfRZyrAmDdEDRfqmHgj2TVZwdGYZJ+YbSksFgOdcNAY/5MpAS9YmWFqZvjh2+8A5XXMxUS2gYMZbW3DOUfcCbJOsyZFFP0EYMQLqpqn3VQK6Zw4tNT1c/VtfxTcfmxbbNn01eCyRbm9FHkHdC1T1rfIILCN2Tbuh4QWxoM4YQ8WxMCvbLyQHkE5mMZnOjmnAlmYxBXJiBAYTHWgbXqACnT4TJswst0R7iZSsIS8Qzc+ml131XS80qFAr8ghMg+Gtz78IdTfAU7N1lBwTlsnU32dqLsYrBdgm66nXEL33hiu346LJqjp3fcAiMxeIPIIg5JgcKqj7rS1rKIiM8WT1bVsbal9ULpMkS/c1NbWjti2qdbgfxJ5XJQ0lvsu5uttWJJm2tncag7pNmuV69o9uFhcSjID2oT9TugcXeQS5NHTOMFS0ceWuka7vMTS6V7ZlrYCcuS02fTVLKScCwbiUnYBoplct2lEFrZfGCAL1ANGDpQeCiyYq8AKO+bqHK3YOq9efu38CQJQ5pM9Ea00/losNpDc9E9JQoW0RkKYn2h8QIyAdV68spgeN2A4Fgm4DNBnpZHTWWoFiBNTllGbdXsBjM1Gi0ZQ11I15vOuz9+ETtz8h1xmOf0dlx1R59sUEI+iYPqqZziQNUbJByxd1BBXHUq1JkrNLXRryAo7Fpq+un15HQO8/vdBUr6cxAts0cPnOYTzvYiFJlm0TtmHEpKHxqgPTMHrKGvI0uYMwXEpIQ2FkFlvaID4xVFCvJxkBtaUoacVecY8gCih+GMIyxdrQjmmoQEDXgSYO5CE1vQQj6CANzdc9ObHQUmFTmjPqa2gTbn9cTFz0SY5eR5BkBPQ9xgKBzgjOs4KyTQ2SFEpJRtD0VOfRtkBQjQIBpfQNFSzUmu2dGbfojEA+QJNDYvsrdkaBau94FBQO7BlTPz/3IjEI/Nq/3I+3f+r7Mc1cMIJkIIjPhuquj7obYGLIaZOGaJClwa5giQcTiGQSCjx03EmZlttNu6cHhPZRa3kYLduwDKYNziJX3/PDWHppTBrSsoZcP8Svf/6BWHbMbY9M4fbD02oxGR1DRUtluRRiHkGX9FFtPyoQDEcySL0VoFwwUUqkPPpBCMYiH4C+j5laSw1UVAGuZwWdXmgqppDmEdDg+9bnXwRA3IeWGUlDFOBtk7XVETxwfAHX/97XY+3FCQHp9kZ0XYaKtmr1TOdhaYFC3bvViBEkZUZqS1G0zZSsIaaWDKX0UdpnyTFVXQatd0DdZ92ACtOymcWL0hdK9joipDX3A8R3+f2jItFCrxdR0hBL8wjigcBg8cyh3CM4j0BZC0oaKkbSkJ5XrmMsjRFIGYNzcROSOUqMgPNogLh46xC++e4Xxxrg7ZvUAsHeUQBigZX9kxW84JIJ+AHH139wJmZSKmnI7MwIppfEQCAGjDgjIAq9VZeGVDM48QBEg3mAaiGqmaAB+kPfeAy//5V4xTI9hE0vEL2FvBDVgo2yY+KYNLW3lB3YlvAIlrSgspgwiz1Z03DTvSfwqTufwlc1I3Cx6WFm2VUzeR17xis4Mi0M7mRBWafiu4YXMQtHMYJIolh2iRGYsWvjBhy2Eendl+8QjG5m2VVBR/cI9AIkut8WtDoCmrHTQPzCSyfx2mt34UcvnYRlCj+g5QdYavqiqaIhFnfR5aG7j87i5EIztqYBQRV+af2UhotJRhBJQ0B0724dLrQ1DCSQnFTSir1aGiNgjEnpUbAi+nxlbSEh6sUU+RCyoMyLm8X0vNaaPr712BRe+sf/iaYXqEAS6wUVhtEqax3WwrjryVn1vaQxAtNAikcQFZQxJu6Vs1p1MT1redbQeQC9UhGAysyIS0Pxco1xLXlaeQTSLKYbf4+c4Y9oC6LoAzZVKxMmqwW1r8u2D6FasLBjSwmMMXzyLc/G//38fWh6YdsMr00a0m703/3iQ6qwaTJFGiJGMFS0RDqnZugRaLBbbvmoFEx1jiQN3fbIFG57JN47io7R8CKDtVq0UClYePCkSM3dN1FRHkEaI3AsQ133WtPHR795BEBkWje9QDRBW3ZTGQFJbUDkyQDUSri7NASIgPGMC7bgOXI1O+o1VC6Y6l7RZ722HAABqHRMnRFUtayhqBCJo1IwYTBdGhILrNPqYICQJf74p6/Bjz19G2wp3dHKbWNVRw3YenYMJR2k9gRKBBoAskZBZwSRWQxEgWCyWujYMZTkpIJs/xCG0WelbQyDiaZzstcQIBgBsW8lDSlGEJnF+oRsp8xcOrXQwHePzODw2RpOzjfU9vpCPkHIVRDRr4cup33nyIzqhqpPtrozgsgjsE0DW4cLqWbxUs4INj7KjiUXzRAPBc3QlpqeoohdpSGNESxpgeCZe0exb6IS8yn0ATsJxpjyDHaMFLF3oowLx6JgQQHlVKJoRZjF0fnRTGyp6eGvv/0E/vTrjwEQjMAxRZohpX6qHj2OiWrRium4an9kFrs+KgVLyR963n+yHxA9IC0vVHJDtWCi7JhKAtkrA0Gyh09UR8DU4PnvD5zC4bO1GPWm980su2imZHaRCQq0M4JubagpoBgGw01vfx7+69U7AVAdgWQEFAg0aci2DPX6VXJN7Omai5YnFmUhqcmXOfTR+ZixtF76ezIgEyzTgB+EKitpvFKIunrGmtVRIGgPeoEKBLo0JO5f3QCPMQJNKovM4qQ0JFbympTPx4n5RswjEMdkCAKx5oStMYJGB4/A1dil/j1OSknsxHxTpbUenamrzxZr2x1wNRmISUNa4FxseBgpib5huuxJH9FM8Qj0OoKCabRtu55ZQ32pLN7MKNlmbKAvWEJqWWz4albRVRqSfYaGihZqrq8GmT3jFdz6rhfFKKptpj/chIsmKzg2V0fZsfDnb7g2Fjiox9HJ+Xgg0Hu6AxEVpypRejD0bA83CFE0TDWAVwoWhgoWbNNoO0clDcmW23rVLiAyIpIZGIoRuBojKNgqiIxVHIyUbDX70hkBBYWCaar3f+2hMyjaBp65d0wxApIxlpo+WtogrF9LQrygLGpNoHeipfNNftf6oFd3fWwdKiqm0tSkIcswVAryVbuIEbhqAFOSW8hj2nrRFvcbMQcv4LGGb0lYJoMXcuW1TFQdVcQWCwTzUeO2JPSWGIThoi1XpQtQKVhidm/EAyhAA3B6IPACDttgeI70te54fFplg9E2psFitQWASNSge5ECAU22Yumj2vNgGAw7RkqxwkvqhQW0t+2mSYUuDaUZ7OWCGds23mKiAyMIRCO+ohVNdPS/r0fWUB4I1oiiY8akH8aYyKBoemo2nGQERdsUDdHcQGm/Q0ULnENRdrpp7RSdtRN+5ccuwWt/RKz5o89oAaiup/qqVYCY1ekPCGUsUZYRY8KfGK86UcvfQNBsGqTLjhh0LZOpqlMCddlclrPhoUJ8wZi02Q4ZfE0/UP2XKoUo4BLzsU2GxWbkEThaywnbYiroPHBiAXvHK9i1pYSHTy21Hdf1w1RpiBArKKOZYRC2zfCaXoAtZSf2mmWINQyUWTxuqpXtdGnIMRleccU21Fs+Lt1WxZayjelaC5681rYRMQJ9ABWMwAQge9yHYWx1sCRsgxiBbDhX1TR7XRpKaeVMCEIOg4nBlKAnSdAKb2kewaSWNZSU2CgT6OKtVUwOFXDH4zO4bs9obHvLYGpyRIkJJcdUaaMLDVGkWUrM4JNmMSDqLE7ON3B8Tkx6Dp+NAoGu83shVwkDujSUxpZE7UqKWWywmMQIaHUEsiNrshEeeQduEKq2KoNCLg2tEeMVB6MVO/bacNHGYsNTM4NkIACENstY1JqCZq8qEMjXTTmQAIhJOGnYP1nFCy9Nb9FNjIC0X4JlMjUQTFQdNbMkRvCm5+zBRVKGSbb8VYzAsXDptiHsm6jEHn4gkj+owV7RNmAwERjELDloqzQmg6/hBipgDBUtlTlDGVJkXhMj0CU3R1a7AiKj5qLJCrYNFzGzLAbXZCZGMhBMDkWeS5IRAOmL06QVDzImGFdLM4vpga4npKGtQ0X8wgv3gzEmqs+XW2oAUDq+ljUEiIwmPZD7AY+tDpYE1REoaajqxPr8LzY9+EGomFPa2ryeZDA6SOLU++jojIHOcetwMVVvp3O3TOFtXL9/HHc8PqPutYgRRBo8fUbdLF5seBgu2VGKaswjiJ/zzi0lPDm9rHR5nRHoLSuER9D+7KUFAv1caFtAMgLte3K0qngv0AOBeI3Lfkq0wNIbPvZd/Pwnvtd2vH4hZwRrxHtf+bS2xUSGSjaWmpE0lBxkAKHNzi97alZFg5yeHglA5aS7fthWrdoLyCM4vZiUhqIB/sKxspqRPDG9jK1DBbz/J65Q/VKSJp8KdAUTf/RTzwAAfP6e47H90/6WXSENMcZUD6AlTdJpeIEKhvTwNrVVvcqOhbL8+74J4X1Qi4mlpoeyEzEGg4nZIu1PbFPB9pEiOBe52slMjLQB/KLJCu4/vpDKCJpa7x79M6R912SyL7fEZyxp+wDaB01AeDLTNRdjZUe00yA5JUxjBNH5eWGIQGrtabBMA17IMb3cgmMaQtIzoqZwr/urO/De//I0tWBNOiNoZxx6thwgzWIzPvCJz+WoBezbpaFITrp+/zhuuvckHjq5KD9nxAjoviEmXtKzhqRWn1ywpukFbTPyXaOl2JoM1BQRiO5tznlbmxGDybUJUorKkv2tAr0NtXb84aKtsrvo2dYXyxF1CuJ6zS67uPdYew+wfiJnBGvE1uGiyvAhDBeFNESGU3KQAQST0AcqxQhqlLsebUMDcLK1cS8gRhCEHBVtsLIMhtGKg23DBWwpOypr6OjMMvaOV2AYUXFUJ0agS2NJaUgFglagpJqqzO/XZ+V64Zd6eN0gWlmqYKnz3jchZC8qKKu1RJChmTadbzUWCKrYLovSTi802xlByndE8lAqI0hJIW246Z0sHVP4CsuuyJxqa4ugdeokTFQLmKm1lKRBgcLz4x4BeVIEzxftF5IzdoJtMGUWj1cdkZIpZ9anFhpwgxD/8J2j0f5SAkEyNRSIGAFJbl6KNCQGaFPN5NNaTNA218tCSKou1z0COgbdT2UnqjugQKD7EJSSXUhIq7tl5hBhVgsKUavo+CQIiDy/joxAk5ViS1Vqxx8uWm3SkJ42S9+xnmE4SOSBYAAgaYg6bybTRwHglVftwGuujZZ2pps66REAEQVeCyOoFiw1mIxqZrVlGnj7iy/GZ3/hejkjEef85EwdeyfiKarJQiDSUfVB1NZmino74VrTV8GOKn51nV5/ePSCMlrvoFKI8u/pvKjX0JKSnczYeeqBlqQhQGQOLSUZQcpMfv9kFYzF/0bHSJOG0rKP6HwWG55c21rLGvIiaSjp/4xXHUwtifTRoq23V4hnDRXtOCPww1Cu8dvBIzANKQ21VNICzdwpGOvdaqm6VUdaVhJ5BHRd/SBuFpdsSwXiTllDIniIv+0eLWGi6mC61lLNFsW5MmUIV1QNT9ws3lK2Y5MWLxCz62Qg2KkFAj3DDtAyuqRvkkwhpuuQRMWJV7Pr0hBVWJsGk116E2axLKTjnKvrrsudg0QuDQ0AwyVLSkN+bK1VHT953e7Y7zR7nUkNBPEZ+WrAGJMGpKsa0QFi4B4u2hgu2ihaYkZSa/mYWmq1MZ12aciPDVIAYnLAFtlTx/VFEVTViQLBUtOPZUPouqpO55c1n2W4aMNgcY/Ak3UEQwVLXTM6TwqugJjdUwbK6YUmFpuebMAmXkwbwN/03L24fOdwjFnQMZIppKrBoNP+HTmWobJBKql1BO0D67bhIhabPuYbHibkzJ0YkJdkBLo0JD2EztIQQ9MPsNAIFUukY6f1f2p1ZATxz6lLQ2HIEXLEGMH/8/JL1Xfc0SzWUkIZY7hq1whufWQqFtRMg6n7piy/35JWd5DGCCjTJ8nWdo2W1D6v3DWMp2brSvahwZzuD302Tz+nLWCfzBoi9YhiYlEWxtlSoqPrQGsrhFx8hxEjyAPBeQtqwNVwg1SjOA00eyUTT79p6abulBKYFSMlEQj0zBb9gaZCHmpbvTcZCBJGqeu3Z87oA9po2cH0Ukvp/PQZqT10jBGkSEOioMxXrbf/23MuxDUXblH7UWaxZAR0Lqo5n5QhqgULW8oOOOdwLANnFpuotXyMlGz4cvBIYwQjZRs/9vRtsdc6mcVeILTkVEZgGpiTWS0Vre5Etb9IkYaIvTw1s4xdW8TPwuiNewRFO97plP7e6V4hRuAFoWrDQN+Z/h3QgJhmFvtaVS9hSOuxlVZw9vQdUf8rx0xnBPqC94Cop7j1kanY5zOZJg0lqvobsjJYBIJIfqLvKskIqDHd9uGiut7j1QKmay2txiO9qFDsO50RpLWYoNXVCrYJ0xAJGro0NFSMJjINL1DXplxYnyG6L9IQY+wGxtgjjLHDjLH3pPy9wBj7Z/n3Oxlje7W/vVe+/ghj7BX9OJ9zjeGSjaYXigEmY8pXNZk1lMgw0P9fLSgAVJxITtBnW0W5MDtlDO0Z7yANUSAI2vPV9QeZlmRUOr8cLKg9dNwjiKfNAaLdRd0NUHFEI7etQ0W8+LKtsfPxAq5qFMjQ069T2bFUuiljDNuHizi92MRi08dwyVYzrqzfEwWbZL+h5LrHOhzLUBXdlYL4LFSABaRLQySjLLuRyUkMxtNm/G2MIBQBqSMjMISctuz6sYAKRCm9QDRb7lRQltw/SVSLTU/NlJOsgWAYTJ2HjqS3QfUU+uczDabuPzp/CgRzdReuH2K4ZAsGZYlsLWJvyUlL0TYxOVTArtGSCopbSjbKthnL6BLbticMpDGCkiMmUyQJ6esRAIIRlGwz1vrblf6Fkh21QFBcIVOwX1hzIGCMmQA+DOCVAC4H8DOMscsTb3sLgDnO+cUA/gTA78ttL4dY7P4KADcA+Eu5v/MaNDt6cqYe6yjZDYoRLLebxcojWGsgkDpuyTEVvdVnbUXbRNMPVI45zZIIShrS094SWrT+IG8piwVYiGbH2mkkGUErxSyWjKDSYVZEHsFi08NQ0VYPuh6cdo+WcLWs1AXEAEtm8XDRUn5J1hxtCjZJRqCWqeyQNTQnpSHyi8hHAtKloe0jkUlY1D6XJ2f849p5xzwC2Wuo0yBsy+rw5VbEVil4E0P5xRftxy+/5BIA6QVlnQrWxGfylcHaqbqZzqOtxUSCEVwtW23ojED/O0l/1M2VVuejDDlawU4xArv9nH/2WRfitdfuUisSbinbKDlWZBbLgTxtUZtOHgEQsatAqyMQ52CKQGCytoKyKJssYn3JlNdBoR9HeRaAw5zzI5xzF8CNAF6deM+rAfy9/PlzAH6MibLMVwO4kXPe4pw/AeCw3N95DdJL7z8+j2dog1A3UD1BulkspY61SkNSEy47kU6tDxhFSzRpo0ErGcSSjIDy33XoD+oWyYxo5htJQ9Is1jyCdEYggkgneY0GtTOLTezaUooYgfaZbnzbc/CeVz5N/b5zSxHH5xpYbIp8czJMs0p4ihEkPIJOLcfpfIgx0OClM4I0KUcPwvS5qAbAC0LVQiHJCEjO6dxiQkgSeoA1E9LQ66/bjddLDys9ayi9YI2y5WgW3U3KJDYHAF+8/yQ+cfsTspFctM224SK2DhVi95ielZZkBKeSgcAy4AaByvBKqwX43y+7FG941oWKEYyUbFQKUTfTQJ6jPlEoKlYYqhUBCeRbUCAJw4Q0ZBlqKVNiFNRrSPeOSHYaZBGZjn4Egl0Ajmm/H5evpb6Hc+4DWAAwnnFbAABj7G2MsYOMsYNTU1Npb9kwoMZznIul/LLAMBgqjqm6Hw7CI6DqYr2oKSkNAaKrpU5VCclA4KVJQ9oAQWbk3jg7WAAAIABJREFUjFqERzbmK4o6i9m6q4rldF2VdGnS76sdGYE4dshFVpBiBNrAMVS0Y5/jkm1Dqq3AcNHGmJTL0mbyaSh0YARqdbIO0hCBBi99IRcvJaAOFW2VLkufi+QEL+AqUBRtU00Q6Fo2/aBzIDBEZljLD9XsNSkNFeSaCqJSO1vWEBDVzyTlkDRQw0AA+MI9J/DJ7xyVTCO+zXV7RhWTFecf/Z0mT/TdJRmBbTLJCOhzdX5+okDgoKRJQ1RtncYIbr7vBF74f25THU+BiBEQu2qThiQjsLUFglxlFkeJCBSAs0qWa0U/AkHat528ezq9J8u24kXOP8Y5P8A5PzA5mV49u1GgFxo9e994l3fGUSlYKrMl5hH0IWsIiAbmkmNGs0xDv8HFTXd2saXSAXVE0lDn/Hf9d3ogp6XURIPgnvEygpDj0IkFbX2CQFUb63nyMzU3Nf1WPx9AFIzRg96NOdFSmWcWWxguWRir9uoRxOsIvvGDM5hddtXAkSxa0rcBooEizgh4LO2WsG2EBvvIz6GAc80FW/D//dfL8aLLJtV9QQGz4QZdzGIt/bIQeQ9ANHip7CtZCHf30Tn84S2PqH2k1REA1IraU3JKt95YjsliE4rFpt9WhAYAH/i/rsJfvvE69bvujdB7KSCkM4JQpUQnzWIdlKY5UrJj1cHKLE5Zy+Dw2RrcIFTXE4jYiWIEWvdRAPhfL70Eb3/xxaodOBDvNQRIRhCky1m9rPXdC/oRCI4DuED7fTeAk53ewxizAIwAmM247XkHGkS3DxdxwVhphXdHoAfZYPGZDy172K9AEJeG0hhBS620pkPlf/vUYbLdI1D53gZT0hIFAvp8F28VBWGHTi5icqgAxgQjeNWHvo0PfeNwTDueWXZjaaA69IFm30RFPUjdrtNl24fUz0NFG8/YPYKLJiodWUcSUR2BWDv5LX9/EDfe9ZRiNGn7iZvXkhWVdI8gPctnuzbrB4SMR4HAsQy85fn7UClYav80AWl6Qdf0UdoHBViaDNQSgYA8mFsOncaHbj2sZtZ+yNsKB8V2Jlw/VNJQp6I2On+9xcJS00sNiGMVRy12L/Yp/q5fZ/ocpxeFTEP3ObEOxQi6GK8T1QIKloHtI4VYXYKXMiCrCZNsTUETIyCa7ESMQLxO38eLL9uK518yIQr79Mpiy4hVrRMrTrLytErvfqAfgeAuAJcwxvYxxhwI8/fmxHtuBvBm+fNPAvgGF/1qbwbwBplVtA/AJQAG11BjnUAD4DP3jbV1qOwGuokKlhnbrh91BECUNVRyLO1hT2MEzdg6CASVPqo9wO2MIKLAtL9kINgvA4EfcqHJOhYWGh4enxJrLus3++yy29ks1hqZ6RJQt5nori0lbREhCzdcuQPfeNeLOpqrSUR1BKKbKOeiiIn6HaUGAm3f9FnaGEHKd0uBQP+ukrn4+v5pAtLwgs69hmIae3wyQINfIWFOkxFO62wHYZjKYChwKLO4y/dgazNiPxCGbtMLum4DRANqWZsckDR0Yr7dLNbTR7sZr0XbxL/98vPxxufsibWsIGmnmCINUW2IvphTkhFELSbix7NMQw32emUx7Y+egWQgSPNs+oE1BwKp+b8DwC0AHgbwGc75IcbYbzPGXiXf9jcAxhljhwG8E8B75LaHAHwGwEMAvgLg7Zzz9Gbv5xHGKg4uHCvjx6/a3tN2dBMlb9j+eQSSEWiDdLwxmHhtqpYuDRVMyp+WgcBP8wikPKN5DFNLcWlouGhjm1zCcbgoqDilrDbcIMYIgpB3lIbo2PtUK4iVA6ZhMFwi5aG0z7gS9PTRpjzPWtNXHVC7MQKDRec4LNeoDuTiK2kD69YEI7DNqPOmzsRo/zQBabidGYEeICKPQAaCVgDG4llqrh8FAvoevQ4FazS4B0oa6vw96FlDNOjN1d0VAzLdrxVHZwTi+jx8clG1KAfEtRYFZZ3NYh2XbhtC2RGtTFTVd5fKYoIbhErPpns8aRYbiQmhLduBcy6Kxwqy1xBAHkF7AALSs7j6gb5UK3DOvwTgS4nX3qf93ATw+g7bfgDAB/pxHhsFBcvEN3/1xT1vl9btEogezG4aZxaQDjpcSk+11FvtJhuqAWl1BCGGnfj7kqYYIGaSjmXEjnXx1irOLLZkloal2l43vHajs9LByKWZMPUESvYa6oTLtlVx37H51M+4EmxTdINt+WGss+pyl0BA51ORTfeAaNCutYQ2ni4NFeTnilJ9qVleWkM3JQ357Vo7QX+9rDyCSBpyTEOdI7X19kPx+7QMBMkmbPrn1Cufu5nFQr8Xgx0NtiFHakDUQfusFNoDgRuEuGrXiDp/CjZRHUG25yeWPqo8gnbmTCA5zDYNdS6UMt3JOLdkO3DVy8iKlipteoH6TtsZwcb1CHL0CUoa6sAI1tJrCBBr4f7Vf/sRvOiyyVhKIkEvXqHMJx3tWUPtHoFKddVu7Olaq22AvFiulzBcslB2TNXyouEGbfS3cx1BnBGkFZSl4VLFCHqfBzHGULRMJWUAghHUEimyOlTvI20Wq/r3Nzy5mldKIBhJ8QjSpCEVCGT3Vj+dYQDxgZbOx9TM4mTasi4NTUmJL7kMZfR+IQ35mcxiA67U7vV+RisxAiUNOe3SEBAt8wlEjKZbHUEaROM4KvZLMYsTE7WWPEbBipZHpe0D3okRiApvYkNJs9jrIA0NihHkgWADgQaR5I3WL4+AMYZXXrUjlrMczxqKfk6bLVPDLDLH0jwCS5MVFCNYSgkE0icYLgqPQC0RKDMm9HFsJbN4X6JL6Eozv+v2jIIxYPdouev7OqEg2wWrQNDyUXNFX6m074he03XtYa1bp5sSUIHo/Oja2SZTOn5swaKERwC0d4El6AMtXVf6DuuJVs1KGpKDD0lDfpiePkqBI4tZbFtRQZUe+LsVoen71IOqY0b9rvTiQTofqvnImpNflmsgc84jaShWWdwu17Q80W4lYgSJOgIjTRoKo6U4tWey6UWvJ5mXbkz3E3kg2ECgPPtOjGCtHoEOPSUxei16UNLMYiAy4IDudQS6WbzsBm0zZTKMh0t2bIAkj0Af1Dp5BFfsGsGz943hwN6xxGfqfp2uvXAUd//GyxQz6BUFy0DLC6POqi1fNb5Lfb/ZPnhRoCXDMe2cr9g5jI+/6QB+VC42ZBkdzOKERyD+3skjaDeuKXgnO3Tasvq1lfAIkquP6fsWBW8rm8WOZhbrK6NlNYv1+4kxplJIr04wgpbfuddQJ5Qdkcbd8qNOr/GCspRA4AdqkSDTYG2VxW3SkKwjiNZkNhO9htqZiDhWLg390CPKGop/LY42y+4XiHXEKov1hTM6BQJLM/n8NEYQSUMXjpXxyiu3wzENXDAaT6O9YucI9o6XccXO4dgAKRhB3KPolNq5a0sJ//wLz1XVwaoNdYaAObaGro5F2xRmscwWqbV8sTB9h/OM2mJH13eoGK+xSJNEGGN42eXbtIkAU4NaatZQUWcEK5vFFGDT1hamY7hBOyNIVgDr+3ZXYRbHpKEuLEL8nQJBfIAsOSa2DxeVwQ5EwYYCWVZpVc/8icxiPfCaMcba8gMlDTHGUHaiZVw7mcWWIariKbXVsQyxnKUl+n25+uvapoNKH827j24gdDaL+yMN6ShoBiRhJWmIziXWa8hK3uARI3AsA3/1xuvQcNvTAkdKNm57tzDU//muqLi84QWo+GFMv8/e/kEOmH28TmkoWiK9UF9rYVkujpOGdI8g3mSwm55O0AdJ/f3XXjiK6/ePx5oEdly8Xq/MldfV1Pal33uOZWCp6UeMoBYxgm5ZQzTD71pZbEX3kd68baXrEHkE8Ws9VnFw0WR7t1w3CGODdBaUVCDwU81iyxBrC+gTopYfKilU70DaySxWmVpuNODTsfWsIdsUcmyYIqP1E3kg2EBQHkFSGqK0yH5KQylZQyuZxYCURTSzuJM0pD84K7Vv0GfSTVfMhsgoTf69G3phBGtBuSDSC5vyIV6Sy252DARUAav9nRgBBYIsQd5OzNYJl20fwqf/x3Nwx+PT6rVuS1XS8WgfsRRifeabNIu19NFO0lDII0PT7jK7L5jx7pvJ8+sEOm41wQg++t+va7tP9KZzvfTs0RkBDeR6gDQNBttgoPXMqGhNpWcXTOURUJKPmWQE5MvIgEGsv2iZscpiWs9ENajbyOmjOfqDyjoygvTKYt3MXFkaSvMIVAuAVTx4gDAs3SCMHT8rI8iaNbRWiDbavlrW0/VDzNXdjuazY9FMUZeG4gsRrSSJAPGMn7QZv/5aJ63dVgOp7id0loY8rUVDzCNICTR0TMpsWqmgTDUvDHVpaHWMILmIEiAMaRqke0m91gNBWhdQy2RyII++/5YXqiBacSyVNUTSEEscnq45SUhxRhB1H7UNIxZE8qyhTQBlFnfyCAZhFnfQh7OYxWm9hsTqS6zNUOsGZVoaDEHIUW8FsYEqa/sHtTDNoBmBrDylAQ8QvYtWlIYSg2/JNjFb60EaMvVAkDIQ64GiU9aQfF0PrqbBVMO6pDRERigQLRSUtkIZEF13ksy6po9q3Ud1uWMlo19lDXXIJIufjwkvMUhnQVlrJe2nMAKDsdh5Rumj0fNLrDlILExDiKQhYgTRtpQ+ahkMhvxH2MgtJnL0CaQhd6wj6ONM97Ltw9g9WoqtiWoYLMpJ72IW04LgaXUEgBhYenvwxEOwWxrKCw0PBTuqtMy6SpNqS5xhkFgLqI223lpgoeGtbBYnmM1wyepJGtIH35UYQbdeQ+Jc4ueaJulRgVjTi1Yzm1pqwQ/T21zbSUbQLX1UtlgQ95FeR9A9IJLpmkUutC0mFqbxgxWrinWoNaXdIEqFNZkybS25whih5cdZR8HWAkGXgjIg6viqF5CRR0Dfp75tzgg2AVaShvqZPvqsfWP49q+9pO2Bopl8pwV1yIALQrEgeNo5PX3HcE+pmTQoXSjpvR9yOGaUftqpsjiJsYqDv/25A3jVM3ZmPvZqUC6YqLcis5jQ8ZqleATi/Tam5UJE/ZCGVmIM+nbJYKlag8TqCESWUtMPVJCeqrUQdGgxYSUYQXezWAzSuiwEdPcVxDHSA1kaCqqyOOxpgRc9n5+CfVEuMQmIz2WZIjtIL1rT18ymAbtj1lCSEZA0ZJtoyToC+g51NpGbxZsAKmsocdPecOV2tPxQraI0SBRts+MKVEB0k6ushpSZ7Od/6Xk9HVO1px7Tsl4shpJtYh5exzqCNLzkadtWftMaUXHECmvJBew7DU6UWdXGCIoWHjtbA7DyTFi8Jz1rSP3d0BlB96yhpIxlmQzw2tufU2O9C8bKuPfYPM4utjreH0oaSql1SILM4uTAtpo6gk6IdHi/J0ag9/xRS5Bqpq1lMtiGgdGyg8WGJ9JHvVAF0YJlKjmtUx0BfX/KIzCJERiYrrkyI0+8ZuSMYHOhXIhuJB07t5Twiy/a31Mn09WiaJtde/DQDIi0yn6wlBdeNolfveEyPOeiaO2Ggqy0dMz0at1zibJjoeWHqm0zodqREUhmk8IIlpo+SraJay5YeSW7+Iy/80As/r4CI3CSjCBdGqL0xm1yRbQluSZxavqoFU+JXMks5hwxnwXoziL088ySQED3zVLT78ksVl1AfVE9TumiNDM3DZFxNVZxYj5Kch0HQGcE8WOoau4kI1Dpo5H/FjOLc4/ghx/DRRuOZWC03HsztH6haBsdjWIgqtakmVyaR9ArqgULv/Sii2MGoCO7lw5a718N6DxJ3yd0qixWLSYSjIF8mHe85OK29aHTYBvxQToJfeDttlQlkOIRpPhQOtuje6LlCzknrZcRMRJlFndLH7XjWTPqmBl7DWVhBCoQtLyeAkEhIQ1RYNDX2ig6JiaqIhDU3QChVpWtm8UhF0EgOYmja6UWNCKPgNJHtWLN9fAIcmloA6Fom7j5Hc/DhWOr64HTr3Po9tCQR+D1kREQ9IFSZNUYmbTg9Qad58yyiy1lW7WJ6DQ47Z+s4Bm7R3DFzuHY61ftGsbRmWW89QX7Mh3XXkka0l4zOxaUpXsEtmIEWjDW9kHrXdfdAJynS090fiSZmd2yhkwyS8WMmNZnWLnXUHplcRrofBbqXk91BLo01PSj/ks0IBuM4XdffSWKtoH/8Q8HsSSXHFVZaxojCHin4rt0j6BgU/potGynfqnzyuJNgqdtH175TQPE635kd1cphgw4agvQz0BQstsZQZYHfr1B5zRTa2GyWlCBoJM0NF4t4KZ3PL/t9bf96H687Uf3Zz5ubMafljWkM4YVWkwkg5beGoTgpDACGrjSZB/Hig9u3QZ1GlypXfN4xcFS01/xfio5FkyDZUoppvNfbPqqMWEWiFbcQEs2FqTAQMHPMhmukj2NHMvAYkMu6ENLpcrutICQhpJGsdhHImvIjMzipqylofqTmFk8oF5DeSDIEcObr9/b9e9tHkEf9XtavBsQAebA3jHM190uW5wbEEuZqbm4evcIGBMN25LVrv1GWsdRHTFGsEJlccf0UTuddZQdC5bB1Aw+taBMSUMrs8UkIxitOHhypr6iWfz6A7tx1a6RTAkE+jU6sHd0xfcTqNV4U9YgFBUjgPxf+x4sA4uKEUTpo66WPprKCORrVHgWpY8ayiNwFCPQPYLBdB/NA0GOnkC9hvrpERBKemthy8A7X3Zp3/bdT5Cs0vAClBwT1YIlW0wM1tvp1Gsoeq27h6Bv12YWq8WP0qWhouysSQvwdOo1BAAN1wdj3Y3fyCOIGAGwchrtcNHGs/aNdX2POn9tkvIjF2YPBID4vA03zgjo3PQg6JgG5mQKsF7Q6AYhwpALaagLI1hOSR/1Q466G6SbxRsxa4gxNsYY+xpj7DH5f9vVZoxdwxj7DmPsEGPsfsbYT2t/+zvG2BOMsXvlv2vWcj45Bg9iBIPwCHRpaK2rsQ0S+my6ZJvKJO4kDfUL9gozfn2A6txiQjKCpDSkLTGq3mvpgcBEwY566KRmLUlpqOEFK9YD0KBJgWBUrqedpcI6K+gcL9laVet1ZwUVdjW8QPXgMlIYQcEyNY8gYgQAVDAwunoEVHzH1HEBxGQy/XgbdYWy9wD4Ouf8EgBfl78nUQfwJs75FQBuAPCnjDE9V+7dnPNr5L9713g+OQaM9QoE/dxvv6H7FsLHkIFgwMa2yuwx0ztp6gNGJ31+olpAyTbV8p6EtOVQkw0JV2IEShrqsmYyQU/tBIAXXDqJn7t+L67cNdJts55Ax+hFFiIUbSENCUYQX8RJZy2OZSh5i45HTKrlh13M4ogROVpnVGIfS01PBWLdY2ht0KyhVwN4kfz57wHcBuDX9Ddwzh/Vfj7JGDsLYBLA/BqPneMcwDFlH/UMOnCvSBYzbVTo+nTRNhUTGLSxTYN7p1kzY0zJEp0klrGKg0O/9Yq2WWpas8A0aajeis9gdUTSUHvb8STou6b00bGyg9981RVdt+kVdP7X7ckmJekoWLQKXYixCg3I4m/6relYouOq2IYWlpJrKPshgrC9qhiIGNti04/15aKgs9DwlOwaZwQbUBoCsI1zfgoA5P9bu72ZMfYsAA6Ax7WXPyAloz9hjBW6bPs2xthBxtjBqampNZ52jtWCZj3Lqo96/6i8YTDFCjZaEZkOXVYp2gaqBQtF21ixhfJaodpGdzkODTDdUjfTpIqVsoZEWrGpNO3UpnOUNeQFK04QovuofenNfuHaC7fg7S/ejxuu3N7ztiQNNf1AM4ujgjJC2hKWBcUIAoQhR9qloEA9tdTCzi3Rok0XyNRxXRraEJXFjLH/YIw9mPLv1b0ciDG2A8AnAfw855w+zXsBPA3AMwGMIcEmdHDOP8Y5P8A5PzA5OdnLoXP0EdFMjh7g/g5+tHbBRpaGYgun2yaGitbAjWIgGuS7XRvFGjL0LtKxkjRUsEQTwOUsWUNusGI9AB1HZSEN4Psu2ibe/YqnZe5eG99WLEcazxpKMYut9qBAAUFJQymMQA98+oJC11ywRTGZtPUizlmvIc75Szv9jTF2hjG2g3N+Sg70Zzu8bxjAvwP4Dc75d7V9n5I/thhjnwDwrp7OPse6w0k8wH0PBOcBI7Bl2wtXrkr1wksnMV7pSGb7elyge6ZWmsGYBZFZrPs0mikqGUGt1ey4f9K0WylLmCaRNIs3mhRYtE3MLruJOgLI/3WzWA8E8Tbort/NLI6226utpVC0TVy9ewQHj86dP1lDAG4G8Gb585sB3JR8A2PMAfB5AP/AOf9s4m875P8MwGsAPLjG88kxYNBNPmhGsJGzhoCogVzRNvHTz7wQv/OaKwd+TBqYs0hDvUotaXUEcWnIQEFjBOnpqyvXMRCSzDJL0731BBV2NbWsoVRGYGZgBF0W8QGAC8fjnQQoPZaktvWoLF7r0/ZBAC9jjD0G4GXydzDGDjDG/lq+56cA/CiAn0tJE/0UY+wBAA8AmADwu2s8nxwDRpIR9HsmR4xgI0tDQGQYFzO2yO4HaNbebZBXmS09Xj8rRRqi75Yx8TMtmgJ0aDGhvZbVLB4Us1wrirZcjlRb5pI+UowR6CnPtEKeqZvFHaQhI50RAMAzKRCcL72GOOczAH4s5fWDAN4qf/5HAP/YYfuXrOX4OdYfZJRSWwW7j2YxcH5IQ0CUIdTLSmxrRRaPQLGGXqWhFLOYjlO0TDDGYrJR6prF+rYreBTKLG6tvJrZuUDRNlBr+ghC3lZQZnZkBJQ1pJnFPF0aijGCRG+x6/aMwmBRkDHWoftoXlmcoydQ8RR13uz3TI5m2Bs9EBAjKK0jI8iyQBEN6L1KLVEb6ni/JyDKbdeDRLpZvHJBGyHpEWw0RlCwTCw0xGSHGEFaQVmqWWxFHkFHRqC+SxbLGgJE9fRH3ngdnr5juO14+cI0OTYEhuRaBYMKBGUlDW2sGWISlImiF8ENGivVEYi/tevY2fbdhRHY8Zku0L3FBLCyNBW1iN6YHkHRNqP6gBUKygj6egSA9AjCDum68rULRsup1/LlV0Qpr+eDWZxjk4GKp+ZkM7i+ewRkFpsbr+uojrJmFq8XsjCCSBpabfpoe0FZNNPtXvltGizWs78bTIPBMtgGzhrSjPLESmFpWUOmXLxGvBZ5BCFPryMwDQbG2o3iNGyIOoIcOXTQurwzihH0dyZXPG88AmkWrycj6KGOoGdpiAJBStZQVkYQO34GRlKwDG01s431fevfK01O0j6bk5CD9NdaftBRGmJMFE8mjeI0xNcszttQ59gAIElkbtldscPkalB2zg9pKGIE6zeA6bpyJyiPoEdGQO9PW+5SX4tX/a3D/h3TyFRHAIgBM2pit7G+71jbBysybRmLz9CTrEn/mRhBmjQEAB9543W4bPvQiueiP2MbtddQjk2GgmXANsUi3nqzrH5h23ABW8r2hpshJlE5Bx6BrdJHM0hDPQ6stin6FOmDFmUBFVNmvZ0mALZlAK1sxxeBRWaf9Ri4Bg2dEejrESSZTlqQjHsE6YwAAH700mwdEozcLM6x0cAYw1DRxuyyOxBd903P3YtXPWNX3/fbb5wLj0BJQ11ks7Se+Vnwmmt3YVcie8VJmsV61lCHgb4XacjR9PVOs+ZzhXggiFhWsoGcYgR2OyOgQJDWdK4X6Jd6Q9YR5NicqBYszC67A6HzRdvE9pGNbRQDwJaSDcayLaLeL6hA0GXQzFJ9nIYrdo7gip3xFtBR1hANdlodQcelMLNLUzRgbjRZCEiYxSp9lLUzgjSPQGtDHXaoLO4F8RXK8kCQY4OAfIKNlvu9nnjtdbuxd6Ki1vJdD2SRhlbLCNJAWUAke+i6eaeBnmb5maQhOdhuNFkI6MQIWNug7qRkVDHG4FiGLCgDnDV+F7q0FIS84/KXa8HG+wZybHj8/+3dfYxc11nH8e9v38Zt3MQvcR0n62AbDMRAcNyt5VCEWsdp07SqTeWAI1AtmhAKQmpVAXGIhIQUpIQ/CKpUKZgGMFLJS9NGsUqq4Dgu/QPFrSFp4tRyvXULtWzFS5u0oJbmxQ9/3DO7d9fzundmZ3bu7yOt5t5z78zce+yZZ855zj23OnKozIHg0iWjvPvnGs663nGtdA1Vt3UiEED267Zmi6BJ11BLyeJq8rsPR4jlA0Ellyye29Kq1SKorr/Wqa6hOf+W3cgT9N+/gPW9aiDo9yGeg2Z61FCjrqEaY92LuKQyPD3FdrMri/PH2Nrw0dnDMvtJrWTxyJAuzhHMmWiuqjIy1JWuIehO95C7hqxtM11D/fcBHmSt/NoeGc76sTs1muuB334H48uzi55aHjVEa11DYyPNu7p6ZXaOIFu+ZWKcX167bNZ+tbqGquuNpphox9xA2Y2EsQOBta06zUQ/foAHWfVq1MbTUA91dLqGiXUzt3mcPelc7WOotkgWfbJ45OIWwcS6FbPqA2pfRwCkHEHqGiraIpADgfWhpc4R9IQkPn7DRt7TIDcxNjzUteRrvvujaddQS8ni/p1yPN8d1Oj4KjWG1lbXX0uzjxZtESxEjsCBwNo2nSPoww/woPvEjp9tuP2WiXGuWdP8atX5yP9KbtY11E6yuB8vHqx2BzW7TmSmRTB7v3yLoGiOYO60192YZsKBwNpWnYq60/cisOJqXQ/QKfkWQb0v+pmuodaHjza69WavVANAsylEGiWLsykmas8+2o5q19DW9St49PeuL/Radd+jK69qA81dQ+WU7/6o993WzqihuTdp7ye1ZlxtvF/9HEHROLcQ1VPoLSStkHRI0qn0uLzOfm/mblN5MFe+XtLR9PxH0v2Nrc+9reJkcRlVvxRHh+uPSpoZNdRCsni09XzCQsvuyDbUvEUwPMSQLp5zKj9qqGiLoGiOoRVFP8n7gMMRsRE4nNZr+XFEbE5/H8qV3wfcn57/CnBbweOxBbDUOYJSqn5xN+rznu4aannSuf79QbFkdLhpjmBoSHzq1uv4za1XzyrPZmHtTLK4GkgiujMgl3PGAAALSUlEQVQFNRQPBDuBA2n5ALCr1Scq+0mxHXhsPs+33vF1BOVUmb5qudHsp61PG1Hp4+sIIMsPtDKp4AevvfKiCfsqox1MFqdA8uaF/g0EqyPiHEB6rDeubYmkY5KelVT9sl8JvBoRb6T1M0DdaScl3ZFe49jU1FTBw7YiLvV1BKU0NjyUrmNo0CIYaadF0L/XEUDW3TPfacZnksUduI4gPb9L96QBWhg1JOlp4Ioam+5u432ujoizkjYAz0h6Efhhjf3qnmpE7Af2A0xMTHSxSqyZ6WSxp5golWq/eaNEcDuT3o21kU/ohUsqI1xSmV8gqCaLo4PXEVzoYougaSCIiB31tkl6WdKaiDgnaQ1wvs5rnE2PpyV9GbgO+DywTNJIahWMA2fncQ62wKpdQ84RlE9lZLhh11A7X+7TN3vv0/9H9+z6xen/6+2qJouHOnAXv8XQNXQQ2JuW9wJPzN1B0nJJlbR8OfAu4BuRZT6OALsbPd/6z9jIEJcuGZn3h8QWr8rIUONkcfV+CIt80jmA665ezsbV87s4Lz8NddHZR6tdQxe6mCwu+km+F3hU0m3AfwG3AEiaAD4WEbcD1wB/I+kCWeC5NyK+kZ5/J/CwpHuA54AHCx6PLZCH7tjGlZe9pfmONlAqoxffpStvpIV7JlRNTzo3gF2MlZHsCuAhXSh8HUA1hdLNFkGhQBAR3wNuqFF+DLg9Lf8b8Et1nn8a2FrkGKw3unX1qvW3yshww2GM+dtPNn+t5tNqL1b5+xYXvo5gOlncv11DZlYiS0aHmgwf1azHRvp5Guqi8lckd+o6gp4mi83MqrIWwZt1t8+MGmolWZwuKBvArqGx3NQTOzatLvRa1UDyRh8ni82sRJoNH23nxjSD3DX0M6uWcsWlSzjw0a1subrmzDst64vho2ZmVR/eMs6PX6/fIhib7hpqI1k8gF1D1//0Sp7904vSp/OyEDkCBwIza9nud4w33F7tEmonWdyvF5T1i+lA0Pn70Uzzv4CZdcxoG9NGjPX5FBP9ojpct5vXETgQmFnHjE1fUNb8q2VpZQQJX5jYxEyLwF1DZrYI/MKVl/HOdctZd/klTfdd9tYxHvrdbVw77mtSGpluETgQmNlisHbFW/ncx36l5f23bVjZxaMZDL6gzMys5Kq59H6edM7MzLrIyWIzs5JbiGSxA4GZWR8bnm4RdO89HAjMzPpY0dlLW3qPrr+DmZnNW9E7nLXCgcDMrI8VvcNZS+9R5MmSVkg6JOlUerxomj1J75H0fO7v/yTtStv+QdK3c9s2FzkeM7NBsxhaBPuAwxGxETic1meJiCMRsTkiNgPbgR8B/5Lb5Y+r2yPi+YLHY2Y2UIre2KYVRQPBTuBAWj4A7Gqy/27gSxHxo4Lva2ZWCouhRbA6Is4BpMe3N9l/D/DQnLK/kPSCpPslVeo9UdIdko5JOjY1NVXsqM3MFom+CASSnpZ0vMbfznbeSNIaspvYP5Urvgv4eeCdwArgznrPj4j9ETEREROrVq1q563NzBathbhdQ9NJ5yJiR71tkl6WtCYizqUv+vMNXuo3gMcj4vXca59Liz+R9PfAH7V43GZmpdD3o4aAg8DetLwXeKLBvrcyp1soBQ8kiSy/cLzg8ZiZDZS+6Bpq4l7gRkmngBvTOpImJH2mupOkdcBa4F/nPP+zkl4EXgQuB+4peDxmZgNlIVoEhe5HEBHfAy66Q3NEHANuz61/B7iqxn7bi7y/mdmgWwwtAjMz6yIHAjOzklsMyWIzM+sitwjMzEpuMUwxYWZmXTS0AN/SDgRmZn3MXUNmZiXnZLGZWcm5RWBmVnJOFpuZlZxvXm9mZl3nQGBmVnIOBGZmJedAYGZWcg4EZmYl50BgZlZyDgRmZiVXKBBIukXSS5IuSJposN9Nkk5KmpS0L1e+XtJRSackPSJprMjxmJlZ+4q2CI4DHwa+Um8HScPAp4H3A5uAWyVtSpvvA+6PiI3AK8BtBY/HzMzaVCgQRMSJiDjZZLetwGREnI6I14CHgZ2SBGwHHkv7HQB2FTkeMzNr30LkCK4CvptbP5PKVgKvRsQbc8prknSHpGOSjk1NTXXtYM3Mymak2Q6SngauqLHp7oh4ooX3qDVRRjQoryki9gP7ASYmJuruZ2Zm7WkaCCJiR8H3OAOsza2PA2eB/waWSRpJrYJquZmZLaCF6Br6GrAxjRAaA/YAByMigCPA7rTfXqCVFoaZmXVQ0eGjvy7pDHA98M+SnkrlV0p6EiD92v9D4CngBPBoRLyUXuJO4JOSJslyBg8WOR4zM2tf066hRiLiceDxGuVngZtz608CT9bY7zTZqCIzM+sRX1lsZlZyDgRmZiXnQGBmVnIOBGZmJVcoWWxmZt33tx+ZIBtx3x0OBGZmfe7GTau7+vruGjIzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzklM3r1brFklTwH/O8+mXk90dzWZzvdTmeqnN9VJfP9fNT0XEqrmFizIQFCHpWERM9Po4+o3rpTbXS22ul/oWY924a8jMrOQcCMzMSq6MgWB/rw+gT7leanO91OZ6qW/R1U3pcgRmZjZbGVsEZmaW40BgZlZypQoEkm6SdFLSpKR9vT6ebpP0d5LOSzqeK1sh6ZCkU+lxeSqXpE+lunlB0pbcc/am/U9J2tuLc+kUSWslHZF0QtJLkj6eyktdLwCSlkj6qqSvp7r581S+XtLRdJ6PSBpL5ZW0Ppm2r8u91l2p/KSk9/XmjDpL0rCk5yR9Ma0PTr1ERCn+gGHgW8AGYAz4OrCp18fV5XP+NWALcDxX9pfAvrS8D7gvLd8MfAkQsA04mspXAKfT4/K0vLzX51agTtYAW9Ly24BvApvKXi/pnAQsTcujwNF0zo8Ce1L5A8Dvp+U/AB5Iy3uAR9LypvT5qgDr0+duuNfn14H6+STwT8AX0/rA1EuZWgRbgcmIOB0RrwEPAzt7fExdFRFfAb4/p3gncCAtHwB25cr/MTLPAsskrQHeBxyKiO9HxCvAIeCm7h99d0TEuYj4j7T8P8AJ4CpKXi8A6Rz/N62Opr8AtgOPpfK5dVOts8eAGyQplT8cET+JiG8Dk2Sfv0VL0jjwAeAzaV0MUL2UKRBcBXw3t34mlZXN6og4B9mXIvD2VF6vfga23lKT/TqyX76uF6a7P54HzpMFt28Br0bEG2mX/HlO10Ha/gNgJYNZN38N/AlwIa2vZIDqpUyBQDXKPHZ2Rr36Gch6k7QU+DzwiYj4YaNda5QNbL1ExJsRsRkYJ/u1ek2t3dJjKepG0geB8xHx7/niGrsu2nopUyA4A6zNrY8DZ3t0LL30curaID2eT+X16mfg6k3SKFkQ+GxEfCEVl75e8iLiVeDLZDmCZZJG0qb8eU7XQdp+GVlX5KDVzbuAD0n6DlmX8nayFsLA1EuZAsHXgI0p0z9GlsQ52ONj6oWDQHWEy17giVz5R9IomW3AD1IXyVPAeyUtTyNp3pvKFqXUV/sgcCIi/iq3qdT1AiBplaRlafktwA6yHMoRYHfabW7dVOtsN/BMZFnRg8CeNHpmPbAR+OrCnEXnRcRdETEeEevIvjeeiYjfYpDqpdfZ6oX8IxsB8k2yfs+7e308C3C+DwHngNfJfo3cRtZXeRg4lR5XpH0FfDrVzYvARO51PkqW2JoEfqfX51WwTn6VrDn+AvB8+ru57PWSzuda4LlUN8eBP0vlG8i+sCaBzwGVVL4krU+m7Rtyr3V3qrOTwPt7fW4drKN3MzNqaGDqxVNMmJmVXJm6hszMrAYHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzK7n/Bzo6S4e4EAW2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LRTrainer.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me dio underfitting. Vamos a agregarle un layer intermedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,h_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(vocab_size,h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "h_dim = 1000\n",
    "model2 = TwoLayerNet(vocab_size,h_dim)\n",
    "device = 'cuda:1'\n",
    "TwoLayerNetTrainer = Trainer(train_loader,dev_loader,model2,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Loss function: BCE\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.0001\n",
      "Number of epochs: 100\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.6930274963378906\n",
      "Accuracy on validation dataset: 1949/5000 (38.98%)\n",
      "Accuracy on training dataset: 7979/20000 (39.90%)\n",
      "\n",
      "Epoch: 2, Batch number: 0, Loss: 0.25974240899086\n",
      "Accuracy on validation dataset: 2528/5000 (50.56%)\n",
      "Accuracy on training dataset: 9998/20000 (49.99%)\n",
      "\n",
      "Epoch: 3, Batch number: 0, Loss: 0.2372168004512787\n",
      "Accuracy on validation dataset: 2538/5000 (50.76%)\n",
      "Accuracy on training dataset: 10014/20000 (50.07%)\n",
      "\n",
      "Epoch: 4, Batch number: 0, Loss: -0.6233561635017395\n",
      "Accuracy on validation dataset: 2565/5000 (51.30%)\n",
      "Accuracy on training dataset: 10101/20000 (50.51%)\n",
      "\n",
      "Epoch: 5, Batch number: 0, Loss: -0.12244324386119843\n",
      "Accuracy on validation dataset: 2630/5000 (52.60%)\n",
      "Accuracy on training dataset: 10429/20000 (52.15%)\n",
      "\n",
      "Epoch: 6, Batch number: 0, Loss: -1.1425628662109375\n",
      "Accuracy on validation dataset: 2725/5000 (54.50%)\n",
      "Accuracy on training dataset: 10818/20000 (54.09%)\n",
      "\n",
      "Epoch: 7, Batch number: 0, Loss: -0.9646027684211731\n",
      "Accuracy on validation dataset: 2788/5000 (55.76%)\n",
      "Accuracy on training dataset: 11109/20000 (55.55%)\n",
      "\n",
      "Epoch: 8, Batch number: 0, Loss: -2.055283546447754\n",
      "Accuracy on validation dataset: 2790/5000 (55.80%)\n",
      "Accuracy on training dataset: 11118/20000 (55.59%)\n",
      "\n",
      "Epoch: 9, Batch number: 0, Loss: -1.750929832458496\n",
      "Accuracy on validation dataset: 2825/5000 (56.50%)\n",
      "Accuracy on training dataset: 11234/20000 (56.17%)\n",
      "\n",
      "Epoch: 10, Batch number: 0, Loss: -3.4832468032836914\n",
      "Accuracy on validation dataset: 2830/5000 (56.60%)\n",
      "Accuracy on training dataset: 11224/20000 (56.12%)\n",
      "\n",
      "Epoch: 11, Batch number: 0, Loss: -5.0467095375061035\n",
      "Accuracy on validation dataset: 2828/5000 (56.56%)\n",
      "Accuracy on training dataset: 11257/20000 (56.28%)\n",
      "\n",
      "Epoch: 12, Batch number: 0, Loss: -2.9449892044067383\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11336/20000 (56.68%)\n",
      "\n",
      "Epoch: 13, Batch number: 0, Loss: -4.2401862144470215\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11322/20000 (56.61%)\n",
      "\n",
      "Epoch: 14, Batch number: 0, Loss: -6.8724188804626465\n",
      "Accuracy on validation dataset: 2839/5000 (56.78%)\n",
      "Accuracy on training dataset: 11307/20000 (56.53%)\n",
      "\n",
      "Epoch: 15, Batch number: 0, Loss: -4.173383712768555\n",
      "Accuracy on validation dataset: 2837/5000 (56.74%)\n",
      "Accuracy on training dataset: 11296/20000 (56.48%)\n",
      "\n",
      "Epoch: 16, Batch number: 0, Loss: -8.280184745788574\n",
      "Accuracy on validation dataset: 2852/5000 (57.04%)\n",
      "Accuracy on training dataset: 11371/20000 (56.85%)\n",
      "\n",
      "Epoch: 17, Batch number: 0, Loss: -11.077707290649414\n",
      "Accuracy on validation dataset: 2849/5000 (56.98%)\n",
      "Accuracy on training dataset: 11361/20000 (56.80%)\n",
      "\n",
      "Epoch: 18, Batch number: 0, Loss: -10.854331970214844\n",
      "Accuracy on validation dataset: 2856/5000 (57.12%)\n",
      "Accuracy on training dataset: 11398/20000 (56.99%)\n",
      "\n",
      "Epoch: 19, Batch number: 0, Loss: -11.474161148071289\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11345/20000 (56.73%)\n",
      "\n",
      "Epoch: 20, Batch number: 0, Loss: -16.579111099243164\n",
      "Accuracy on validation dataset: 2841/5000 (56.82%)\n",
      "Accuracy on training dataset: 11305/20000 (56.52%)\n",
      "\n",
      "Epoch: 21, Batch number: 0, Loss: -13.028992652893066\n",
      "Accuracy on validation dataset: 2852/5000 (57.04%)\n",
      "Accuracy on training dataset: 11379/20000 (56.90%)\n",
      "\n",
      "Epoch: 22, Batch number: 0, Loss: -17.679019927978516\n",
      "Accuracy on validation dataset: 2854/5000 (57.08%)\n",
      "Accuracy on training dataset: 11386/20000 (56.93%)\n",
      "\n",
      "Epoch: 23, Batch number: 0, Loss: -22.502239227294922\n",
      "Accuracy on validation dataset: 2856/5000 (57.12%)\n",
      "Accuracy on training dataset: 11401/20000 (57.01%)\n",
      "\n",
      "Epoch: 24, Batch number: 0, Loss: -18.088687896728516\n",
      "Accuracy on validation dataset: 2858/5000 (57.16%)\n",
      "Accuracy on training dataset: 11403/20000 (57.02%)\n",
      "\n",
      "Epoch: 25, Batch number: 0, Loss: -12.12723159790039\n",
      "Accuracy on validation dataset: 2849/5000 (56.98%)\n",
      "Accuracy on training dataset: 11362/20000 (56.81%)\n",
      "\n",
      "Epoch: 26, Batch number: 0, Loss: -27.14358139038086\n",
      "Accuracy on validation dataset: 2845/5000 (56.90%)\n",
      "Accuracy on training dataset: 11332/20000 (56.66%)\n",
      "\n",
      "Epoch: 27, Batch number: 0, Loss: -26.276742935180664\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11351/20000 (56.76%)\n",
      "\n",
      "Epoch: 28, Batch number: 0, Loss: -38.29875564575195\n",
      "Accuracy on validation dataset: 2858/5000 (57.16%)\n",
      "Accuracy on training dataset: 11415/20000 (57.08%)\n",
      "\n",
      "Epoch: 29, Batch number: 0, Loss: -25.169614791870117\n",
      "Accuracy on validation dataset: 2864/5000 (57.28%)\n",
      "Accuracy on training dataset: 11449/20000 (57.24%)\n",
      "\n",
      "Epoch: 30, Batch number: 0, Loss: -62.33893966674805\n",
      "Accuracy on validation dataset: 2860/5000 (57.20%)\n",
      "Accuracy on training dataset: 11399/20000 (56.99%)\n",
      "\n",
      "Epoch: 31, Batch number: 0, Loss: -25.579723358154297\n",
      "Accuracy on validation dataset: 2869/5000 (57.38%)\n",
      "Accuracy on training dataset: 11454/20000 (57.27%)\n",
      "\n",
      "Epoch: 32, Batch number: 0, Loss: -30.487308502197266\n",
      "Accuracy on validation dataset: 2864/5000 (57.28%)\n",
      "Accuracy on training dataset: 11438/20000 (57.19%)\n",
      "\n",
      "Epoch: 33, Batch number: 0, Loss: -39.68431854248047\n",
      "Accuracy on validation dataset: 2859/5000 (57.18%)\n",
      "Accuracy on training dataset: 11401/20000 (57.01%)\n",
      "\n",
      "Epoch: 34, Batch number: 0, Loss: -54.528045654296875\n",
      "Accuracy on validation dataset: 2864/5000 (57.28%)\n",
      "Accuracy on training dataset: 11413/20000 (57.06%)\n",
      "\n",
      "Epoch: 35, Batch number: 0, Loss: -44.06672668457031\n",
      "Accuracy on validation dataset: 2865/5000 (57.30%)\n",
      "Accuracy on training dataset: 11454/20000 (57.27%)\n",
      "\n",
      "Epoch: 36, Batch number: 0, Loss: -54.756797790527344\n",
      "Accuracy on validation dataset: 2861/5000 (57.22%)\n",
      "Accuracy on training dataset: 11431/20000 (57.16%)\n",
      "\n",
      "Epoch: 37, Batch number: 0, Loss: -71.57633972167969\n",
      "Accuracy on validation dataset: 2861/5000 (57.22%)\n",
      "Accuracy on training dataset: 11430/20000 (57.15%)\n",
      "\n",
      "Epoch: 38, Batch number: 0, Loss: -62.52044677734375\n",
      "Accuracy on validation dataset: 2861/5000 (57.22%)\n",
      "Accuracy on training dataset: 11429/20000 (57.15%)\n",
      "\n",
      "Epoch: 39, Batch number: 0, Loss: -62.107994079589844\n",
      "Accuracy on validation dataset: 2871/5000 (57.42%)\n",
      "Accuracy on training dataset: 11472/20000 (57.36%)\n",
      "\n",
      "Epoch: 40, Batch number: 0, Loss: -62.619728088378906\n",
      "Accuracy on validation dataset: 2868/5000 (57.36%)\n",
      "Accuracy on training dataset: 11461/20000 (57.30%)\n",
      "\n",
      "Epoch: 41, Batch number: 0, Loss: -75.04025268554688\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11498/20000 (57.49%)\n",
      "\n",
      "Epoch: 42, Batch number: 0, Loss: -81.6015396118164\n",
      "Accuracy on validation dataset: 2871/5000 (57.42%)\n",
      "Accuracy on training dataset: 11473/20000 (57.37%)\n",
      "\n",
      "Epoch: 43, Batch number: 0, Loss: -100.4404525756836\n",
      "Accuracy on validation dataset: 2872/5000 (57.44%)\n",
      "Accuracy on training dataset: 11466/20000 (57.33%)\n",
      "\n",
      "Epoch: 44, Batch number: 0, Loss: -81.52525329589844\n",
      "Accuracy on validation dataset: 2871/5000 (57.42%)\n",
      "Accuracy on training dataset: 11466/20000 (57.33%)\n",
      "\n",
      "Epoch: 45, Batch number: 0, Loss: -58.657230377197266\n",
      "Accuracy on validation dataset: 2871/5000 (57.42%)\n",
      "Accuracy on training dataset: 11453/20000 (57.27%)\n",
      "\n",
      "Epoch: 46, Batch number: 0, Loss: -107.90834045410156\n",
      "Accuracy on validation dataset: 2866/5000 (57.32%)\n",
      "Accuracy on training dataset: 11427/20000 (57.13%)\n",
      "\n",
      "Epoch: 47, Batch number: 0, Loss: -82.94672393798828\n",
      "Accuracy on validation dataset: 2873/5000 (57.46%)\n",
      "Accuracy on training dataset: 11477/20000 (57.38%)\n",
      "\n",
      "Epoch: 48, Batch number: 0, Loss: -119.59503173828125\n",
      "Accuracy on validation dataset: 2873/5000 (57.46%)\n",
      "Accuracy on training dataset: 11453/20000 (57.27%)\n",
      "\n",
      "Epoch: 49, Batch number: 0, Loss: -98.46298217773438\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11486/20000 (57.43%)\n",
      "\n",
      "Epoch: 50, Batch number: 0, Loss: -114.75825500488281\n",
      "Accuracy on validation dataset: 2864/5000 (57.28%)\n",
      "Accuracy on training dataset: 11386/20000 (56.93%)\n",
      "\n",
      "Epoch: 51, Batch number: 0, Loss: -125.70240783691406\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11479/20000 (57.40%)\n",
      "\n",
      "Epoch: 52, Batch number: 0, Loss: -104.17420959472656\n",
      "Accuracy on validation dataset: 2872/5000 (57.44%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 11429/20000 (57.15%)\n",
      "\n",
      "Epoch: 53, Batch number: 0, Loss: -139.82244873046875\n",
      "Accuracy on validation dataset: 2873/5000 (57.46%)\n",
      "Accuracy on training dataset: 11450/20000 (57.25%)\n",
      "\n",
      "Epoch: 54, Batch number: 0, Loss: -128.30020141601562\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11505/20000 (57.52%)\n",
      "\n",
      "Epoch: 55, Batch number: 0, Loss: -111.06503295898438\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11473/20000 (57.37%)\n",
      "\n",
      "Epoch: 56, Batch number: 0, Loss: -168.22274780273438\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11489/20000 (57.45%)\n",
      "\n",
      "Epoch: 57, Batch number: 0, Loss: -206.28118896484375\n",
      "Accuracy on validation dataset: 2871/5000 (57.42%)\n",
      "Accuracy on training dataset: 11410/20000 (57.05%)\n",
      "\n",
      "Epoch: 58, Batch number: 0, Loss: -168.37181091308594\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11501/20000 (57.51%)\n",
      "\n",
      "Epoch: 59, Batch number: 0, Loss: -194.78602600097656\n",
      "Accuracy on validation dataset: 2873/5000 (57.46%)\n",
      "Accuracy on training dataset: 11438/20000 (57.19%)\n",
      "\n",
      "Epoch: 60, Batch number: 0, Loss: -165.8206787109375\n",
      "Accuracy on validation dataset: 2878/5000 (57.56%)\n",
      "Accuracy on training dataset: 11513/20000 (57.56%)\n",
      "\n",
      "Epoch: 61, Batch number: 0, Loss: -190.68673706054688\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11455/20000 (57.27%)\n",
      "\n",
      "Epoch: 62, Batch number: 0, Loss: -210.1436767578125\n",
      "Accuracy on validation dataset: 2868/5000 (57.36%)\n",
      "Accuracy on training dataset: 11378/20000 (56.89%)\n",
      "\n",
      "Epoch: 63, Batch number: 0, Loss: -243.33184814453125\n",
      "Accuracy on validation dataset: 2870/5000 (57.40%)\n",
      "Accuracy on training dataset: 11395/20000 (56.98%)\n",
      "\n",
      "Epoch: 64, Batch number: 0, Loss: -180.91510009765625\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11512/20000 (57.56%)\n",
      "\n",
      "Epoch: 65, Batch number: 0, Loss: -141.4734649658203\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11505/20000 (57.52%)\n",
      "\n",
      "Epoch: 66, Batch number: 0, Loss: -224.20144653320312\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11493/20000 (57.47%)\n",
      "\n",
      "Epoch: 67, Batch number: 0, Loss: -218.1966552734375\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11478/20000 (57.39%)\n",
      "\n",
      "Epoch: 68, Batch number: 0, Loss: -172.1482391357422\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11441/20000 (57.20%)\n",
      "\n",
      "Epoch: 69, Batch number: 0, Loss: -274.7476501464844\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11466/20000 (57.33%)\n",
      "\n",
      "Epoch: 70, Batch number: 0, Loss: -213.01805114746094\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11445/20000 (57.23%)\n",
      "\n",
      "Epoch: 71, Batch number: 0, Loss: -135.18617248535156\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11495/20000 (57.48%)\n",
      "\n",
      "Epoch: 72, Batch number: 0, Loss: -219.23146057128906\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11460/20000 (57.30%)\n",
      "\n",
      "Epoch: 73, Batch number: 0, Loss: -216.5315399169922\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11461/20000 (57.30%)\n",
      "\n",
      "Epoch: 74, Batch number: 0, Loss: -304.54156494140625\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11431/20000 (57.16%)\n",
      "\n",
      "Epoch: 75, Batch number: 0, Loss: -275.0845642089844\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11524/20000 (57.62%)\n",
      "\n",
      "Epoch: 76, Batch number: 0, Loss: -243.609130859375\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11449/20000 (57.24%)\n",
      "\n",
      "Epoch: 77, Batch number: 0, Loss: -180.26486206054688\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11513/20000 (57.56%)\n",
      "\n",
      "Epoch: 78, Batch number: 0, Loss: -406.70794677734375\n",
      "Accuracy on validation dataset: 2873/5000 (57.46%)\n",
      "Accuracy on training dataset: 11378/20000 (56.89%)\n",
      "\n",
      "Epoch: 79, Batch number: 0, Loss: -167.93251037597656\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11472/20000 (57.36%)\n",
      "\n",
      "Epoch: 80, Batch number: 0, Loss: -211.50076293945312\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11507/20000 (57.53%)\n",
      "\n",
      "Epoch: 81, Batch number: 0, Loss: -274.96630859375\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11497/20000 (57.48%)\n",
      "\n",
      "Epoch: 82, Batch number: 0, Loss: -282.9083251953125\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11473/20000 (57.37%)\n",
      "\n",
      "Epoch: 83, Batch number: 0, Loss: -348.6310729980469\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11479/20000 (57.40%)\n",
      "\n",
      "Epoch: 84, Batch number: 0, Loss: -423.6070556640625\n",
      "Accuracy on validation dataset: 2877/5000 (57.54%)\n",
      "Accuracy on training dataset: 11462/20000 (57.31%)\n",
      "\n",
      "Epoch: 85, Batch number: 0, Loss: -244.50057983398438\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11473/20000 (57.37%)\n",
      "\n",
      "Epoch: 86, Batch number: 0, Loss: -227.03387451171875\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11454/20000 (57.27%)\n",
      "\n",
      "Epoch: 87, Batch number: 0, Loss: -346.5586853027344\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11418/20000 (57.09%)\n",
      "\n",
      "Epoch: 88, Batch number: 0, Loss: -459.62860107421875\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11462/20000 (57.31%)\n",
      "\n",
      "Epoch: 89, Batch number: 0, Loss: -426.8961181640625\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11485/20000 (57.42%)\n",
      "\n",
      "Epoch: 90, Batch number: 0, Loss: -477.2727355957031\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11449/20000 (57.24%)\n",
      "\n",
      "Epoch: 91, Batch number: 0, Loss: -363.71905517578125\n",
      "Accuracy on validation dataset: 2877/5000 (57.54%)\n",
      "Accuracy on training dataset: 11502/20000 (57.51%)\n",
      "\n",
      "Epoch: 92, Batch number: 0, Loss: -412.54632568359375\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11473/20000 (57.37%)\n",
      "\n",
      "Epoch: 93, Batch number: 0, Loss: -300.1685791015625\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11470/20000 (57.35%)\n",
      "\n",
      "Epoch: 94, Batch number: 0, Loss: -479.2504577636719\n",
      "Accuracy on validation dataset: 2875/5000 (57.50%)\n",
      "Accuracy on training dataset: 11399/20000 (56.99%)\n",
      "\n",
      "Epoch: 95, Batch number: 0, Loss: -403.5462646484375\n",
      "Accuracy on validation dataset: 2874/5000 (57.48%)\n",
      "Accuracy on training dataset: 11404/20000 (57.02%)\n",
      "\n",
      "Epoch: 96, Batch number: 0, Loss: -560.9255981445312\n",
      "Accuracy on validation dataset: 2877/5000 (57.54%)\n",
      "Accuracy on training dataset: 11513/20000 (57.56%)\n",
      "\n",
      "Epoch: 97, Batch number: 0, Loss: -500.5032043457031\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11425/20000 (57.12%)\n",
      "\n",
      "Epoch: 98, Batch number: 0, Loss: -502.3585510253906\n",
      "Accuracy on validation dataset: 2877/5000 (57.54%)\n",
      "Accuracy on training dataset: 11489/20000 (57.45%)\n",
      "\n",
      "Epoch: 99, Batch number: 0, Loss: -553.1286010742188\n",
      "Accuracy on validation dataset: 2876/5000 (57.52%)\n",
      "Accuracy on training dataset: 11420/20000 (57.10%)\n",
      "\n",
      "Epoch: 100, Batch number: 0, Loss: -621.3609619140625\n",
      "Accuracy on validation dataset: 2877/5000 (57.54%)\n",
      "Accuracy on training dataset: 11516/20000 (57.58%)\n",
      "\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entrenamiento:\n",
    "loss_fn = 'BCE'\n",
    "optim_algorithm = 'Adam'\n",
    "epochs = 100\n",
    "sample_loss_every = 20\n",
    "check_on_train = True\n",
    "learning_rate = 1e-4\n",
    "\n",
    "TwoLayerNetTrainer.train(loss_fn,optim_algorithm,epochs,sample_loss_every,check_on_train,lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un layer intermedio no se puede hacer porque son muchos parámetros. Vamos a reducir el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos y separamos en train / dev / test:\n",
    "df_train, df_dev, df_test, vocab = read_and_split_dataset()\n",
    "idx_to_tk = {idx: tk for idx, tk in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# Nos quedamos con las palabras más frecuentes:\n",
    "\n",
    "import nltk\n",
    "df = df_train\n",
    "df = df.copy()\n",
    "df['comment'] = df['comment'].str.lower().apply(nltk.tokenize.word_tokenize,args=('english', False))\n",
    "\n",
    "tk_to_freq = {tk: 0 for tk in vocab}\n",
    "for comment in df['comment']:\n",
    "    for tk in comment:\n",
    "        try:\n",
    "            tk_to_freq[tk] += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "freqs = np.array(list(tk_to_freq.values()))\n",
    "arg_freqs = np.argsort(freqs)[::-1]\n",
    "vocab = [vocab[i] for i in arg_freqs[:10000]]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos:\n",
    "samples = {}\n",
    "unk_tk = '<UNK>'\n",
    "unk_idx = vocab_size\n",
    "for data, df in zip(['train', 'dev'],[df_train, df_dev]):\n",
    "    samples[data] = tokenize_dataframe(df, vocab, unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class IMDbBOWDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, samples, vocab_size):\n",
    "        \n",
    "        num_samples = len(samples)\n",
    "        self.x = torch.zeros(num_samples,vocab_size, dtype=torch.float)\n",
    "        for i, sample in enumerate(samples):\n",
    "            for j in sample[0]:\n",
    "                if j!= vocab_size:\n",
    "                    self.x[i,j] += 1.\n",
    "            \n",
    "        self.y = torch.tensor([sample[1] for sample in samples], dtype=torch.float).view(-1,1)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "padding_idx = vocab_size\n",
    "train_dataset = IMDbBOWDataset(samples['train'],padding_idx)\n",
    "dev_dataset = IMDbBOWDataset(samples['dev'], padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers import *\n",
    "\n",
    "classifier = LogisticRegressionClassifier(vocab_size,bias=True,device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 finished. Approximate loss: 0.0974\n",
      "Epoch 2 finished. Approximate loss: -0.6364\n",
      "Epoch 3 finished. Approximate loss: -1.0847\n",
      "Epoch 4 finished. Approximate loss: -1.8165\n",
      "Epoch 5 finished. Approximate loss: -1.9596\n",
      "Epoch 6 finished. Approximate loss: -2.7733\n",
      "Epoch 7 finished. Approximate loss: -3.0011\n",
      "Epoch 8 finished. Approximate loss: -3.7192\n",
      "Epoch 9 finished. Approximate loss: -4.5293\n",
      "Epoch 10 finished. Approximate loss: -4.9631\n",
      "Epoch 11 finished. Approximate loss: -5.3402\n",
      "Epoch 12 finished. Approximate loss: -5.4280\n",
      "Epoch 13 finished. Approximate loss: -6.4952\n",
      "Epoch 14 finished. Approximate loss: -7.9116\n",
      "Epoch 15 finished. Approximate loss: -8.2333\n",
      "Epoch 16 finished. Approximate loss: -8.5309\n",
      "Epoch 17 finished. Approximate loss: -8.5437\n",
      "Epoch 18 finished. Approximate loss: -9.4093\n",
      "Epoch 19 finished. Approximate loss: -9.2783\n",
      "Epoch 20 finished. Approximate loss: -10.1181\n",
      "Epoch 21 finished. Approximate loss: -10.8163\n",
      "Epoch 22 finished. Approximate loss: -11.3700\n",
      "Epoch 23 finished. Approximate loss: -12.4537\n",
      "Epoch 24 finished. Approximate loss: -13.4629\n",
      "Epoch 25 finished. Approximate loss: -13.6318\n",
      "Epoch 26 finished. Approximate loss: -13.8970\n",
      "Epoch 27 finished. Approximate loss: -14.7469\n",
      "Epoch 28 finished. Approximate loss: -15.0175\n",
      "Epoch 29 finished. Approximate loss: -16.8547\n",
      "Epoch 30 finished. Approximate loss: -16.8712\n",
      "Epoch 31 finished. Approximate loss: -19.1464\n",
      "Epoch 32 finished. Approximate loss: -15.4972\n",
      "Epoch 33 finished. Approximate loss: -17.1651\n",
      "Epoch 34 finished. Approximate loss: -18.1523\n",
      "Epoch 35 finished. Approximate loss: -18.1457\n",
      "Epoch 36 finished. Approximate loss: -18.1974\n",
      "Epoch 37 finished. Approximate loss: -21.7954\n",
      "Epoch 38 finished. Approximate loss: -22.6501\n",
      "Epoch 39 finished. Approximate loss: -22.8212\n",
      "Epoch 40 finished. Approximate loss: -24.0930\n",
      "Epoch 41 finished. Approximate loss: -21.0556\n",
      "Epoch 42 finished. Approximate loss: -22.9006\n",
      "Epoch 43 finished. Approximate loss: -24.2518\n",
      "Epoch 44 finished. Approximate loss: -28.2546\n",
      "Epoch 45 finished. Approximate loss: -27.6601\n",
      "Epoch 46 finished. Approximate loss: -24.6147\n",
      "Epoch 47 finished. Approximate loss: -26.0437\n",
      "Epoch 48 finished. Approximate loss: -24.5098\n",
      "Epoch 49 finished. Approximate loss: -29.4021\n",
      "Epoch 50 finished. Approximate loss: -23.0195\n",
      "Epoch 51 finished. Approximate loss: -29.6544\n",
      "Epoch 52 finished. Approximate loss: -27.5574\n",
      "Epoch 53 finished. Approximate loss: -31.9852\n",
      "Epoch 54 finished. Approximate loss: -31.2133\n",
      "Epoch 55 finished. Approximate loss: -32.8723\n",
      "Epoch 56 finished. Approximate loss: -31.5037\n",
      "Epoch 57 finished. Approximate loss: -32.7183\n",
      "Epoch 58 finished. Approximate loss: -32.4521\n",
      "Epoch 59 finished. Approximate loss: -36.7136\n",
      "Epoch 60 finished. Approximate loss: -34.3812\n",
      "Epoch 61 finished. Approximate loss: -32.3794\n",
      "Epoch 62 finished. Approximate loss: -36.5369\n",
      "Epoch 63 finished. Approximate loss: -36.9192\n",
      "Epoch 64 finished. Approximate loss: -34.6302\n",
      "Epoch 65 finished. Approximate loss: -36.8588\n",
      "Epoch 66 finished. Approximate loss: -37.4000\n",
      "Epoch 67 finished. Approximate loss: -40.9657\n",
      "Epoch 68 finished. Approximate loss: -36.7609\n",
      "Epoch 69 finished. Approximate loss: -38.7734\n",
      "Epoch 70 finished. Approximate loss: -41.2707\n",
      "Epoch 71 finished. Approximate loss: -39.3265\n",
      "Epoch 72 finished. Approximate loss: -42.6955\n",
      "Epoch 73 finished. Approximate loss: -41.6934\n",
      "Epoch 74 finished. Approximate loss: -46.4611\n",
      "Epoch 75 finished. Approximate loss: -42.4041\n",
      "Epoch 76 finished. Approximate loss: -39.0199\n",
      "Epoch 77 finished. Approximate loss: -40.0923\n",
      "Epoch 78 finished. Approximate loss: -46.1851\n",
      "Epoch 79 finished. Approximate loss: -41.1630\n",
      "Epoch 80 finished. Approximate loss: -50.1846\n",
      "Epoch 81 finished. Approximate loss: -44.4239\n",
      "Epoch 82 finished. Approximate loss: -53.7497\n",
      "Epoch 83 finished. Approximate loss: -46.8997\n",
      "Epoch 84 finished. Approximate loss: -46.6804\n",
      "Epoch 85 finished. Approximate loss: -52.0240\n",
      "Epoch 86 finished. Approximate loss: -53.0181\n",
      "Epoch 87 finished. Approximate loss: -50.6559\n",
      "Epoch 88 finished. Approximate loss: -49.9237\n",
      "Epoch 89 finished. Approximate loss: -47.5510\n",
      "Epoch 90 finished. Approximate loss: -47.0969\n",
      "Epoch 91 finished. Approximate loss: -50.0407\n",
      "Epoch 92 finished. Approximate loss: -55.6678\n",
      "Epoch 93 finished. Approximate loss: -51.6000\n",
      "Epoch 94 finished. Approximate loss: -58.0074\n",
      "Epoch 95 finished. Approximate loss: -53.9351\n",
      "Epoch 96 finished. Approximate loss: -51.5227\n",
      "Epoch 97 finished. Approximate loss: -51.1316\n",
      "Epoch 98 finished. Approximate loss: -60.3307\n",
      "Epoch 99 finished. Approximate loss: -55.6237\n",
      "Epoch 100 finished. Approximate loss: -55.3112\n",
      "Epoch 101 finished. Approximate loss: -54.4264\n",
      "Epoch 102 finished. Approximate loss: -62.9488\n",
      "Epoch 103 finished. Approximate loss: -55.3707\n",
      "Epoch 104 finished. Approximate loss: -61.0719\n",
      "Epoch 105 finished. Approximate loss: -62.9806\n",
      "Epoch 106 finished. Approximate loss: -66.0511\n",
      "Epoch 107 finished. Approximate loss: -60.4062\n",
      "Epoch 108 finished. Approximate loss: -67.5740\n",
      "Epoch 109 finished. Approximate loss: -67.7976\n",
      "Epoch 110 finished. Approximate loss: -64.4786\n",
      "Epoch 111 finished. Approximate loss: -67.9305\n",
      "Epoch 112 finished. Approximate loss: -57.5929\n",
      "Epoch 113 finished. Approximate loss: -65.8672\n",
      "Epoch 114 finished. Approximate loss: -66.0360\n",
      "Epoch 115 finished. Approximate loss: -68.4926\n",
      "Epoch 116 finished. Approximate loss: -61.6981\n",
      "Epoch 117 finished. Approximate loss: -66.6943\n",
      "Epoch 118 finished. Approximate loss: -61.6163\n",
      "Epoch 119 finished. Approximate loss: -75.2125\n",
      "Epoch 120 finished. Approximate loss: -75.5677\n",
      "Epoch 121 finished. Approximate loss: -79.1512\n",
      "Epoch 122 finished. Approximate loss: -72.9282\n",
      "Epoch 123 finished. Approximate loss: -72.5291\n",
      "Epoch 124 finished. Approximate loss: -81.2817\n",
      "Epoch 125 finished. Approximate loss: -73.7921\n",
      "Epoch 126 finished. Approximate loss: -80.5088\n",
      "Epoch 127 finished. Approximate loss: -68.9358\n",
      "Epoch 128 finished. Approximate loss: -86.7494\n",
      "Epoch 129 finished. Approximate loss: -86.3226\n",
      "Epoch 130 finished. Approximate loss: -80.9440\n",
      "Epoch 131 finished. Approximate loss: -90.9767\n",
      "Epoch 132 finished. Approximate loss: -72.1436\n",
      "Epoch 133 finished. Approximate loss: -84.0071\n",
      "Epoch 134 finished. Approximate loss: -85.1799\n",
      "Epoch 135 finished. Approximate loss: -81.4725\n",
      "Epoch 136 finished. Approximate loss: -67.9983\n",
      "Epoch 137 finished. Approximate loss: -76.4475\n",
      "Epoch 138 finished. Approximate loss: -71.7748\n",
      "Epoch 139 finished. Approximate loss: -78.2426\n",
      "Epoch 140 finished. Approximate loss: -84.1459\n",
      "Epoch 141 finished. Approximate loss: -85.5472\n",
      "Epoch 142 finished. Approximate loss: -90.5353\n",
      "Epoch 143 finished. Approximate loss: -79.2710\n",
      "Epoch 144 finished. Approximate loss: -80.3247\n",
      "Epoch 145 finished. Approximate loss: -73.6469\n",
      "Epoch 146 finished. Approximate loss: -92.5601\n",
      "Epoch 147 finished. Approximate loss: -86.4950\n",
      "Epoch 148 finished. Approximate loss: -88.3913\n",
      "Epoch 149 finished. Approximate loss: -90.4485\n",
      "Epoch 150 finished. Approximate loss: -90.2423\n",
      "Epoch 151 finished. Approximate loss: -85.7012\n",
      "Epoch 152 finished. Approximate loss: -92.0366\n",
      "Epoch 153 finished. Approximate loss: -89.3939\n",
      "Epoch 154 finished. Approximate loss: -95.8976\n",
      "Epoch 155 finished. Approximate loss: -93.0848\n",
      "Epoch 156 finished. Approximate loss: -97.2297\n",
      "Epoch 157 finished. Approximate loss: -79.7672\n",
      "Epoch 158 finished. Approximate loss: -98.2489\n",
      "Epoch 159 finished. Approximate loss: -98.6837\n",
      "Epoch 160 finished. Approximate loss: -103.2361\n",
      "Epoch 161 finished. Approximate loss: -92.2873\n",
      "Epoch 162 finished. Approximate loss: -96.2250\n",
      "Epoch 163 finished. Approximate loss: -77.8928\n",
      "Epoch 164 finished. Approximate loss: -98.8015\n",
      "Epoch 165 finished. Approximate loss: -95.1941\n",
      "Epoch 166 finished. Approximate loss: -105.4083\n",
      "Epoch 167 finished. Approximate loss: -108.6156\n",
      "Epoch 168 finished. Approximate loss: -95.6198\n",
      "Epoch 169 finished. Approximate loss: -98.8073\n",
      "Epoch 170 finished. Approximate loss: -110.4726\n",
      "Epoch 171 finished. Approximate loss: -97.8910\n",
      "Epoch 172 finished. Approximate loss: -101.3646\n",
      "Epoch 173 finished. Approximate loss: -85.6032\n",
      "Epoch 174 finished. Approximate loss: -107.4438\n",
      "Epoch 175 finished. Approximate loss: -99.2755\n",
      "Epoch 176 finished. Approximate loss: -95.9630\n",
      "Epoch 177 finished. Approximate loss: -107.1371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178 finished. Approximate loss: -105.1836\n",
      "Epoch 179 finished. Approximate loss: -114.4433\n",
      "Epoch 180 finished. Approximate loss: -100.4671\n",
      "Epoch 181 finished. Approximate loss: -103.6795\n",
      "Epoch 182 finished. Approximate loss: -105.2397\n",
      "Epoch 183 finished. Approximate loss: -116.8301\n",
      "Epoch 184 finished. Approximate loss: -108.0475\n",
      "Epoch 185 finished. Approximate loss: -100.0250\n",
      "Epoch 186 finished. Approximate loss: -119.1226\n",
      "Epoch 187 finished. Approximate loss: -119.4275\n",
      "Epoch 188 finished. Approximate loss: -112.6976\n",
      "Epoch 189 finished. Approximate loss: -121.9364\n",
      "Epoch 190 finished. Approximate loss: -119.0801\n",
      "Epoch 191 finished. Approximate loss: -117.9372\n",
      "Epoch 192 finished. Approximate loss: -113.4332\n",
      "Epoch 193 finished. Approximate loss: -111.1450\n",
      "Epoch 194 finished. Approximate loss: -118.8548\n",
      "Epoch 195 finished. Approximate loss: -116.2715\n",
      "Epoch 196 finished. Approximate loss: -115.3569\n",
      "Epoch 197 finished. Approximate loss: -117.2007\n",
      "Epoch 198 finished. Approximate loss: -105.8801\n",
      "Epoch 199 finished. Approximate loss: -118.4597\n",
      "Epoch 200 finished. Approximate loss: -125.3444\n",
      "Epoch 201 finished. Approximate loss: -125.0137\n",
      "Epoch 202 finished. Approximate loss: -127.6043\n",
      "Epoch 203 finished. Approximate loss: -125.8110\n",
      "Epoch 204 finished. Approximate loss: -116.9306\n",
      "Epoch 205 finished. Approximate loss: -120.5071\n",
      "Epoch 206 finished. Approximate loss: -121.9961\n",
      "Epoch 207 finished. Approximate loss: -116.8875\n",
      "Epoch 208 finished. Approximate loss: -141.4072\n",
      "Epoch 209 finished. Approximate loss: -111.6097\n",
      "Epoch 210 finished. Approximate loss: -127.7187\n",
      "Epoch 211 finished. Approximate loss: -123.1142\n",
      "Epoch 212 finished. Approximate loss: -118.9176\n",
      "Epoch 213 finished. Approximate loss: -132.5784\n",
      "Epoch 214 finished. Approximate loss: -139.7701\n",
      "Epoch 215 finished. Approximate loss: -126.3858\n",
      "Epoch 216 finished. Approximate loss: -121.3318\n",
      "Epoch 217 finished. Approximate loss: -121.1898\n",
      "Epoch 218 finished. Approximate loss: -117.8690\n",
      "Epoch 219 finished. Approximate loss: -134.6830\n",
      "Epoch 220 finished. Approximate loss: -125.5730\n",
      "Epoch 221 finished. Approximate loss: -128.7540\n",
      "Epoch 222 finished. Approximate loss: -115.6370\n",
      "Epoch 223 finished. Approximate loss: -125.7730\n",
      "Epoch 224 finished. Approximate loss: -133.4535\n",
      "Epoch 225 finished. Approximate loss: -130.2768\n",
      "Epoch 226 finished. Approximate loss: -121.6475\n",
      "Epoch 227 finished. Approximate loss: -134.0583\n",
      "Epoch 228 finished. Approximate loss: -122.4021\n",
      "Epoch 229 finished. Approximate loss: -129.0855\n",
      "Epoch 230 finished. Approximate loss: -130.4198\n",
      "Epoch 231 finished. Approximate loss: -146.4885\n",
      "Epoch 232 finished. Approximate loss: -140.2075\n",
      "Epoch 233 finished. Approximate loss: -134.7400\n",
      "Epoch 234 finished. Approximate loss: -140.8002\n",
      "Epoch 235 finished. Approximate loss: -145.0178\n",
      "Epoch 236 finished. Approximate loss: -129.4753\n",
      "Epoch 237 finished. Approximate loss: -149.1050\n",
      "Epoch 238 finished. Approximate loss: -147.5420\n",
      "Epoch 239 finished. Approximate loss: -141.6069\n",
      "Epoch 240 finished. Approximate loss: -131.3659\n",
      "Epoch 241 finished. Approximate loss: -146.9456\n",
      "Epoch 242 finished. Approximate loss: -149.5938\n",
      "Epoch 243 finished. Approximate loss: -142.8972\n",
      "Epoch 244 finished. Approximate loss: -134.2237\n",
      "Epoch 245 finished. Approximate loss: -160.5410\n",
      "Epoch 246 finished. Approximate loss: -157.6607\n",
      "Epoch 247 finished. Approximate loss: -144.4895\n",
      "Epoch 248 finished. Approximate loss: -148.6945\n",
      "Epoch 249 finished. Approximate loss: -132.6848\n",
      "Epoch 250 finished. Approximate loss: -145.6384\n",
      "Epoch 251 finished. Approximate loss: -149.3176\n",
      "Epoch 252 finished. Approximate loss: -181.2283\n",
      "Epoch 253 finished. Approximate loss: -146.5327\n",
      "Epoch 254 finished. Approximate loss: -165.2198\n",
      "Epoch 255 finished. Approximate loss: -142.5764\n",
      "Epoch 256 finished. Approximate loss: -152.3843\n",
      "Epoch 257 finished. Approximate loss: -164.6333\n",
      "Epoch 258 finished. Approximate loss: -168.2046\n",
      "Epoch 259 finished. Approximate loss: -144.4395\n",
      "Epoch 260 finished. Approximate loss: -181.0090\n",
      "Epoch 261 finished. Approximate loss: -141.6642\n",
      "Epoch 262 finished. Approximate loss: -174.0704\n",
      "Epoch 263 finished. Approximate loss: -153.5000\n",
      "Epoch 264 finished. Approximate loss: -147.8049\n",
      "Epoch 265 finished. Approximate loss: -162.2863\n",
      "Epoch 266 finished. Approximate loss: -150.2048\n",
      "Epoch 267 finished. Approximate loss: -170.5520\n",
      "Epoch 268 finished. Approximate loss: -171.6377\n",
      "Epoch 269 finished. Approximate loss: -164.5420\n",
      "Epoch 270 finished. Approximate loss: -156.7185\n",
      "Epoch 271 finished. Approximate loss: -169.8266\n",
      "Epoch 272 finished. Approximate loss: -168.1908\n",
      "Epoch 273 finished. Approximate loss: -156.5189\n",
      "Epoch 274 finished. Approximate loss: -185.9949\n",
      "Epoch 275 finished. Approximate loss: -148.1343\n",
      "Epoch 276 finished. Approximate loss: -194.2133\n",
      "Epoch 277 finished. Approximate loss: -162.2652\n",
      "Epoch 278 finished. Approximate loss: -153.4972\n",
      "Epoch 279 finished. Approximate loss: -168.7037\n",
      "Epoch 280 finished. Approximate loss: -165.2833\n",
      "Epoch 281 finished. Approximate loss: -173.0531\n",
      "Epoch 282 finished. Approximate loss: -169.8728\n",
      "Epoch 283 finished. Approximate loss: -195.4434\n",
      "Epoch 284 finished. Approximate loss: -158.6274\n",
      "Epoch 285 finished. Approximate loss: -194.9369\n",
      "Epoch 286 finished. Approximate loss: -156.2974\n",
      "Epoch 287 finished. Approximate loss: -177.3732\n",
      "Epoch 288 finished. Approximate loss: -174.3072\n",
      "Epoch 289 finished. Approximate loss: -166.0493\n",
      "Epoch 290 finished. Approximate loss: -184.1203\n",
      "Epoch 291 finished. Approximate loss: -164.6582\n",
      "Epoch 292 finished. Approximate loss: -190.5524\n",
      "Epoch 293 finished. Approximate loss: -179.7080\n",
      "Epoch 294 finished. Approximate loss: -169.1261\n",
      "Epoch 295 finished. Approximate loss: -161.1872\n",
      "Epoch 296 finished. Approximate loss: -179.6062\n",
      "Epoch 297 finished. Approximate loss: -198.9419\n",
      "Epoch 298 finished. Approximate loss: -185.5552\n",
      "Epoch 299 finished. Approximate loss: -162.2069\n",
      "Epoch 300 finished. Approximate loss: -192.2398\n",
      "Epoch 301 finished. Approximate loss: -184.2240\n",
      "Epoch 302 finished. Approximate loss: -174.9998\n",
      "Epoch 303 finished. Approximate loss: -179.0756\n",
      "Epoch 304 finished. Approximate loss: -184.6651\n",
      "Epoch 305 finished. Approximate loss: -186.4550\n",
      "Epoch 306 finished. Approximate loss: -187.6043\n",
      "Epoch 307 finished. Approximate loss: -168.6499\n",
      "Epoch 308 finished. Approximate loss: -187.1606\n",
      "Epoch 309 finished. Approximate loss: -178.6600\n",
      "Epoch 310 finished. Approximate loss: -236.4052\n",
      "Epoch 311 finished. Approximate loss: -184.9680\n",
      "Epoch 312 finished. Approximate loss: -191.4058\n",
      "Epoch 313 finished. Approximate loss: -202.2132\n",
      "Epoch 314 finished. Approximate loss: -198.5653\n",
      "Epoch 315 finished. Approximate loss: -192.2290\n",
      "Epoch 316 finished. Approximate loss: -197.1161\n",
      "Epoch 317 finished. Approximate loss: -209.0980\n",
      "Epoch 318 finished. Approximate loss: -194.2063\n",
      "Epoch 319 finished. Approximate loss: -202.8090\n",
      "Epoch 320 finished. Approximate loss: -203.3622\n",
      "Epoch 321 finished. Approximate loss: -201.2183\n",
      "Epoch 322 finished. Approximate loss: -183.8581\n",
      "Epoch 323 finished. Approximate loss: -207.8594\n",
      "Epoch 324 finished. Approximate loss: -202.6351\n",
      "Epoch 325 finished. Approximate loss: -230.8447\n",
      "Epoch 326 finished. Approximate loss: -190.9815\n",
      "Epoch 327 finished. Approximate loss: -188.8487\n",
      "Epoch 328 finished. Approximate loss: -205.8268\n",
      "Epoch 329 finished. Approximate loss: -178.6593\n",
      "Epoch 330 finished. Approximate loss: -188.3348\n",
      "Epoch 331 finished. Approximate loss: -190.7467\n",
      "Epoch 332 finished. Approximate loss: -211.4810\n",
      "Epoch 333 finished. Approximate loss: -197.3421\n",
      "Epoch 334 finished. Approximate loss: -221.5401\n",
      "Epoch 335 finished. Approximate loss: -189.1178\n",
      "Epoch 336 finished. Approximate loss: -212.3559\n",
      "Epoch 337 finished. Approximate loss: -209.3080\n",
      "Epoch 338 finished. Approximate loss: -216.3362\n",
      "Epoch 339 finished. Approximate loss: -242.9207\n",
      "Epoch 340 finished. Approximate loss: -203.5194\n",
      "Epoch 341 finished. Approximate loss: -229.8202\n",
      "Epoch 342 finished. Approximate loss: -218.2160\n",
      "Epoch 343 finished. Approximate loss: -213.0123\n",
      "Epoch 344 finished. Approximate loss: -193.2704\n",
      "Epoch 345 finished. Approximate loss: -228.7949\n",
      "Epoch 346 finished. Approximate loss: -210.5366\n",
      "Epoch 347 finished. Approximate loss: -214.9119\n",
      "Epoch 348 finished. Approximate loss: -235.6752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 finished. Approximate loss: -175.6995\n",
      "Epoch 350 finished. Approximate loss: -212.4259\n",
      "Epoch 351 finished. Approximate loss: -199.5140\n",
      "Epoch 352 finished. Approximate loss: -207.6127\n",
      "Epoch 353 finished. Approximate loss: -213.5981\n",
      "Epoch 354 finished. Approximate loss: -230.8863\n",
      "Epoch 355 finished. Approximate loss: -216.0765\n",
      "Epoch 356 finished. Approximate loss: -232.8093\n",
      "Epoch 357 finished. Approximate loss: -210.8007\n",
      "Epoch 358 finished. Approximate loss: -213.3401\n",
      "Epoch 359 finished. Approximate loss: -231.3944\n",
      "Epoch 360 finished. Approximate loss: -196.0487\n",
      "Epoch 361 finished. Approximate loss: -210.5972\n",
      "Epoch 362 finished. Approximate loss: -233.0009\n",
      "Epoch 363 finished. Approximate loss: -257.1661\n",
      "Epoch 364 finished. Approximate loss: -191.1000\n",
      "Epoch 365 finished. Approximate loss: -206.1449\n",
      "Epoch 366 finished. Approximate loss: -201.2202\n",
      "Epoch 367 finished. Approximate loss: -227.2124\n",
      "Epoch 368 finished. Approximate loss: -212.2109\n",
      "Epoch 369 finished. Approximate loss: -215.9147\n",
      "Epoch 370 finished. Approximate loss: -220.8943\n",
      "Epoch 371 finished. Approximate loss: -215.0439\n",
      "Epoch 372 finished. Approximate loss: -227.2748\n",
      "Epoch 373 finished. Approximate loss: -186.4447\n",
      "Epoch 374 finished. Approximate loss: -217.6575\n",
      "Epoch 375 finished. Approximate loss: -238.9274\n",
      "Epoch 376 finished. Approximate loss: -220.6761\n",
      "Epoch 377 finished. Approximate loss: -209.2476\n",
      "Epoch 378 finished. Approximate loss: -214.0423\n",
      "Epoch 379 finished. Approximate loss: -215.6608\n",
      "Epoch 380 finished. Approximate loss: -222.8514\n",
      "Epoch 381 finished. Approximate loss: -250.5063\n",
      "Epoch 382 finished. Approximate loss: -242.1468\n",
      "Epoch 383 finished. Approximate loss: -255.1700\n",
      "Epoch 384 finished. Approximate loss: -247.3131\n",
      "Epoch 385 finished. Approximate loss: -239.2740\n",
      "Epoch 386 finished. Approximate loss: -229.9674\n",
      "Epoch 387 finished. Approximate loss: -231.5847\n",
      "Epoch 388 finished. Approximate loss: -225.7390\n",
      "Epoch 389 finished. Approximate loss: -234.6164\n",
      "Epoch 390 finished. Approximate loss: -242.5756\n",
      "Epoch 391 finished. Approximate loss: -214.8107\n",
      "Epoch 392 finished. Approximate loss: -226.8665\n",
      "Epoch 393 finished. Approximate loss: -211.9639\n",
      "Epoch 394 finished. Approximate loss: -236.7012\n",
      "Epoch 395 finished. Approximate loss: -234.2980\n",
      "Epoch 396 finished. Approximate loss: -260.4228\n",
      "Epoch 397 finished. Approximate loss: -273.4694\n",
      "Epoch 398 finished. Approximate loss: -246.0451\n",
      "Epoch 399 finished. Approximate loss: -275.5319\n",
      "Epoch 400 finished. Approximate loss: -219.1449\n",
      "Epoch 401 finished. Approximate loss: -249.1502\n",
      "Epoch 402 finished. Approximate loss: -284.2851\n",
      "Epoch 403 finished. Approximate loss: -243.9183\n",
      "Epoch 404 finished. Approximate loss: -283.1374\n",
      "Epoch 405 finished. Approximate loss: -252.6042\n",
      "Epoch 406 finished. Approximate loss: -230.0471\n",
      "Epoch 407 finished. Approximate loss: -262.1642\n",
      "Epoch 408 finished. Approximate loss: -300.1053\n",
      "Epoch 409 finished. Approximate loss: -252.0021\n",
      "Epoch 410 finished. Approximate loss: -250.9744\n",
      "Epoch 411 finished. Approximate loss: -262.7314\n",
      "Epoch 412 finished. Approximate loss: -265.3513\n",
      "Epoch 413 finished. Approximate loss: -254.3995\n",
      "Epoch 414 finished. Approximate loss: -219.3276\n",
      "Epoch 415 finished. Approximate loss: -262.9403\n",
      "Epoch 416 finished. Approximate loss: -257.8292\n",
      "Epoch 417 finished. Approximate loss: -251.5281\n",
      "Epoch 418 finished. Approximate loss: -237.2371\n",
      "Epoch 419 finished. Approximate loss: -271.4585\n",
      "Epoch 420 finished. Approximate loss: -246.0623\n",
      "Epoch 421 finished. Approximate loss: -292.3038\n",
      "Epoch 422 finished. Approximate loss: -229.7491\n",
      "Epoch 423 finished. Approximate loss: -266.9085\n",
      "Epoch 424 finished. Approximate loss: -264.5287\n",
      "Epoch 425 finished. Approximate loss: -274.3184\n",
      "Epoch 426 finished. Approximate loss: -250.4986\n",
      "Epoch 427 finished. Approximate loss: -289.4617\n",
      "Epoch 428 finished. Approximate loss: -222.3766\n",
      "Epoch 429 finished. Approximate loss: -280.7374\n",
      "Epoch 430 finished. Approximate loss: -260.6719\n",
      "Epoch 431 finished. Approximate loss: -240.4002\n",
      "Epoch 432 finished. Approximate loss: -277.7905\n",
      "Epoch 433 finished. Approximate loss: -243.5654\n",
      "Epoch 434 finished. Approximate loss: -248.3364\n",
      "Epoch 435 finished. Approximate loss: -276.7786\n",
      "Epoch 436 finished. Approximate loss: -270.4085\n",
      "Epoch 437 finished. Approximate loss: -273.8341\n",
      "Epoch 438 finished. Approximate loss: -248.5755\n",
      "Epoch 439 finished. Approximate loss: -261.8603\n",
      "Epoch 440 finished. Approximate loss: -264.5178\n",
      "Epoch 441 finished. Approximate loss: -260.6003\n",
      "Epoch 442 finished. Approximate loss: -264.5739\n",
      "Epoch 443 finished. Approximate loss: -292.2187\n",
      "Epoch 444 finished. Approximate loss: -274.0254\n",
      "Epoch 445 finished. Approximate loss: -276.5798\n",
      "Epoch 446 finished. Approximate loss: -318.1868\n",
      "Epoch 447 finished. Approximate loss: -260.8054\n",
      "Epoch 448 finished. Approximate loss: -258.1695\n",
      "Epoch 449 finished. Approximate loss: -259.2703\n",
      "Epoch 450 finished. Approximate loss: -304.9891\n",
      "Epoch 451 finished. Approximate loss: -259.1099\n",
      "Epoch 452 finished. Approximate loss: -258.3382\n",
      "Epoch 453 finished. Approximate loss: -250.0547\n",
      "Epoch 454 finished. Approximate loss: -286.8667\n",
      "Epoch 455 finished. Approximate loss: -316.2481\n",
      "Epoch 456 finished. Approximate loss: -262.2289\n",
      "Epoch 457 finished. Approximate loss: -308.5569\n",
      "Epoch 458 finished. Approximate loss: -274.4883\n",
      "Epoch 459 finished. Approximate loss: -263.7908\n",
      "Epoch 460 finished. Approximate loss: -271.3256\n",
      "Epoch 461 finished. Approximate loss: -296.2422\n",
      "Epoch 462 finished. Approximate loss: -286.0321\n",
      "Epoch 463 finished. Approximate loss: -307.3990\n",
      "Epoch 464 finished. Approximate loss: -302.1202\n",
      "Epoch 465 finished. Approximate loss: -287.0224\n",
      "Epoch 466 finished. Approximate loss: -267.9544\n",
      "Epoch 467 finished. Approximate loss: -291.4908\n",
      "Epoch 468 finished. Approximate loss: -275.8121\n",
      "Epoch 469 finished. Approximate loss: -302.7551\n",
      "Epoch 470 finished. Approximate loss: -277.1320\n",
      "Epoch 471 finished. Approximate loss: -308.5639\n",
      "Epoch 472 finished. Approximate loss: -308.8537\n",
      "Epoch 473 finished. Approximate loss: -333.2638\n",
      "Epoch 474 finished. Approximate loss: -287.9367\n",
      "Epoch 475 finished. Approximate loss: -306.9177\n",
      "Epoch 476 finished. Approximate loss: -315.1485\n",
      "Epoch 477 finished. Approximate loss: -305.3784\n",
      "Epoch 478 finished. Approximate loss: -275.7159\n",
      "Epoch 479 finished. Approximate loss: -301.7270\n",
      "Epoch 480 finished. Approximate loss: -273.4982\n",
      "Epoch 481 finished. Approximate loss: -294.1025\n",
      "Epoch 482 finished. Approximate loss: -249.5724\n",
      "Epoch 483 finished. Approximate loss: -316.2266\n",
      "Epoch 484 finished. Approximate loss: -288.5340\n",
      "Epoch 485 finished. Approximate loss: -260.3435\n",
      "Epoch 486 finished. Approximate loss: -291.6251\n",
      "Epoch 487 finished. Approximate loss: -312.2771\n",
      "Epoch 488 finished. Approximate loss: -258.4626\n",
      "Epoch 489 finished. Approximate loss: -265.4155\n",
      "Epoch 490 finished. Approximate loss: -291.5433\n",
      "Epoch 491 finished. Approximate loss: -290.6636\n",
      "Epoch 492 finished. Approximate loss: -287.8093\n",
      "Epoch 493 finished. Approximate loss: -289.6948\n",
      "Epoch 494 finished. Approximate loss: -271.1117\n",
      "Epoch 495 finished. Approximate loss: -304.0322\n",
      "Epoch 496 finished. Approximate loss: -308.1623\n",
      "Epoch 497 finished. Approximate loss: -310.1583\n",
      "Epoch 498 finished. Approximate loss: -281.2098\n",
      "Epoch 499 finished. Approximate loss: -341.1502\n",
      "Epoch 500 finished. Approximate loss: -310.5315\n",
      "Epoch 501 finished. Approximate loss: -315.3621\n",
      "Epoch 502 finished. Approximate loss: -276.8746\n",
      "Epoch 503 finished. Approximate loss: -303.6540\n",
      "Epoch 504 finished. Approximate loss: -295.1417\n",
      "Epoch 505 finished. Approximate loss: -320.6122\n",
      "Epoch 506 finished. Approximate loss: -325.3733\n",
      "Epoch 507 finished. Approximate loss: -264.4198\n",
      "Epoch 508 finished. Approximate loss: -274.8982\n",
      "Epoch 509 finished. Approximate loss: -321.5355\n",
      "Epoch 510 finished. Approximate loss: -296.2215\n",
      "Epoch 511 finished. Approximate loss: -252.3913\n",
      "Epoch 512 finished. Approximate loss: -318.1546\n",
      "Epoch 513 finished. Approximate loss: -322.8259\n",
      "Epoch 514 finished. Approximate loss: -322.2401\n",
      "Epoch 515 finished. Approximate loss: -304.0350\n",
      "Epoch 516 finished. Approximate loss: -317.6695\n",
      "Epoch 517 finished. Approximate loss: -291.0988\n",
      "Epoch 518 finished. Approximate loss: -284.4174\n",
      "Epoch 519 finished. Approximate loss: -290.0534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 520 finished. Approximate loss: -288.4724\n",
      "Epoch 521 finished. Approximate loss: -321.6734\n",
      "Epoch 522 finished. Approximate loss: -329.3170\n",
      "Epoch 523 finished. Approximate loss: -313.9495\n",
      "Epoch 524 finished. Approximate loss: -298.9719\n",
      "Epoch 525 finished. Approximate loss: -283.8829\n",
      "Epoch 526 finished. Approximate loss: -397.4103\n",
      "Epoch 527 finished. Approximate loss: -304.1426\n",
      "Epoch 528 finished. Approximate loss: -336.0091\n",
      "Epoch 529 finished. Approximate loss: -329.0070\n",
      "Epoch 530 finished. Approximate loss: -332.3178\n",
      "Epoch 531 finished. Approximate loss: -286.8459\n",
      "Epoch 532 finished. Approximate loss: -292.0172\n",
      "Epoch 533 finished. Approximate loss: -302.5944\n",
      "Epoch 534 finished. Approximate loss: -332.1918\n",
      "Epoch 535 finished. Approximate loss: -333.0146\n",
      "Epoch 536 finished. Approximate loss: -321.9497\n",
      "Epoch 537 finished. Approximate loss: -279.1184\n",
      "Epoch 538 finished. Approximate loss: -331.7827\n",
      "Epoch 539 finished. Approximate loss: -310.3246\n",
      "Epoch 540 finished. Approximate loss: -349.0785\n",
      "Epoch 541 finished. Approximate loss: -360.3988\n",
      "Epoch 542 finished. Approximate loss: -359.1844\n",
      "Epoch 543 finished. Approximate loss: -325.2546\n",
      "Epoch 544 finished. Approximate loss: -256.6828\n",
      "Epoch 545 finished. Approximate loss: -325.7276\n",
      "Epoch 546 finished. Approximate loss: -346.6774\n",
      "Epoch 547 finished. Approximate loss: -266.5985\n",
      "Epoch 548 finished. Approximate loss: -359.8597\n",
      "Epoch 549 finished. Approximate loss: -330.4662\n",
      "Epoch 550 finished. Approximate loss: -317.3642\n",
      "Epoch 551 finished. Approximate loss: -318.5836\n",
      "Epoch 552 finished. Approximate loss: -354.0212\n",
      "Epoch 553 finished. Approximate loss: -318.8783\n",
      "Epoch 554 finished. Approximate loss: -351.4352\n",
      "Epoch 555 finished. Approximate loss: -349.7575\n",
      "Epoch 556 finished. Approximate loss: -316.3728\n",
      "Epoch 557 finished. Approximate loss: -331.2828\n",
      "Epoch 558 finished. Approximate loss: -289.1089\n",
      "Epoch 559 finished. Approximate loss: -297.8353\n",
      "Epoch 560 finished. Approximate loss: -343.7880\n",
      "Epoch 561 finished. Approximate loss: -307.0174\n",
      "Epoch 562 finished. Approximate loss: -342.5890\n",
      "Epoch 563 finished. Approximate loss: -326.3421\n",
      "Epoch 564 finished. Approximate loss: -373.0804\n",
      "Epoch 565 finished. Approximate loss: -376.7216\n",
      "Epoch 566 finished. Approximate loss: -358.1190\n",
      "Epoch 567 finished. Approximate loss: -356.5792\n",
      "Epoch 568 finished. Approximate loss: -323.5883\n",
      "Epoch 569 finished. Approximate loss: -321.0472\n",
      "Epoch 570 finished. Approximate loss: -350.7030\n",
      "Epoch 571 finished. Approximate loss: -345.7003\n",
      "Epoch 572 finished. Approximate loss: -311.6240\n",
      "Epoch 573 finished. Approximate loss: -338.5852\n",
      "Epoch 574 finished. Approximate loss: -388.7334\n",
      "Epoch 575 finished. Approximate loss: -365.8636\n",
      "Epoch 576 finished. Approximate loss: -373.5990\n",
      "Epoch 577 finished. Approximate loss: -369.5988\n",
      "Epoch 578 finished. Approximate loss: -358.9125\n",
      "Epoch 579 finished. Approximate loss: -263.8140\n",
      "Epoch 580 finished. Approximate loss: -392.7728\n",
      "Epoch 581 finished. Approximate loss: -392.1709\n",
      "Epoch 582 finished. Approximate loss: -342.0326\n",
      "Epoch 583 finished. Approximate loss: -355.3468\n",
      "Epoch 584 finished. Approximate loss: -368.5817\n",
      "Epoch 585 finished. Approximate loss: -340.7714\n",
      "Epoch 586 finished. Approximate loss: -341.6264\n",
      "Epoch 587 finished. Approximate loss: -348.4157\n",
      "Epoch 588 finished. Approximate loss: -411.1545\n",
      "Epoch 589 finished. Approximate loss: -369.1948\n",
      "Epoch 590 finished. Approximate loss: -394.0753\n",
      "Epoch 591 finished. Approximate loss: -372.8432\n",
      "Epoch 592 finished. Approximate loss: -373.6205\n",
      "Epoch 593 finished. Approximate loss: -340.4744\n",
      "Epoch 594 finished. Approximate loss: -365.3599\n",
      "Epoch 595 finished. Approximate loss: -362.0419\n",
      "Epoch 596 finished. Approximate loss: -383.0186\n",
      "Epoch 597 finished. Approximate loss: -374.3530\n",
      "Epoch 598 finished. Approximate loss: -364.9733\n",
      "Epoch 599 finished. Approximate loss: -368.9245\n",
      "Epoch 600 finished. Approximate loss: -361.3449\n",
      "Epoch 601 finished. Approximate loss: -344.5888\n",
      "Epoch 602 finished. Approximate loss: -367.9540\n",
      "Epoch 603 finished. Approximate loss: -401.5936\n",
      "Epoch 604 finished. Approximate loss: -368.1289\n",
      "Epoch 605 finished. Approximate loss: -362.9661\n",
      "Epoch 606 finished. Approximate loss: -391.6813\n",
      "Epoch 607 finished. Approximate loss: -383.5536\n",
      "Epoch 608 finished. Approximate loss: -433.3797\n",
      "Epoch 609 finished. Approximate loss: -357.4140\n",
      "Epoch 610 finished. Approximate loss: -365.8686\n",
      "Epoch 611 finished. Approximate loss: -423.4071\n",
      "Epoch 612 finished. Approximate loss: -396.7942\n",
      "Epoch 613 finished. Approximate loss: -352.1591\n",
      "Epoch 614 finished. Approximate loss: -408.3765\n",
      "Epoch 615 finished. Approximate loss: -391.0656\n",
      "Epoch 616 finished. Approximate loss: -389.1754\n",
      "Epoch 617 finished. Approximate loss: -369.5149\n",
      "Epoch 618 finished. Approximate loss: -370.8395\n",
      "Epoch 619 finished. Approximate loss: -353.1037\n",
      "Epoch 620 finished. Approximate loss: -359.3576\n",
      "Epoch 621 finished. Approximate loss: -338.4643\n",
      "Epoch 622 finished. Approximate loss: -348.4336\n",
      "Epoch 623 finished. Approximate loss: -399.2118\n",
      "Epoch 624 finished. Approximate loss: -391.4926\n",
      "Epoch 625 finished. Approximate loss: -392.4838\n",
      "Epoch 626 finished. Approximate loss: -363.2742\n",
      "Epoch 627 finished. Approximate loss: -407.3930\n",
      "Epoch 628 finished. Approximate loss: -441.6082\n",
      "Epoch 629 finished. Approximate loss: -377.4089\n",
      "Epoch 630 finished. Approximate loss: -374.5531\n",
      "Epoch 631 finished. Approximate loss: -336.5941\n",
      "Epoch 632 finished. Approximate loss: -372.4962\n",
      "Epoch 633 finished. Approximate loss: -389.9642\n",
      "Epoch 634 finished. Approximate loss: -427.1728\n",
      "Epoch 635 finished. Approximate loss: -356.6253\n",
      "Epoch 636 finished. Approximate loss: -408.9009\n",
      "Epoch 637 finished. Approximate loss: -361.1972\n",
      "Epoch 638 finished. Approximate loss: -376.5913\n",
      "Epoch 639 finished. Approximate loss: -413.1121\n",
      "Epoch 640 finished. Approximate loss: -380.3769\n",
      "Epoch 641 finished. Approximate loss: -387.3806\n",
      "Epoch 642 finished. Approximate loss: -424.8509\n",
      "Epoch 643 finished. Approximate loss: -394.1632\n",
      "Epoch 644 finished. Approximate loss: -393.6145\n",
      "Epoch 645 finished. Approximate loss: -391.5814\n",
      "Epoch 646 finished. Approximate loss: -387.8864\n",
      "Epoch 647 finished. Approximate loss: -402.5247\n",
      "Epoch 648 finished. Approximate loss: -377.2436\n",
      "Epoch 649 finished. Approximate loss: -395.9417\n",
      "Epoch 650 finished. Approximate loss: -398.3501\n",
      "Epoch 651 finished. Approximate loss: -415.9881\n",
      "Epoch 652 finished. Approximate loss: -386.3988\n",
      "Epoch 653 finished. Approximate loss: -398.3693\n",
      "Epoch 654 finished. Approximate loss: -348.9450\n",
      "Epoch 655 finished. Approximate loss: -403.2823\n",
      "Epoch 656 finished. Approximate loss: -422.4835\n",
      "Epoch 657 finished. Approximate loss: -416.1959\n",
      "Epoch 658 finished. Approximate loss: -398.8734\n",
      "Epoch 659 finished. Approximate loss: -440.7626\n",
      "Epoch 660 finished. Approximate loss: -419.4479\n",
      "Epoch 661 finished. Approximate loss: -398.0390\n",
      "Epoch 662 finished. Approximate loss: -356.3369\n",
      "Epoch 663 finished. Approximate loss: -454.0715\n",
      "Epoch 664 finished. Approximate loss: -422.2253\n",
      "Epoch 665 finished. Approximate loss: -425.4920\n",
      "Epoch 666 finished. Approximate loss: -474.7099\n",
      "Epoch 667 finished. Approximate loss: -374.7582\n",
      "Epoch 668 finished. Approximate loss: -368.2640\n",
      "Epoch 669 finished. Approximate loss: -402.2368\n",
      "Epoch 670 finished. Approximate loss: -420.7872\n",
      "Epoch 671 finished. Approximate loss: -444.4780\n",
      "Epoch 672 finished. Approximate loss: -445.1681\n",
      "Epoch 673 finished. Approximate loss: -401.4347\n",
      "Epoch 674 finished. Approximate loss: -393.0004\n",
      "Epoch 675 finished. Approximate loss: -401.4760\n",
      "Epoch 676 finished. Approximate loss: -325.6252\n",
      "Epoch 677 finished. Approximate loss: -414.3479\n",
      "Epoch 678 finished. Approximate loss: -429.6116\n",
      "Epoch 679 finished. Approximate loss: -480.6453\n",
      "Epoch 680 finished. Approximate loss: -420.7382\n",
      "Epoch 681 finished. Approximate loss: -430.6223\n",
      "Epoch 682 finished. Approximate loss: -391.8144\n",
      "Epoch 683 finished. Approximate loss: -419.0404\n",
      "Epoch 684 finished. Approximate loss: -419.5999\n",
      "Epoch 685 finished. Approximate loss: -438.7792\n",
      "Epoch 686 finished. Approximate loss: -385.7034\n",
      "Epoch 687 finished. Approximate loss: -393.4665\n",
      "Epoch 688 finished. Approximate loss: -383.4723\n",
      "Epoch 689 finished. Approximate loss: -429.2150\n",
      "Epoch 690 finished. Approximate loss: -418.4801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 691 finished. Approximate loss: -451.8960\n",
      "Epoch 692 finished. Approximate loss: -415.6849\n",
      "Epoch 693 finished. Approximate loss: -424.2487\n",
      "Epoch 694 finished. Approximate loss: -386.4717\n",
      "Epoch 695 finished. Approximate loss: -384.0064\n",
      "Epoch 696 finished. Approximate loss: -399.5861\n",
      "Epoch 697 finished. Approximate loss: -388.1553\n",
      "Epoch 698 finished. Approximate loss: -420.6832\n",
      "Epoch 699 finished. Approximate loss: -418.4183\n",
      "Epoch 700 finished. Approximate loss: -338.3044\n",
      "Epoch 701 finished. Approximate loss: -385.1776\n",
      "Epoch 702 finished. Approximate loss: -499.0565\n",
      "Epoch 703 finished. Approximate loss: -406.5690\n",
      "Epoch 704 finished. Approximate loss: -437.4010\n",
      "Epoch 705 finished. Approximate loss: -406.5791\n",
      "Epoch 706 finished. Approximate loss: -437.1051\n",
      "Epoch 707 finished. Approximate loss: -401.7551\n",
      "Epoch 708 finished. Approximate loss: -472.1497\n",
      "Epoch 709 finished. Approximate loss: -448.5664\n",
      "Epoch 710 finished. Approximate loss: -413.1811\n",
      "Epoch 711 finished. Approximate loss: -452.7429\n",
      "Epoch 712 finished. Approximate loss: -410.9307\n",
      "Epoch 713 finished. Approximate loss: -435.1583\n",
      "Epoch 714 finished. Approximate loss: -458.8879\n",
      "Epoch 715 finished. Approximate loss: -433.8082\n",
      "Epoch 716 finished. Approximate loss: -472.1357\n",
      "Epoch 717 finished. Approximate loss: -415.2902\n",
      "Epoch 718 finished. Approximate loss: -481.3421\n",
      "Epoch 719 finished. Approximate loss: -422.7368\n",
      "Epoch 720 finished. Approximate loss: -396.5859\n",
      "Epoch 721 finished. Approximate loss: -420.3587\n",
      "Epoch 722 finished. Approximate loss: -408.5758\n",
      "Epoch 723 finished. Approximate loss: -448.4099\n",
      "Epoch 724 finished. Approximate loss: -469.8084\n",
      "Epoch 725 finished. Approximate loss: -411.6387\n",
      "Epoch 726 finished. Approximate loss: -447.4407\n",
      "Epoch 727 finished. Approximate loss: -431.3302\n",
      "Epoch 728 finished. Approximate loss: -417.6336\n",
      "Epoch 729 finished. Approximate loss: -469.3770\n",
      "Epoch 730 finished. Approximate loss: -420.3413\n",
      "Epoch 731 finished. Approximate loss: -450.4894\n",
      "Epoch 732 finished. Approximate loss: -494.4104\n",
      "Epoch 733 finished. Approximate loss: -469.4903\n",
      "Epoch 734 finished. Approximate loss: -541.0101\n",
      "Epoch 735 finished. Approximate loss: -470.3605\n",
      "Epoch 736 finished. Approximate loss: -492.5423\n",
      "Epoch 737 finished. Approximate loss: -468.4761\n",
      "Epoch 738 finished. Approximate loss: -391.9196\n",
      "Epoch 739 finished. Approximate loss: -472.4395\n",
      "Epoch 740 finished. Approximate loss: -462.2649\n",
      "Epoch 741 finished. Approximate loss: -465.8189\n",
      "Epoch 742 finished. Approximate loss: -419.2995\n",
      "Epoch 743 finished. Approximate loss: -509.5089\n",
      "Epoch 744 finished. Approximate loss: -438.0390\n",
      "Epoch 745 finished. Approximate loss: -446.6958\n",
      "Epoch 746 finished. Approximate loss: -511.4984\n",
      "Epoch 747 finished. Approximate loss: -460.5118\n",
      "Epoch 748 finished. Approximate loss: -475.7973\n",
      "Epoch 749 finished. Approximate loss: -434.5157\n",
      "Epoch 750 finished. Approximate loss: -456.3598\n",
      "Epoch 751 finished. Approximate loss: -450.2753\n",
      "Epoch 752 finished. Approximate loss: -456.4087\n",
      "Epoch 753 finished. Approximate loss: -479.9158\n",
      "Epoch 754 finished. Approximate loss: -406.3197\n",
      "Epoch 755 finished. Approximate loss: -429.1148\n",
      "Epoch 756 finished. Approximate loss: -441.2791\n",
      "Epoch 757 finished. Approximate loss: -519.2250\n",
      "Epoch 758 finished. Approximate loss: -496.0229\n",
      "Epoch 759 finished. Approximate loss: -475.7047\n",
      "Epoch 760 finished. Approximate loss: -446.4005\n",
      "Epoch 761 finished. Approximate loss: -499.8620\n",
      "Epoch 762 finished. Approximate loss: -494.3525\n",
      "Epoch 763 finished. Approximate loss: -450.5201\n",
      "Epoch 764 finished. Approximate loss: -469.5777\n",
      "Epoch 765 finished. Approximate loss: -459.1838\n",
      "Epoch 766 finished. Approximate loss: -413.7205\n",
      "Epoch 767 finished. Approximate loss: -532.6024\n",
      "Epoch 768 finished. Approximate loss: -436.1130\n",
      "Epoch 769 finished. Approximate loss: -537.8576\n",
      "Epoch 770 finished. Approximate loss: -448.8327\n",
      "Epoch 771 finished. Approximate loss: -482.5653\n",
      "Epoch 772 finished. Approximate loss: -458.8637\n",
      "Epoch 773 finished. Approximate loss: -461.5381\n",
      "Epoch 774 finished. Approximate loss: -531.8130\n",
      "Epoch 775 finished. Approximate loss: -510.0186\n",
      "Epoch 776 finished. Approximate loss: -494.7341\n",
      "Epoch 777 finished. Approximate loss: -394.9896\n",
      "Epoch 778 finished. Approximate loss: -493.1514\n",
      "Epoch 779 finished. Approximate loss: -491.2297\n",
      "Epoch 780 finished. Approximate loss: -439.8244\n",
      "Epoch 781 finished. Approximate loss: -488.0622\n",
      "Epoch 782 finished. Approximate loss: -496.0209\n",
      "Epoch 783 finished. Approximate loss: -400.3583\n",
      "Epoch 784 finished. Approximate loss: -529.1000\n",
      "Epoch 785 finished. Approximate loss: -476.9379\n",
      "Epoch 786 finished. Approximate loss: -479.4352\n",
      "Epoch 787 finished. Approximate loss: -503.2912\n",
      "Epoch 788 finished. Approximate loss: -403.3238\n",
      "Epoch 789 finished. Approximate loss: -449.4991\n",
      "Epoch 790 finished. Approximate loss: -525.9476\n",
      "Epoch 791 finished. Approximate loss: -555.7645\n",
      "Epoch 792 finished. Approximate loss: -446.8134\n",
      "Epoch 793 finished. Approximate loss: -471.5964\n",
      "Epoch 794 finished. Approximate loss: -409.1017\n",
      "Epoch 795 finished. Approximate loss: -456.1431\n",
      "Epoch 796 finished. Approximate loss: -538.8470\n",
      "Epoch 797 finished. Approximate loss: -362.9954\n",
      "Epoch 798 finished. Approximate loss: -535.4051\n",
      "Epoch 799 finished. Approximate loss: -534.9978\n",
      "Epoch 800 finished. Approximate loss: -497.9512\n",
      "Epoch 801 finished. Approximate loss: -504.5403\n",
      "Epoch 802 finished. Approximate loss: -488.9567\n",
      "Epoch 803 finished. Approximate loss: -450.2026\n",
      "Epoch 804 finished. Approximate loss: -495.1814\n",
      "Epoch 805 finished. Approximate loss: -512.7217\n",
      "Epoch 806 finished. Approximate loss: -480.2940\n",
      "Epoch 807 finished. Approximate loss: -458.0554\n",
      "Epoch 808 finished. Approximate loss: -505.2913\n",
      "Epoch 809 finished. Approximate loss: -478.7269\n",
      "Epoch 810 finished. Approximate loss: -460.7785\n",
      "Epoch 811 finished. Approximate loss: -494.9720\n",
      "Epoch 812 finished. Approximate loss: -469.2412\n",
      "Epoch 813 finished. Approximate loss: -426.1576\n",
      "Epoch 814 finished. Approximate loss: -453.4651\n",
      "Epoch 815 finished. Approximate loss: -485.7944\n",
      "Epoch 816 finished. Approximate loss: -576.7222\n",
      "Epoch 817 finished. Approximate loss: -440.0233\n",
      "Epoch 818 finished. Approximate loss: -513.3790\n",
      "Epoch 819 finished. Approximate loss: -550.6267\n",
      "Epoch 820 finished. Approximate loss: -512.1723\n",
      "Epoch 821 finished. Approximate loss: -515.8553\n",
      "Epoch 822 finished. Approximate loss: -426.5399\n",
      "Epoch 823 finished. Approximate loss: -520.5965\n",
      "Epoch 824 finished. Approximate loss: -481.8435\n",
      "Epoch 825 finished. Approximate loss: -503.0557\n",
      "Epoch 826 finished. Approximate loss: -469.5870\n",
      "Epoch 827 finished. Approximate loss: -554.2938\n",
      "Epoch 828 finished. Approximate loss: -464.4484\n",
      "Epoch 829 finished. Approximate loss: -508.2983\n",
      "Epoch 830 finished. Approximate loss: -563.9344\n",
      "Epoch 831 finished. Approximate loss: -549.5441\n",
      "Epoch 832 finished. Approximate loss: -465.1646\n",
      "Epoch 833 finished. Approximate loss: -521.8865\n",
      "Epoch 834 finished. Approximate loss: -485.0701\n",
      "Epoch 835 finished. Approximate loss: -503.3845\n",
      "Epoch 836 finished. Approximate loss: -541.9672\n",
      "Epoch 837 finished. Approximate loss: -465.7039\n",
      "Epoch 838 finished. Approximate loss: -529.0862\n",
      "Epoch 839 finished. Approximate loss: -571.0667\n",
      "Epoch 840 finished. Approximate loss: -529.1166\n",
      "Epoch 841 finished. Approximate loss: -442.2277\n",
      "Epoch 842 finished. Approximate loss: -509.0656\n",
      "Epoch 843 finished. Approximate loss: -531.0339\n",
      "Epoch 844 finished. Approximate loss: -554.6469\n",
      "Epoch 845 finished. Approximate loss: -496.9958\n",
      "Epoch 846 finished. Approximate loss: -565.1027\n",
      "Epoch 847 finished. Approximate loss: -477.0673\n",
      "Epoch 848 finished. Approximate loss: -514.8446\n",
      "Epoch 849 finished. Approximate loss: -548.4226\n",
      "Epoch 850 finished. Approximate loss: -545.2944\n",
      "Epoch 851 finished. Approximate loss: -496.3902\n",
      "Epoch 852 finished. Approximate loss: -526.3548\n",
      "Epoch 853 finished. Approximate loss: -530.7971\n",
      "Epoch 854 finished. Approximate loss: -500.6229\n",
      "Epoch 855 finished. Approximate loss: -495.3967\n",
      "Epoch 856 finished. Approximate loss: -511.1803\n",
      "Epoch 857 finished. Approximate loss: -500.4705\n",
      "Epoch 858 finished. Approximate loss: -490.8079\n",
      "Epoch 859 finished. Approximate loss: -584.9737\n",
      "Epoch 860 finished. Approximate loss: -532.6652\n",
      "Epoch 861 finished. Approximate loss: -519.2322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 862 finished. Approximate loss: -540.5111\n",
      "Epoch 863 finished. Approximate loss: -503.1412\n",
      "Epoch 864 finished. Approximate loss: -500.5101\n",
      "Epoch 865 finished. Approximate loss: -531.5670\n",
      "Epoch 866 finished. Approximate loss: -424.2885\n",
      "Epoch 867 finished. Approximate loss: -485.3376\n",
      "Epoch 868 finished. Approximate loss: -534.4977\n",
      "Epoch 869 finished. Approximate loss: -516.0002\n",
      "Epoch 870 finished. Approximate loss: -501.2491\n",
      "Epoch 871 finished. Approximate loss: -562.4482\n",
      "Epoch 872 finished. Approximate loss: -519.5982\n",
      "Epoch 873 finished. Approximate loss: -540.5998\n",
      "Epoch 874 finished. Approximate loss: -540.6643\n",
      "Epoch 875 finished. Approximate loss: -512.7111\n",
      "Epoch 876 finished. Approximate loss: -591.4170\n",
      "Epoch 877 finished. Approximate loss: -499.3108\n",
      "Epoch 878 finished. Approximate loss: -495.0967\n",
      "Epoch 879 finished. Approximate loss: -570.4924\n",
      "Epoch 880 finished. Approximate loss: -545.2364\n",
      "Epoch 881 finished. Approximate loss: -488.5263\n",
      "Epoch 882 finished. Approximate loss: -487.3736\n",
      "Epoch 883 finished. Approximate loss: -587.9369\n",
      "Epoch 884 finished. Approximate loss: -516.5122\n",
      "Epoch 885 finished. Approximate loss: -509.4533\n",
      "Epoch 886 finished. Approximate loss: -519.0318\n",
      "Epoch 887 finished. Approximate loss: -549.3085\n",
      "Epoch 888 finished. Approximate loss: -585.7245\n",
      "Epoch 889 finished. Approximate loss: -479.5038\n",
      "Epoch 890 finished. Approximate loss: -499.0905\n",
      "Epoch 891 finished. Approximate loss: -590.2520\n",
      "Epoch 892 finished. Approximate loss: -518.0141\n",
      "Epoch 893 finished. Approximate loss: -580.3308\n",
      "Epoch 894 finished. Approximate loss: -607.8517\n",
      "Epoch 895 finished. Approximate loss: -552.1278\n",
      "Epoch 896 finished. Approximate loss: -498.1457\n",
      "Epoch 897 finished. Approximate loss: -551.1755\n",
      "Epoch 898 finished. Approximate loss: -547.9287\n",
      "Epoch 899 finished. Approximate loss: -563.1383\n",
      "Epoch 900 finished. Approximate loss: -534.0554\n",
      "Epoch 901 finished. Approximate loss: -537.8927\n",
      "Epoch 902 finished. Approximate loss: -574.3303\n",
      "Epoch 903 finished. Approximate loss: -528.7288\n",
      "Epoch 904 finished. Approximate loss: -511.3449\n",
      "Epoch 905 finished. Approximate loss: -584.9209\n",
      "Epoch 906 finished. Approximate loss: -581.0748\n",
      "Epoch 907 finished. Approximate loss: -479.7463\n",
      "Epoch 908 finished. Approximate loss: -547.1624\n",
      "Epoch 909 finished. Approximate loss: -580.0513\n",
      "Epoch 910 finished. Approximate loss: -515.3359\n",
      "Epoch 911 finished. Approximate loss: -518.7859\n",
      "Epoch 912 finished. Approximate loss: -552.1278\n",
      "Epoch 913 finished. Approximate loss: -482.6196\n",
      "Epoch 914 finished. Approximate loss: -641.1500\n",
      "Epoch 915 finished. Approximate loss: -529.9828\n",
      "Epoch 916 finished. Approximate loss: -520.8206\n",
      "Epoch 917 finished. Approximate loss: -597.5033\n",
      "Epoch 918 finished. Approximate loss: -565.0949\n",
      "Epoch 919 finished. Approximate loss: -551.2437\n",
      "Epoch 920 finished. Approximate loss: -606.5958\n",
      "Epoch 921 finished. Approximate loss: -561.2536\n",
      "Epoch 922 finished. Approximate loss: -512.3846\n",
      "Epoch 923 finished. Approximate loss: -568.4217\n",
      "Epoch 924 finished. Approximate loss: -505.9103\n",
      "Epoch 925 finished. Approximate loss: -671.2733\n",
      "Epoch 926 finished. Approximate loss: -570.3569\n",
      "Epoch 927 finished. Approximate loss: -583.6068\n",
      "Epoch 928 finished. Approximate loss: -536.7196\n",
      "Epoch 929 finished. Approximate loss: -574.0265\n",
      "Epoch 930 finished. Approximate loss: -546.2428\n",
      "Epoch 931 finished. Approximate loss: -585.4567\n",
      "Epoch 932 finished. Approximate loss: -553.2974\n",
      "Epoch 933 finished. Approximate loss: -582.4865\n",
      "Epoch 934 finished. Approximate loss: -560.0916\n",
      "Epoch 935 finished. Approximate loss: -609.5882\n",
      "Epoch 936 finished. Approximate loss: -535.7107\n",
      "Epoch 937 finished. Approximate loss: -579.2525\n",
      "Epoch 938 finished. Approximate loss: -578.5080\n",
      "Epoch 939 finished. Approximate loss: -526.6726\n",
      "Epoch 940 finished. Approximate loss: -560.3035\n",
      "Epoch 941 finished. Approximate loss: -527.0111\n",
      "Epoch 942 finished. Approximate loss: -588.0310\n",
      "Epoch 943 finished. Approximate loss: -559.5602\n",
      "Epoch 944 finished. Approximate loss: -512.3230\n",
      "Epoch 945 finished. Approximate loss: -552.9418\n",
      "Epoch 946 finished. Approximate loss: -564.9223\n",
      "Epoch 947 finished. Approximate loss: -548.9039\n",
      "Epoch 948 finished. Approximate loss: -558.6719\n",
      "Epoch 949 finished. Approximate loss: -508.9040\n",
      "Epoch 950 finished. Approximate loss: -560.5123\n",
      "Epoch 951 finished. Approximate loss: -632.3250\n",
      "Epoch 952 finished. Approximate loss: -558.3006\n",
      "Epoch 953 finished. Approximate loss: -554.8028\n",
      "Epoch 954 finished. Approximate loss: -578.6574\n",
      "Epoch 955 finished. Approximate loss: -591.5024\n",
      "Epoch 956 finished. Approximate loss: -641.7409\n",
      "Epoch 957 finished. Approximate loss: -610.0786\n",
      "Epoch 958 finished. Approximate loss: -617.2786\n",
      "Epoch 959 finished. Approximate loss: -591.0192\n",
      "Epoch 960 finished. Approximate loss: -560.2436\n",
      "Epoch 961 finished. Approximate loss: -520.9763\n",
      "Epoch 962 finished. Approximate loss: -575.8889\n",
      "Epoch 963 finished. Approximate loss: -684.8970\n",
      "Epoch 964 finished. Approximate loss: -599.6705\n",
      "Epoch 965 finished. Approximate loss: -536.4832\n",
      "Epoch 966 finished. Approximate loss: -579.2382\n",
      "Epoch 967 finished. Approximate loss: -622.5471\n",
      "Epoch 968 finished. Approximate loss: -575.8494\n",
      "Epoch 969 finished. Approximate loss: -527.4911\n",
      "Epoch 970 finished. Approximate loss: -636.5419\n",
      "Epoch 971 finished. Approximate loss: -612.2488\n",
      "Epoch 972 finished. Approximate loss: -542.1928\n",
      "Epoch 973 finished. Approximate loss: -561.5007\n",
      "Epoch 974 finished. Approximate loss: -553.5125\n",
      "Epoch 975 finished. Approximate loss: -540.4993\n",
      "Epoch 976 finished. Approximate loss: -611.2992\n",
      "Epoch 977 finished. Approximate loss: -556.6449\n",
      "Epoch 978 finished. Approximate loss: -623.9721\n",
      "Epoch 979 finished. Approximate loss: -582.0949\n",
      "Epoch 980 finished. Approximate loss: -587.7951\n",
      "Epoch 981 finished. Approximate loss: -614.2679\n",
      "Epoch 982 finished. Approximate loss: -593.9799\n",
      "Epoch 983 finished. Approximate loss: -639.6092\n",
      "Epoch 984 finished. Approximate loss: -565.6458\n",
      "Epoch 985 finished. Approximate loss: -640.9568\n",
      "Epoch 986 finished. Approximate loss: -601.5009\n",
      "Epoch 987 finished. Approximate loss: -653.5091\n",
      "Epoch 988 finished. Approximate loss: -562.0734\n",
      "Epoch 989 finished. Approximate loss: -636.6873\n",
      "Epoch 990 finished. Approximate loss: -534.7778\n",
      "Epoch 991 finished. Approximate loss: -624.6371\n",
      "Epoch 992 finished. Approximate loss: -574.0851\n",
      "Epoch 993 finished. Approximate loss: -599.5948\n",
      "Epoch 994 finished. Approximate loss: -568.0364\n",
      "Epoch 995 finished. Approximate loss: -631.9260\n",
      "Epoch 996 finished. Approximate loss: -643.2224\n",
      "Epoch 997 finished. Approximate loss: -609.4770\n",
      "Epoch 998 finished. Approximate loss: -524.5712\n",
      "Epoch 999 finished. Approximate loss: -718.9845\n",
      "Epoch 1000 finished. Approximate loss: -627.9540\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.train(train_dataset, optim_algorithm='Adam',\n",
    "                 epochs=1000, batch_size=2048, lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f029aac54e0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(classifier.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 2791/5000 (55.82%)\n"
     ]
    }
   ],
   "source": [
    "classifier.predict(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(vocab_size,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "model = LogisticRegression(vocab_size)\n",
    "device = 'cuda:1'\n",
    "LRTrainer = Trainer(train_loader,dev_loader,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Loss function: BCE\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 100\n",
      "Running on device (cuda:1)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.7051641941070557\n",
      "Accuracy on validation dataset: 1982/5000 (39.64%)\n",
      "Accuracy on training dataset: 8092/20000 (40.46%)\n",
      "\n",
      "Epoch: 2, Batch number: 0, Loss: 0.2502511739730835\n",
      "Accuracy on validation dataset: 2521/5000 (50.42%)\n",
      "Accuracy on training dataset: 9989/20000 (49.95%)\n",
      "\n",
      "Epoch: 3, Batch number: 0, Loss: 0.07242456078529358\n",
      "Accuracy on validation dataset: 2539/5000 (50.78%)\n",
      "Accuracy on training dataset: 10010/20000 (50.05%)\n",
      "\n",
      "Epoch: 4, Batch number: 0, Loss: 0.21413671970367432\n",
      "Accuracy on validation dataset: 2563/5000 (51.26%)\n",
      "Accuracy on training dataset: 10066/20000 (50.33%)\n",
      "\n",
      "Epoch: 5, Batch number: 0, Loss: -0.2427576780319214\n",
      "Accuracy on validation dataset: 2595/5000 (51.90%)\n",
      "Accuracy on training dataset: 10234/20000 (51.17%)\n",
      "\n",
      "Epoch: 6, Batch number: 0, Loss: -0.13706590235233307\n",
      "Accuracy on validation dataset: 2629/5000 (52.58%)\n",
      "Accuracy on training dataset: 10371/20000 (51.85%)\n",
      "\n",
      "Epoch: 7, Batch number: 0, Loss: -0.45216310024261475\n",
      "Accuracy on validation dataset: 2678/5000 (53.56%)\n",
      "Accuracy on training dataset: 10565/20000 (52.83%)\n",
      "\n",
      "Epoch: 8, Batch number: 0, Loss: -0.5465776920318604\n",
      "Accuracy on validation dataset: 2716/5000 (54.32%)\n",
      "Accuracy on training dataset: 10701/20000 (53.51%)\n",
      "\n",
      "Epoch: 9, Batch number: 0, Loss: -0.8770203590393066\n",
      "Accuracy on validation dataset: 2730/5000 (54.60%)\n",
      "Accuracy on training dataset: 10786/20000 (53.93%)\n",
      "\n",
      "Epoch: 10, Batch number: 0, Loss: -0.5439810752868652\n",
      "Accuracy on validation dataset: 2748/5000 (54.96%)\n",
      "Accuracy on training dataset: 10845/20000 (54.23%)\n",
      "\n",
      "Epoch: 11, Batch number: 0, Loss: -0.6939297914505005\n",
      "Accuracy on validation dataset: 2779/5000 (55.58%)\n",
      "Accuracy on training dataset: 10975/20000 (54.88%)\n",
      "\n",
      "Epoch: 12, Batch number: 0, Loss: -1.0047197341918945\n",
      "Accuracy on validation dataset: 2789/5000 (55.78%)\n",
      "Accuracy on training dataset: 10998/20000 (54.99%)\n",
      "\n",
      "Epoch: 13, Batch number: 0, Loss: -0.8000127077102661\n",
      "Accuracy on validation dataset: 2782/5000 (55.64%)\n",
      "Accuracy on training dataset: 10959/20000 (54.80%)\n",
      "\n",
      "Epoch: 14, Batch number: 0, Loss: -1.1668834686279297\n",
      "Accuracy on validation dataset: 2795/5000 (55.90%)\n",
      "Accuracy on training dataset: 11010/20000 (55.05%)\n",
      "\n",
      "Epoch: 15, Batch number: 0, Loss: -0.9273906350135803\n",
      "Accuracy on validation dataset: 2804/5000 (56.08%)\n",
      "Accuracy on training dataset: 11055/20000 (55.27%)\n",
      "\n",
      "Epoch: 16, Batch number: 0, Loss: -1.1220664978027344\n",
      "Accuracy on validation dataset: 2799/5000 (55.98%)\n",
      "Accuracy on training dataset: 11019/20000 (55.09%)\n",
      "\n",
      "Epoch: 17, Batch number: 0, Loss: -1.6131081581115723\n",
      "Accuracy on validation dataset: 2804/5000 (56.08%)\n",
      "Accuracy on training dataset: 11047/20000 (55.23%)\n",
      "\n",
      "Epoch: 18, Batch number: 0, Loss: -1.4112944602966309\n",
      "Accuracy on validation dataset: 2810/5000 (56.20%)\n",
      "Accuracy on training dataset: 11063/20000 (55.31%)\n",
      "\n",
      "Epoch: 19, Batch number: 0, Loss: -0.29964759945869446\n",
      "Accuracy on validation dataset: 2820/5000 (56.40%)\n",
      "Accuracy on training dataset: 11104/20000 (55.52%)\n",
      "\n",
      "Epoch: 20, Batch number: 0, Loss: -1.6572473049163818\n",
      "Accuracy on validation dataset: 2815/5000 (56.30%)\n",
      "Accuracy on training dataset: 11072/20000 (55.36%)\n",
      "\n",
      "Epoch: 21, Batch number: 0, Loss: -1.1273150444030762\n",
      "Accuracy on validation dataset: 2817/5000 (56.34%)\n",
      "Accuracy on training dataset: 11089/20000 (55.45%)\n",
      "\n",
      "Epoch: 22, Batch number: 0, Loss: -1.7877657413482666\n",
      "Accuracy on validation dataset: 2824/5000 (56.48%)\n",
      "Accuracy on training dataset: 11127/20000 (55.63%)\n",
      "\n",
      "Epoch: 23, Batch number: 0, Loss: -3.212980270385742\n",
      "Accuracy on validation dataset: 2821/5000 (56.42%)\n",
      "Accuracy on training dataset: 11095/20000 (55.48%)\n",
      "\n",
      "Epoch: 24, Batch number: 0, Loss: -1.9959115982055664\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11134/20000 (55.67%)\n",
      "\n",
      "Epoch: 25, Batch number: 0, Loss: -0.2658688426017761\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11133/20000 (55.66%)\n",
      "\n",
      "Epoch: 26, Batch number: 0, Loss: -2.645481586456299\n",
      "Accuracy on validation dataset: 2824/5000 (56.48%)\n",
      "Accuracy on training dataset: 11132/20000 (55.66%)\n",
      "\n",
      "Epoch: 27, Batch number: 0, Loss: -2.1632392406463623\n",
      "Accuracy on validation dataset: 2828/5000 (56.56%)\n",
      "Accuracy on training dataset: 11140/20000 (55.70%)\n",
      "\n",
      "Epoch: 28, Batch number: 0, Loss: -1.313934564590454\n",
      "Accuracy on validation dataset: 2830/5000 (56.60%)\n",
      "Accuracy on training dataset: 11146/20000 (55.73%)\n",
      "\n",
      "Epoch: 29, Batch number: 0, Loss: -2.343421220779419\n",
      "Accuracy on validation dataset: 2827/5000 (56.54%)\n",
      "Accuracy on training dataset: 11150/20000 (55.75%)\n",
      "\n",
      "Epoch: 30, Batch number: 0, Loss: -1.8736892938613892\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11113/20000 (55.56%)\n",
      "\n",
      "Epoch: 31, Batch number: 0, Loss: -3.0798532962799072\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11127/20000 (55.63%)\n",
      "\n",
      "Epoch: 32, Batch number: 0, Loss: -4.257862091064453\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11114/20000 (55.57%)\n",
      "\n",
      "Epoch: 33, Batch number: 0, Loss: -2.120298147201538\n",
      "Accuracy on validation dataset: 2815/5000 (56.30%)\n",
      "Accuracy on training dataset: 11091/20000 (55.45%)\n",
      "\n",
      "Epoch: 34, Batch number: 0, Loss: -3.1774702072143555\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11151/20000 (55.76%)\n",
      "\n",
      "Epoch: 35, Batch number: 0, Loss: -1.9427452087402344\n",
      "Accuracy on validation dataset: 2825/5000 (56.50%)\n",
      "Accuracy on training dataset: 11142/20000 (55.71%)\n",
      "\n",
      "Epoch: 36, Batch number: 0, Loss: -4.884182929992676\n",
      "Accuracy on validation dataset: 2825/5000 (56.50%)\n",
      "Accuracy on training dataset: 11145/20000 (55.73%)\n",
      "\n",
      "Epoch: 37, Batch number: 0, Loss: -2.355991840362549\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11150/20000 (55.75%)\n",
      "\n",
      "Epoch: 38, Batch number: 0, Loss: -2.296626091003418\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11133/20000 (55.66%)\n",
      "\n",
      "Epoch: 39, Batch number: 0, Loss: -3.603098154067993\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11148/20000 (55.74%)\n",
      "\n",
      "Epoch: 40, Batch number: 0, Loss: -4.709354400634766\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11149/20000 (55.74%)\n",
      "\n",
      "Epoch: 41, Batch number: 0, Loss: -2.2236993312835693\n",
      "Accuracy on validation dataset: 2825/5000 (56.50%)\n",
      "Accuracy on training dataset: 11144/20000 (55.72%)\n",
      "\n",
      "Epoch: 42, Batch number: 0, Loss: -4.567106246948242\n",
      "Accuracy on validation dataset: 2825/5000 (56.50%)\n",
      "Accuracy on training dataset: 11157/20000 (55.78%)\n",
      "\n",
      "Epoch: 43, Batch number: 0, Loss: -2.5759987831115723\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11143/20000 (55.72%)\n",
      "\n",
      "Epoch: 44, Batch number: 0, Loss: -3.434969425201416\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11150/20000 (55.75%)\n",
      "\n",
      "Epoch: 45, Batch number: 0, Loss: -5.243112087249756\n",
      "Accuracy on validation dataset: 2827/5000 (56.54%)\n",
      "Accuracy on training dataset: 11164/20000 (55.82%)\n",
      "\n",
      "Epoch: 46, Batch number: 0, Loss: -4.160979270935059\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11154/20000 (55.77%)\n",
      "\n",
      "Epoch: 47, Batch number: 0, Loss: -5.461456298828125\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11151/20000 (55.76%)\n",
      "\n",
      "Epoch: 48, Batch number: 0, Loss: -5.0168304443359375\n",
      "Accuracy on validation dataset: 2815/5000 (56.30%)\n",
      "Accuracy on training dataset: 11111/20000 (55.55%)\n",
      "\n",
      "Epoch: 49, Batch number: 0, Loss: -5.504678726196289\n",
      "Accuracy on validation dataset: 2820/5000 (56.40%)\n",
      "Accuracy on training dataset: 11151/20000 (55.76%)\n",
      "\n",
      "Epoch: 50, Batch number: 0, Loss: -3.869520902633667\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11144/20000 (55.72%)\n",
      "\n",
      "Epoch: 51, Batch number: 0, Loss: -5.380892276763916\n",
      "Accuracy on validation dataset: 2823/5000 (56.46%)\n",
      "Accuracy on training dataset: 11166/20000 (55.83%)\n",
      "\n",
      "Epoch: 52, Batch number: 0, Loss: -3.86971378326416\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 11158/20000 (55.79%)\n",
      "\n",
      "Epoch: 53, Batch number: 0, Loss: -4.003573417663574\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11159/20000 (55.80%)\n",
      "\n",
      "Epoch: 54, Batch number: 0, Loss: -3.9952917098999023\n",
      "Accuracy on validation dataset: 2818/5000 (56.36%)\n",
      "Accuracy on training dataset: 11156/20000 (55.78%)\n",
      "\n",
      "Epoch: 55, Batch number: 0, Loss: -6.253493309020996\n",
      "Accuracy on validation dataset: 2817/5000 (56.34%)\n",
      "Accuracy on training dataset: 11134/20000 (55.67%)\n",
      "\n",
      "Epoch: 56, Batch number: 0, Loss: -5.852270126342773\n",
      "Accuracy on validation dataset: 2817/5000 (56.34%)\n",
      "Accuracy on training dataset: 11141/20000 (55.70%)\n",
      "\n",
      "Epoch: 57, Batch number: 0, Loss: -4.244622230529785\n",
      "Accuracy on validation dataset: 2815/5000 (56.30%)\n",
      "Accuracy on training dataset: 11143/20000 (55.72%)\n",
      "\n",
      "Epoch: 58, Batch number: 0, Loss: -6.939418792724609\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11159/20000 (55.80%)\n",
      "\n",
      "Epoch: 59, Batch number: 0, Loss: -5.395369052886963\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11150/20000 (55.75%)\n",
      "\n",
      "Epoch: 60, Batch number: 0, Loss: -4.782454490661621\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11151/20000 (55.76%)\n",
      "\n",
      "Epoch: 61, Batch number: 0, Loss: -4.20786714553833\n",
      "Accuracy on validation dataset: 2814/5000 (56.28%)\n",
      "Accuracy on training dataset: 11127/20000 (55.63%)\n",
      "\n",
      "Epoch: 62, Batch number: 0, Loss: -1.511182427406311\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11153/20000 (55.77%)\n",
      "\n",
      "Epoch: 63, Batch number: 0, Loss: -4.94550085067749\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11165/20000 (55.83%)\n",
      "\n",
      "Epoch: 64, Batch number: 0, Loss: -5.536791801452637\n",
      "Accuracy on validation dataset: 2825/5000 (56.50%)\n",
      "Accuracy on training dataset: 11173/20000 (55.87%)\n",
      "\n",
      "Epoch: 65, Batch number: 0, Loss: -6.15117073059082\n",
      "Accuracy on validation dataset: 2812/5000 (56.24%)\n",
      "Accuracy on training dataset: 11132/20000 (55.66%)\n",
      "\n",
      "Epoch: 66, Batch number: 0, Loss: -6.005308151245117\n",
      "Accuracy on validation dataset: 2816/5000 (56.32%)\n",
      "Accuracy on training dataset: 11142/20000 (55.71%)\n",
      "\n",
      "Epoch: 67, Batch number: 0, Loss: -6.828183650970459\n",
      "Accuracy on validation dataset: 2810/5000 (56.20%)\n",
      "Accuracy on training dataset: 11126/20000 (55.63%)\n",
      "\n",
      "Epoch: 68, Batch number: 0, Loss: -5.10382080078125\n",
      "Accuracy on validation dataset: 2816/5000 (56.32%)\n",
      "Accuracy on training dataset: 11142/20000 (55.71%)\n",
      "\n",
      "Epoch: 69, Batch number: 0, Loss: -5.290642261505127\n",
      "Accuracy on validation dataset: 2815/5000 (56.30%)\n",
      "Accuracy on training dataset: 11139/20000 (55.70%)\n",
      "\n",
      "Epoch: 70, Batch number: 0, Loss: -8.728625297546387\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11118/20000 (55.59%)\n",
      "\n",
      "Epoch: 71, Batch number: 0, Loss: -7.733748435974121\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11127/20000 (55.63%)\n",
      "\n",
      "Epoch: 72, Batch number: 0, Loss: -9.28969669342041\n",
      "Accuracy on validation dataset: 2806/5000 (56.12%)\n",
      "Accuracy on training dataset: 11120/20000 (55.60%)\n",
      "\n",
      "Epoch: 73, Batch number: 0, Loss: -7.972284317016602\n",
      "Accuracy on validation dataset: 2805/5000 (56.10%)\n",
      "Accuracy on training dataset: 11115/20000 (55.58%)\n",
      "\n",
      "Epoch: 74, Batch number: 0, Loss: -6.129477500915527\n",
      "Accuracy on validation dataset: 2805/5000 (56.10%)\n",
      "Accuracy on training dataset: 11118/20000 (55.59%)\n",
      "\n",
      "Epoch: 75, Batch number: 0, Loss: -7.018082618713379\n",
      "Accuracy on validation dataset: 2809/5000 (56.18%)\n",
      "Accuracy on training dataset: 11126/20000 (55.63%)\n",
      "\n",
      "Epoch: 76, Batch number: 0, Loss: -6.022342681884766\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11131/20000 (55.66%)\n",
      "\n",
      "Epoch: 77, Batch number: 0, Loss: -4.324767112731934\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11126/20000 (55.63%)\n",
      "\n",
      "Epoch: 78, Batch number: 0, Loss: -9.545323371887207\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11125/20000 (55.62%)\n",
      "\n",
      "Epoch: 79, Batch number: 0, Loss: -8.680150985717773\n",
      "Accuracy on validation dataset: 2804/5000 (56.08%)\n",
      "Accuracy on training dataset: 11121/20000 (55.60%)\n",
      "\n",
      "Epoch: 80, Batch number: 0, Loss: -9.250089645385742\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11127/20000 (55.63%)\n",
      "\n",
      "Epoch: 81, Batch number: 0, Loss: -8.099594116210938\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11127/20000 (55.63%)\n",
      "\n",
      "Epoch: 82, Batch number: 0, Loss: -10.378290176391602\n",
      "Accuracy on validation dataset: 2802/5000 (56.04%)\n",
      "Accuracy on training dataset: 11113/20000 (55.56%)\n",
      "\n",
      "Epoch: 83, Batch number: 0, Loss: -13.317201614379883\n",
      "Accuracy on validation dataset: 2802/5000 (56.04%)\n",
      "Accuracy on training dataset: 11110/20000 (55.55%)\n",
      "\n",
      "Epoch: 84, Batch number: 0, Loss: -6.958621978759766\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11119/20000 (55.59%)\n",
      "\n",
      "Epoch: 85, Batch number: 0, Loss: -3.165130138397217\n",
      "Accuracy on validation dataset: 2806/5000 (56.12%)\n",
      "Accuracy on training dataset: 11122/20000 (55.61%)\n",
      "\n",
      "Epoch: 86, Batch number: 0, Loss: -4.149637222290039\n",
      "Accuracy on validation dataset: 2803/5000 (56.06%)\n",
      "Accuracy on training dataset: 11122/20000 (55.61%)\n",
      "\n",
      "Epoch: 87, Batch number: 0, Loss: -5.354043960571289\n",
      "Accuracy on validation dataset: 2808/5000 (56.16%)\n",
      "Accuracy on training dataset: 11134/20000 (55.67%)\n",
      "\n",
      "Epoch: 88, Batch number: 0, Loss: -9.539375305175781\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11116/20000 (55.58%)\n",
      "\n",
      "Epoch: 89, Batch number: 0, Loss: -8.058160781860352\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11119/20000 (55.59%)\n",
      "\n",
      "Epoch: 90, Batch number: 0, Loss: -6.947601318359375\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11120/20000 (55.60%)\n",
      "\n",
      "Epoch: 91, Batch number: 0, Loss: -5.4433817863464355\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11117/20000 (55.59%)\n",
      "\n",
      "Epoch: 92, Batch number: 0, Loss: -9.217570304870605\n",
      "Accuracy on validation dataset: 2802/5000 (56.04%)\n",
      "Accuracy on training dataset: 11104/20000 (55.52%)\n",
      "\n",
      "Epoch: 93, Batch number: 0, Loss: -8.89522647857666\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11106/20000 (55.53%)\n",
      "\n",
      "Epoch: 94, Batch number: 0, Loss: -9.571593284606934\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11111/20000 (55.55%)\n",
      "\n",
      "Epoch: 95, Batch number: 0, Loss: -6.242730140686035\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11112/20000 (55.56%)\n",
      "\n",
      "Epoch: 96, Batch number: 0, Loss: -10.58833122253418\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11115/20000 (55.58%)\n",
      "\n",
      "Epoch: 97, Batch number: 0, Loss: -4.85727596282959\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11113/20000 (55.56%)\n",
      "\n",
      "Epoch: 98, Batch number: 0, Loss: -10.099958419799805\n",
      "Accuracy on validation dataset: 2802/5000 (56.04%)\n",
      "Accuracy on training dataset: 11122/20000 (55.61%)\n",
      "\n",
      "Epoch: 99, Batch number: 0, Loss: -10.722900390625\n",
      "Accuracy on validation dataset: 2801/5000 (56.02%)\n",
      "Accuracy on training dataset: 11119/20000 (55.59%)\n",
      "\n",
      "Epoch: 100, Batch number: 0, Loss: -6.279279708862305\n",
      "Accuracy on validation dataset: 2805/5000 (56.10%)\n",
      "Accuracy on training dataset: 11130/20000 (55.65%)\n",
      "\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entrenamiento:\n",
    "loss_fn = 'BCE'\n",
    "optim_algorithm = 'Adam'\n",
    "epochs = 100\n",
    "sample_loss_every = 20\n",
    "check_on_train = True\n",
    "learning_rate = 1e-3\n",
    "\n",
    "LRTrainer.train(loss_fn,optim_algorithm,epochs,sample_loss_every,check_on_train,lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigue dando underfitting. Vamos a ver si podemos aumentar la cantidad de parámetros ahora sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,h_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(vocab_size,h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "h_dim = 50000\n",
    "model2 = TwoLayerNet(vocab_size,h_dim)\n",
    "device = 'cpu'\n",
    "TwoLayerNetTrainer = Trainer(train_loader,dev_loader,model2,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Loss function: BCE\n",
      "Optimization method: Adam\n",
      "Learning Rate: 0.001\n",
      "Number of epochs: 100\n",
      "Running on device (cpu)\n",
      "\n",
      "Epoch: 1, Batch number: 0, Loss: 0.6697266697883606\n",
      "Accuracy on validation dataset: 2526/5000 (50.52%)\n",
      "Accuracy on training dataset: 9974/20000 (49.87%)\n",
      "\n",
      "Epoch: 2, Batch number: 0, Loss: -16.05726432800293\n",
      "Accuracy on validation dataset: 2736/5000 (54.72%)\n",
      "Accuracy on training dataset: 10805/20000 (54.02%)\n",
      "\n",
      "Epoch: 3, Batch number: 0, Loss: -171.23428344726562\n",
      "Accuracy on validation dataset: 2796/5000 (55.92%)\n",
      "Accuracy on training dataset: 11051/20000 (55.26%)\n",
      "\n",
      "Epoch: 4, Batch number: 0, Loss: -432.89459228515625\n",
      "Accuracy on validation dataset: 2800/5000 (56.00%)\n",
      "Accuracy on training dataset: 11035/20000 (55.17%)\n",
      "\n",
      "Epoch: 5, Batch number: 0, Loss: -941.2314453125\n",
      "Accuracy on validation dataset: 2849/5000 (56.98%)\n",
      "Accuracy on training dataset: 11234/20000 (56.17%)\n",
      "\n",
      "Epoch: 6, Batch number: 0, Loss: -1632.5648193359375\n",
      "Accuracy on validation dataset: 2797/5000 (55.94%)\n",
      "Accuracy on training dataset: 11000/20000 (55.00%)\n",
      "\n",
      "Epoch: 7, Batch number: 0, Loss: -2046.6234130859375\n",
      "Accuracy on validation dataset: 2841/5000 (56.82%)\n",
      "Accuracy on training dataset: 11210/20000 (56.05%)\n",
      "\n",
      "Epoch: 8, Batch number: 0, Loss: -4739.703125\n",
      "Accuracy on validation dataset: 2795/5000 (55.90%)\n",
      "Accuracy on training dataset: 10987/20000 (54.94%)\n",
      "\n",
      "Epoch: 9, Batch number: 0, Loss: -7088.6181640625\n",
      "Accuracy on validation dataset: 2829/5000 (56.58%)\n",
      "Accuracy on training dataset: 11176/20000 (55.88%)\n",
      "\n",
      "Epoch: 10, Batch number: 0, Loss: -5994.685546875\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11216/20000 (56.08%)\n",
      "\n",
      "Epoch: 11, Batch number: 0, Loss: -7981.1015625\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11144/20000 (55.72%)\n",
      "\n",
      "Epoch: 12, Batch number: 0, Loss: -20127.34765625\n",
      "Accuracy on validation dataset: 2814/5000 (56.28%)\n",
      "Accuracy on training dataset: 11091/20000 (55.45%)\n",
      "\n",
      "Epoch: 13, Batch number: 0, Loss: -14315.5888671875\n",
      "Accuracy on validation dataset: 2834/5000 (56.68%)\n",
      "Accuracy on training dataset: 11206/20000 (56.03%)\n",
      "\n",
      "Epoch: 14, Batch number: 0, Loss: -19273.66796875\n",
      "Accuracy on validation dataset: 2826/5000 (56.52%)\n",
      "Accuracy on training dataset: 11150/20000 (55.75%)\n",
      "\n",
      "Epoch: 15, Batch number: 0, Loss: -31201.740234375\n",
      "Accuracy on validation dataset: 2839/5000 (56.78%)\n",
      "Accuracy on training dataset: 11204/20000 (56.02%)\n",
      "\n",
      "Epoch: 16, Batch number: 0, Loss: -24980.2578125\n",
      "Accuracy on validation dataset: 2819/5000 (56.38%)\n",
      "Accuracy on training dataset: 11117/20000 (55.59%)\n",
      "\n",
      "Epoch: 17, Batch number: 0, Loss: -18437.416015625\n",
      "Accuracy on validation dataset: 2850/5000 (57.00%)\n",
      "Accuracy on training dataset: 11230/20000 (56.15%)\n",
      "\n",
      "Epoch: 18, Batch number: 0, Loss: -14488.33984375\n",
      "Accuracy on validation dataset: 2839/5000 (56.78%)\n",
      "Accuracy on training dataset: 11207/20000 (56.03%)\n",
      "\n",
      "Epoch: 19, Batch number: 0, Loss: -37852.58984375\n",
      "Accuracy on validation dataset: 2839/5000 (56.78%)\n",
      "Accuracy on training dataset: 11202/20000 (56.01%)\n",
      "\n",
      "Epoch: 20, Batch number: 0, Loss: -49732.5\n",
      "Accuracy on validation dataset: 2830/5000 (56.60%)\n",
      "Accuracy on training dataset: 11168/20000 (55.84%)\n",
      "\n",
      "Epoch: 21, Batch number: 0, Loss: -82110.484375\n",
      "Accuracy on validation dataset: 2845/5000 (56.90%)\n",
      "Accuracy on training dataset: 11206/20000 (56.03%)\n",
      "\n",
      "Epoch: 22, Batch number: 0, Loss: -31816.1484375\n",
      "Accuracy on validation dataset: 2843/5000 (56.86%)\n",
      "Accuracy on training dataset: 11202/20000 (56.01%)\n",
      "\n",
      "Epoch: 23, Batch number: 0, Loss: -67722.75\n",
      "Accuracy on validation dataset: 2851/5000 (57.02%)\n",
      "Accuracy on training dataset: 11231/20000 (56.16%)\n",
      "\n",
      "Epoch: 24, Batch number: 0, Loss: -47183.265625\n",
      "Accuracy on validation dataset: 2856/5000 (57.12%)\n",
      "Accuracy on training dataset: 11262/20000 (56.31%)\n",
      "\n",
      "Epoch: 25, Batch number: 0, Loss: -102339.703125\n",
      "Accuracy on validation dataset: 2836/5000 (56.72%)\n",
      "Accuracy on training dataset: 11180/20000 (55.90%)\n",
      "\n",
      "Epoch: 26, Batch number: 0, Loss: -48342.39453125\n",
      "Accuracy on validation dataset: 2848/5000 (56.96%)\n",
      "Accuracy on training dataset: 11216/20000 (56.08%)\n",
      "\n",
      "Epoch: 27, Batch number: 0, Loss: -75803.2890625\n",
      "Accuracy on validation dataset: 2851/5000 (57.02%)\n",
      "Accuracy on training dataset: 11218/20000 (56.09%)\n",
      "\n",
      "Epoch: 28, Batch number: 0, Loss: -100523.1796875\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11197/20000 (55.98%)\n",
      "\n",
      "Epoch: 29, Batch number: 0, Loss: -82346.9375\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11196/20000 (55.98%)\n",
      "\n",
      "Epoch: 30, Batch number: 0, Loss: -155119.21875\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11195/20000 (55.98%)\n",
      "\n",
      "Epoch: 31, Batch number: 0, Loss: -148283.75\n",
      "Accuracy on validation dataset: 2834/5000 (56.68%)\n",
      "Accuracy on training dataset: 11162/20000 (55.81%)\n",
      "\n",
      "Epoch: 32, Batch number: 0, Loss: -94909.296875\n",
      "Accuracy on validation dataset: 2848/5000 (56.96%)\n",
      "Accuracy on training dataset: 11209/20000 (56.05%)\n",
      "\n",
      "Epoch: 33, Batch number: 0, Loss: -92118.453125\n",
      "Accuracy on validation dataset: 2850/5000 (57.00%)\n",
      "Accuracy on training dataset: 11209/20000 (56.05%)\n",
      "\n",
      "Epoch: 34, Batch number: 0, Loss: -201387.3125\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11184/20000 (55.92%)\n",
      "\n",
      "Epoch: 35, Batch number: 0, Loss: -122485.3984375\n",
      "Accuracy on validation dataset: 2850/5000 (57.00%)\n",
      "Accuracy on training dataset: 11198/20000 (55.99%)\n",
      "\n",
      "Epoch: 36, Batch number: 0, Loss: -146459.375\n",
      "Accuracy on validation dataset: 2849/5000 (56.98%)\n",
      "Accuracy on training dataset: 11209/20000 (56.05%)\n",
      "\n",
      "Epoch: 37, Batch number: 0, Loss: -187085.421875\n",
      "Accuracy on validation dataset: 2850/5000 (57.00%)\n",
      "Accuracy on training dataset: 11217/20000 (56.09%)\n",
      "\n",
      "Epoch: 38, Batch number: 0, Loss: -111568.2578125\n",
      "Accuracy on validation dataset: 2849/5000 (56.98%)\n",
      "Accuracy on training dataset: 11201/20000 (56.01%)\n",
      "\n",
      "Epoch: 39, Batch number: 0, Loss: -149162.578125\n",
      "Accuracy on validation dataset: 2852/5000 (57.04%)\n",
      "Accuracy on training dataset: 11238/20000 (56.19%)\n",
      "\n",
      "Epoch: 40, Batch number: 0, Loss: -249887.328125\n",
      "Accuracy on validation dataset: 2836/5000 (56.72%)\n",
      "Accuracy on training dataset: 11170/20000 (55.85%)\n",
      "\n",
      "Epoch: 41, Batch number: 0, Loss: -130183.046875\n",
      "Accuracy on validation dataset: 2845/5000 (56.90%)\n",
      "Accuracy on training dataset: 11182/20000 (55.91%)\n",
      "\n",
      "Epoch: 42, Batch number: 0, Loss: -129604.53125\n",
      "Accuracy on validation dataset: 2845/5000 (56.90%)\n",
      "Accuracy on training dataset: 11191/20000 (55.95%)\n",
      "\n",
      "Epoch: 43, Batch number: 0, Loss: -160798.859375\n",
      "Accuracy on validation dataset: 2837/5000 (56.74%)\n",
      "Accuracy on training dataset: 11171/20000 (55.85%)\n",
      "\n",
      "Epoch: 44, Batch number: 0, Loss: -252349.203125\n",
      "Accuracy on validation dataset: 2840/5000 (56.80%)\n",
      "Accuracy on training dataset: 11181/20000 (55.91%)\n",
      "\n",
      "Epoch: 45, Batch number: 0, Loss: -207140.75\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11195/20000 (55.98%)\n",
      "\n",
      "Epoch: 46, Batch number: 0, Loss: -250796.609375\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11195/20000 (55.98%)\n",
      "\n",
      "Epoch: 47, Batch number: 0, Loss: -371659.75\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11199/20000 (55.99%)\n",
      "\n",
      "Epoch: 48, Batch number: 0, Loss: -345789.0625\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11204/20000 (56.02%)\n",
      "\n",
      "Epoch: 49, Batch number: 0, Loss: -199727.09375\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11191/20000 (55.95%)\n",
      "\n",
      "Epoch: 50, Batch number: 0, Loss: -260368.34375\n",
      "Accuracy on validation dataset: 2850/5000 (57.00%)\n",
      "Accuracy on training dataset: 11212/20000 (56.06%)\n",
      "\n",
      "Epoch: 51, Batch number: 0, Loss: -361699.875\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11193/20000 (55.97%)\n",
      "\n",
      "Epoch: 52, Batch number: 0, Loss: -439123.71875\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11199/20000 (55.99%)\n",
      "\n",
      "Epoch: 53, Batch number: 0, Loss: -438297.65625\n",
      "Accuracy on validation dataset: 2840/5000 (56.80%)\n",
      "Accuracy on training dataset: 11168/20000 (55.84%)\n",
      "\n",
      "Epoch: 54, Batch number: 0, Loss: -304092.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 2839/5000 (56.78%)\n",
      "Accuracy on training dataset: 11183/20000 (55.91%)\n",
      "\n",
      "Epoch: 55, Batch number: 0, Loss: -460320.25\n",
      "Accuracy on validation dataset: 2845/5000 (56.90%)\n",
      "Accuracy on training dataset: 11186/20000 (55.93%)\n",
      "\n",
      "Epoch: 56, Batch number: 0, Loss: -441035.0\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11204/20000 (56.02%)\n",
      "\n",
      "Epoch: 57, Batch number: 0, Loss: -270629.46875\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11206/20000 (56.03%)\n",
      "\n",
      "Epoch: 58, Batch number: 0, Loss: -249320.5\n",
      "Accuracy on validation dataset: 2849/5000 (56.98%)\n",
      "Accuracy on training dataset: 11219/20000 (56.09%)\n",
      "\n",
      "Epoch: 59, Batch number: 0, Loss: -447226.59375\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11199/20000 (55.99%)\n",
      "\n",
      "Epoch: 60, Batch number: 0, Loss: -305880.375\n",
      "Accuracy on validation dataset: 2843/5000 (56.86%)\n",
      "Accuracy on training dataset: 11177/20000 (55.88%)\n",
      "\n",
      "Epoch: 61, Batch number: 0, Loss: -500553.75\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11209/20000 (56.05%)\n",
      "\n",
      "Epoch: 62, Batch number: 0, Loss: -590905.9375\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11193/20000 (55.97%)\n",
      "\n",
      "Epoch: 63, Batch number: 0, Loss: -745995.625\n",
      "Accuracy on validation dataset: 2846/5000 (56.92%)\n",
      "Accuracy on training dataset: 11195/20000 (55.98%)\n",
      "\n",
      "Epoch: 64, Batch number: 0, Loss: -356769.0625\n",
      "Accuracy on validation dataset: 2847/5000 (56.94%)\n",
      "Accuracy on training dataset: 11201/20000 (56.01%)\n",
      "\n",
      "Epoch: 65, Batch number: 0, Loss: -465743.90625\n",
      "Accuracy on validation dataset: 2848/5000 (56.96%)\n",
      "Accuracy on training dataset: 11210/20000 (56.05%)\n",
      "\n",
      "Epoch: 66, Batch number: 0, Loss: -336959.46875\n",
      "Accuracy on validation dataset: 2843/5000 (56.86%)\n",
      "Accuracy on training dataset: 11175/20000 (55.88%)\n",
      "\n",
      "Epoch: 67, Batch number: 0, Loss: -367466.875\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11182/20000 (55.91%)\n",
      "\n",
      "Epoch: 68, Batch number: 0, Loss: -580201.125\n",
      "Accuracy on validation dataset: 2840/5000 (56.80%)\n",
      "Accuracy on training dataset: 11174/20000 (55.87%)\n",
      "\n",
      "Epoch: 69, Batch number: 0, Loss: -710332.0\n",
      "Accuracy on validation dataset: 2838/5000 (56.76%)\n",
      "Accuracy on training dataset: 11165/20000 (55.83%)\n",
      "\n",
      "Epoch: 70, Batch number: 0, Loss: -964124.125\n",
      "Accuracy on validation dataset: 2840/5000 (56.80%)\n",
      "Accuracy on training dataset: 11176/20000 (55.88%)\n",
      "\n",
      "Epoch: 71, Batch number: 0, Loss: -812886.3125\n",
      "Accuracy on validation dataset: 2843/5000 (56.86%)\n",
      "Accuracy on training dataset: 11179/20000 (55.90%)\n",
      "\n",
      "Epoch: 72, Batch number: 0, Loss: -1095831.375\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11193/20000 (55.97%)\n",
      "\n",
      "Epoch: 73, Batch number: 0, Loss: -537538.75\n",
      "Accuracy on validation dataset: 2840/5000 (56.80%)\n",
      "Accuracy on training dataset: 11182/20000 (55.91%)\n",
      "\n",
      "Epoch: 74, Batch number: 0, Loss: -372375.40625\n",
      "Accuracy on validation dataset: 2845/5000 (56.90%)\n",
      "Accuracy on training dataset: 11205/20000 (56.02%)\n",
      "\n",
      "Epoch: 75, Batch number: 0, Loss: -630822.5\n",
      "Accuracy on validation dataset: 2844/5000 (56.88%)\n",
      "Accuracy on training dataset: 11189/20000 (55.95%)\n",
      "\n",
      "Epoch: 76, Batch number: 0, Loss: -847817.0625\n",
      "Accuracy on validation dataset: 2839/5000 (56.78%)\n",
      "Accuracy on training dataset: 11174/20000 (55.87%)\n",
      "\n",
      "Epoch: 77, Batch number: 0, Loss: -456002.78125\n",
      "Accuracy on validation dataset: 2842/5000 (56.84%)\n",
      "Accuracy on training dataset: 11194/20000 (55.97%)\n",
      "\n",
      "Exiting training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entrenamiento:\n",
    "loss_fn = 'BCE'\n",
    "optim_algorithm = 'Adam'\n",
    "epochs = 100\n",
    "sample_loss_every = 20\n",
    "check_on_train = True\n",
    "learning_rate = 1e-3\n",
    "\n",
    "TwoLayerNetTrainer.train(loss_fn,optim_algorithm,epochs,sample_loss_every,check_on_train,lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2020-04-28 13:52:12.932628\n",
      "date and time = 28/04/2020 13:52:12\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(loss_history, train_acc=False, **kwargs):\n",
    "        \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].plot(loss_history['iter'],loss_history['loss'], **kwargs)\n",
    "    ax[0].set_title('Loss')\n",
    "\n",
    "    ax[1].plot(loss_history['iter'],loss_history['dev_acc'], label='Validation', **kwargs)\n",
    "    if train_acc:\n",
    "        ax[1].plot(loss_history['iter'],loss_history['train_acc'], label='Train', **kwargs)\n",
    "    \n",
    "    ax[1].set_title('Accuracy error')\n",
    "    ax[1].legend()\n",
    "    fig.savefig('./loss_history-{}.png'.format(now.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "\n",
    "loss_history = {'iter': [0,1,2,3,4,5],\n",
    "                'loss': [1, .5, .05, .01, .009, .008],\n",
    "                'dev_acc': [10, 20, 30, 40, 50, 50],\n",
    "                'train_acc': [13, 21, 32, 40, 52, 52]}\n",
    "\n",
    "plot_history(loss_history, train_acc=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
