{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0].split('BecaNLP')[0],'BecaNLP/Utils'))\n",
    "\n",
    "import NLPUtils as nlp\n",
    "from NLPUtils.datasets import tass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125 1125\n",
      "@myendlesshazza a. que puto mal escribo b. me sigo surrando help 3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA N\n",
      "581 581\n"
     ]
    }
   ],
   "source": [
    "lang = ['es']\n",
    "train_data = [(text, label) for text, label in tass.train_reader(lang=lang)]\n",
    "train_corpus, train_labels = zip(*train_data)\n",
    "dev_data = [(text, label) for text, label in tass.dev_reader(lang=lang)]\n",
    "dev_corpus, dev_labels = zip(*dev_data)\n",
    "print(len(train_corpus),len(train_labels))\n",
    "print(train_corpus[0], train_labels[0])\n",
    "print(len(dev_corpus),len(dev_labels))\n",
    "\n",
    "labels_dict = {label:idx for idx,label in enumerate(set(train_labels))}\n",
    "y_train = np.array([labels_dict[label] for label in train_labels])\n",
    "y_dev = np.array([labels_dict[label] for label in dev_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5709\n"
     ]
    }
   ],
   "source": [
    "from NLPUtils.datasets.utils import NgramCountVectorizer\n",
    "\n",
    "vectorizer = NgramCountVectorizer(token_pattern=r'\\b\\w+\\b',unk_token=None,\n",
    "             min_freq=1,max_freq=np.inf,ngram_range=(1,1),max_features=None)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "X_dev = vectorizer.transform(dev_corpus)\n",
    "\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones correctas: 311/581 (53.53%)\n"
     ]
    }
   ],
   "source": [
    "from NLPUtils.classifiers import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_predict = clf.predict(X_dev)\n",
    "\n",
    "n_correct = (y_dev == y_predict).sum()\n",
    "n_total = len(y_dev)\n",
    "print('Predicciones correctas: {}/{} ({:.2f}%)'.format(n_correct,n_total,n_correct/n_total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones correctas: 318/581 (54.73%)\n",
      "Predicciones correctas: 232/498 (46.59%)\n",
      "Predicciones correctas: 182/390 (46.67%)\n",
      "Predicciones correctas: 242/486 (49.79%)\n",
      "Predicciones correctas: 294/510 (57.65%)\n"
     ]
    }
   ],
   "source": [
    "def NaiveBayesTASSPredict(lang=['es'],**kwargs):\n",
    "    train_data = [(text, label) for text, label in tass.train_reader(lang=lang)]\n",
    "    train_corpus, train_labels = zip(*train_data)\n",
    "    dev_data = [(text, label) for text, label in tass.dev_reader(lang=lang)]\n",
    "    dev_corpus, dev_labels = zip(*dev_data)\n",
    "    \n",
    "    labels_dict = {label:idx for idx,label in enumerate(set(train_labels))}\n",
    "    y_train = np.array([labels_dict[label] for label in train_labels])\n",
    "    y_dev = np.array([labels_dict[label] for label in dev_labels])\n",
    "    \n",
    "    vectorizer = NgramCountVectorizer(**kwargs)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_corpus)\n",
    "    X_dev = vectorizer.transform(dev_corpus)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_predict = clf.predict(X_dev)\n",
    "\n",
    "    n_correct = (y_dev == y_predict).sum()\n",
    "    n_total = len(y_dev)\n",
    "    print('Predicciones correctas: {}/{} ({:.2f}%)'.format(n_correct,n_total,n_correct/n_total*100))\n",
    "    \n",
    "\n",
    "token_pattern = r'\\b\\w+\\b'\n",
    "unk_token = 'UNK'\n",
    "min_freq, max_freq = 2, np.inf\n",
    "ngram_range = (1,3)\n",
    "max_features = None\n",
    "\n",
    "for lang in [['es'],['pe'],['cr'],['uy'],['mx']]:\n",
    "    NaiveBayesTASSPredict(lang=lang,token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones correctas: 295/581 (50.77%)\n",
      "Predicciones correctas: 228/498 (45.78%)\n",
      "Predicciones correctas: 163/390 (41.79%)\n",
      "Predicciones correctas: 260/486 (53.50%)\n",
      "Predicciones correctas: 300/510 (58.82%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "def SVMTASSPredict(lang=['es'],classifier='svc',**kwargs):\n",
    "    if classifier == 'svc':\n",
    "        clf = SVC(max_iter=3000)\n",
    "    elif classifier == 'linear':\n",
    "        clf = LinearSVC(max_iter=3000)\n",
    "    \n",
    "    train_data = [(text, label) for text, label in tass.train_reader(lang=lang)]\n",
    "    train_corpus, train_labels = zip(*train_data)\n",
    "    dev_data = [(text, label) for text, label in tass.dev_reader(lang=lang)]\n",
    "    dev_corpus, dev_labels = zip(*dev_data)\n",
    "    \n",
    "    labels_dict = {label:idx for idx,label in enumerate(set(train_labels))}\n",
    "    y_train = np.array([labels_dict[label] for label in train_labels])\n",
    "    y_dev = np.array([labels_dict[label] for label in dev_labels])\n",
    "    \n",
    "    vectorizer = NgramCountVectorizer(**kwargs)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_corpus)\n",
    "    X_dev = vectorizer.transform(dev_corpus)\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    y_predict = clf.predict(X_dev)\n",
    "\n",
    "    n_correct = (y_dev == y_predict).sum()\n",
    "    n_total = len(y_dev)\n",
    "    print('Predicciones correctas: {}/{} ({:.2f}%)'.format(n_correct,n_total,n_correct/n_total*100))\n",
    "    \n",
    "\n",
    "token_pattern = r'\\b\\w+\\b'\n",
    "unk_token = 'UNK'\n",
    "min_freq, max_freq = 1, np.inf\n",
    "ngram_range = (1,3)\n",
    "max_features = None\n",
    "\n",
    "for lang in [['es'],['pe'],['cr'],['uy'],['mx']]:\n",
    "    SVMTASSPredict(lang=lang,classifier='linear',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb3957d34b4da195afe28ca8a596f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242120.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220d12f0ad1d4c87b3390f02ac7e3ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06baad048389437889bcc320aafdd49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068004cbc40b4af589f62801a9bc542f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=43.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: es\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 308/581 (53.01%)\n",
      "SVC:\n",
      "Predicciones correctas: 307/581 (52.84%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 294/581 (50.60%)\n",
      "\n",
      "Language: pe\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 212/498 (42.57%)\n",
      "SVC:\n",
      "Predicciones correctas: 231/498 (46.39%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 196/498 (39.36%)\n",
      "\n",
      "Language: cr\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 180/390 (46.15%)\n",
      "SVC:\n",
      "Predicciones correctas: 172/390 (44.10%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 158/390 (40.51%)\n",
      "\n",
      "Language: uy\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 255/486 (52.47%)\n",
      "SVC:\n",
      "Predicciones correctas: 256/486 (52.67%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 240/486 (49.38%)\n",
      "\n",
      "Language: mx\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 305/510 (59.80%)\n",
      "SVC:\n",
      "Predicciones correctas: 290/510 (56.86%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 292/510 (57.25%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "token_pattern = tokenizer.tokenize\n",
    "unk_token = None\n",
    "min_freq, max_freq = 1, np.inf\n",
    "ngram_range = (1,1)\n",
    "max_features = None\n",
    "\n",
    "for lang in ['es','pe','cr','uy','mx']:\n",
    "    print('Language:',lang)\n",
    "    print('Naive Bayes:')\n",
    "    NaiveBayesTASSPredict(lang=[lang],token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print('SVC:')\n",
    "    SVMTASSPredict(lang=[lang],classifier='svc',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print('LinearSVC:')\n",
    "    SVMTASSPredict(lang=[lang],classifier='linear',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: es\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 301/581 (51.81%)\n",
      "SVC:\n",
      "Predicciones correctas: 297/581 (51.12%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 294/581 (50.60%)\n",
      "\n",
      "Language: pe\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 205/498 (41.16%)\n",
      "SVC:\n",
      "Predicciones correctas: 223/498 (44.78%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 196/498 (39.36%)\n",
      "\n",
      "Language: cr\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 178/390 (45.64%)\n",
      "SVC:\n",
      "Predicciones correctas: 170/390 (43.59%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 158/390 (40.51%)\n",
      "\n",
      "Language: uy\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 254/486 (52.26%)\n",
      "SVC:\n",
      "Predicciones correctas: 252/486 (51.85%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 240/486 (49.38%)\n",
      "\n",
      "Language: mx\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 300/510 (58.82%)\n",
      "SVC:\n",
      "Predicciones correctas: 286/510 (56.08%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 292/510 (57.25%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_pattern = tokenizer.tokenize\n",
    "unk_token = 'UNK'\n",
    "min_freq, max_freq = 1, np.inf\n",
    "ngram_range = (1,1)\n",
    "max_features = None\n",
    "\n",
    "for lang in ['es','pe','cr','uy','mx']:\n",
    "    print('Language:',lang)\n",
    "    print('Naive Bayes:')\n",
    "    NaiveBayesTASSPredict(lang=[lang],token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print('SVC:')\n",
    "    SVMTASSPredict(lang=[lang],classifier='svc',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print('LinearSVC:')\n",
    "    SVMTASSPredict(lang=[lang],classifier='linear',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: es\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 311/581 (53.53%)\n",
      "SVC:\n",
      "Predicciones correctas: 309/581 (53.18%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 306/581 (52.67%)\n",
      "\n",
      "Language: pe\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 222/498 (44.58%)\n",
      "SVC:\n",
      "Predicciones correctas: 227/498 (45.58%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 208/498 (41.77%)\n",
      "\n",
      "Language: cr\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 176/390 (45.13%)\n",
      "SVC:\n",
      "Predicciones correctas: 164/390 (42.05%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 166/390 (42.56%)\n",
      "\n",
      "Language: uy\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 252/486 (51.85%)\n",
      "SVC:\n",
      "Predicciones correctas: 254/486 (52.26%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 244/486 (50.21%)\n",
      "\n",
      "Language: mx\n",
      "Naive Bayes:\n",
      "Predicciones correctas: 298/510 (58.43%)\n",
      "SVC:\n",
      "Predicciones correctas: 285/510 (55.88%)\n",
      "LinearSVC:\n",
      "Predicciones correctas: 297/510 (58.24%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer1 = TweetTokenizer()\n",
    "tokenizer2 = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "def token_pattern(tweet):\n",
    "    return tokenizer2.tokenize(' '.join(tokenizer1.tokenize(tweet)))\n",
    "    \n",
    "\n",
    "#token_pattern = tokenizer.tokenize\n",
    "unk_token = None\n",
    "min_freq, max_freq = 1, np.inf\n",
    "ngram_range = (1,2)\n",
    "max_features = None\n",
    "\n",
    "for lang in ['es','pe','cr','uy','mx']:\n",
    "    print('Language:',lang)\n",
    "    print('Naive Bayes:')\n",
    "    NaiveBayesTASSPredict(lang=[lang],token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print('SVC:')\n",
    "    SVMTASSPredict(lang=[lang],classifier='svc',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print('LinearSVC:')\n",
    "    SVMTASSPredict(lang=[lang],classifier='linear',token_pattern=token_pattern,unk_token=unk_token,\n",
    "    min_freq=min_freq,max_freq=max_freq,ngram_range=ngram_range,max_features=max_features)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
