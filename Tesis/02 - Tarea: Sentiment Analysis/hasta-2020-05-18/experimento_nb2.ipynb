{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0].split('BecaNLP')[0],'BecaNLP/Utils'))\n",
    "\n",
    "import NLPUtils as nlp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1, 2: 1, 3: 1}\n",
      "{'Esto': 0, 'es': 1, 'una': 2, 'prueba': 3}\n",
      "{4: 1, 5: 1}\n",
      "{'Esto': 0, 'es': 1, 'una': 2, 'prueba': 3, 'esto': 4, 'también': 5}\n",
      "{0: 1, 1: 1, 2: 1, 3: 1, 6: 1, 7: 1, 4: 1, 5: 1}\n",
      "{'Esto': 0, 'es': 1, 'una': 2, 'prueba': 3, 'esto': 4, 'también': 5, 'más': 6, 'y': 7}\n",
      "{4: 1, 1: 1, 8: 1, 9: 1, 3: 1}\n",
      "{'Esto': 0, 'es': 1, 'una': 2, 'prueba': 3, 'esto': 4, 'también': 5, 'más': 6, 'y': 7, 'la': 8, 'última': 9}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='\\\\b\\\\w+\\\\b', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NLPUtils.datasets import imdb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from NLPUtils.datasets.utils import NgramTextVectorizer\n",
    "import numpy as np\n",
    "\n",
    "train_corpus = ['Esto es una prueba',\n",
    "          'esto también',\n",
    "          'Esto es una prueba más, y esto también',\n",
    "          'esto es la última prueba.']\n",
    "\n",
    "dev_corpus = ['Esto es una prueba para dev',\n",
    "              'esto también',\n",
    "              'esto ya no importa pero igual']\n",
    "\n",
    "token_pattern = r'\\b\\w+\\b'\n",
    "ngram_range = (1,1)\n",
    "vocabulary = None #['esto', 'es', 'prueba','igual']\n",
    "unk_token = None\n",
    "vec1 = NgramTextVectorizer(token_pattern=token_pattern,ngram_range=ngram_range,vocabulary=vocabulary,unk_token=unk_token)\n",
    "vec2 = CountVectorizer(lowercase=False,token_pattern=token_pattern,ngram_range=ngram_range,vocabulary=vocabulary)\n",
    "\n",
    "vec1.fit(train_corpus)\n",
    "vec2.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec1.vocabulary)\n",
    "print()\n",
    "print(vec2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'esto': 0, 'es': 1, 'prueba': 2, 'igual': 3, None: 4}\n",
      "{'esto': 0, 'es': 1, 'prueba': 2}\n",
      "\n",
      "{'esto': 0, 'es': 1, 'prueba': 2, 'igual': 3}\n",
      "[[0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "[[0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X1 = vec1.transform(dev_corpus)\n",
    "X2 = vec2.transform(dev_corpus)\n",
    "indeces = [vec2.vocabulary_[tk if isinstance(tk,str) else ' '.join(tk)] for tk in vec1.vocabulary]\n",
    "X2 = X2[:,indeces]\n",
    "\n",
    "print(X1.toarray())\n",
    "print(X2.toarray())\n",
    "print(np.all(X1 != X2).sum() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todas las posibilidades:\n",
    "\n",
    "## Vocabulario abierto, unk_token = UNK\n",
    "\n",
    "En train:\n",
    "```\n",
    "inicializo el vocabulario en cero\n",
    "Para cada texto:\n",
    "    tokenizo ngramas. Para cada token:\n",
    "        si pertenece al vocabulario, obtengo el índice.\n",
    "        si no, le asigno un nuevo índice y lo agrego al final del vocabulario\n",
    "    agrego la cuenta de todos los tokens a la fila de la matriz\n",
    "agrego un token UNK (columna de ceros en la matriz y último índice en el vocab)\n",
    "me fijo qué columnas suman entre max_freq y min_freq, las elimino y sumo su contenido a columna UNK\n",
    "```\n",
    "\n",
    "En dev:\n",
    "```\n",
    "para cada texto:\n",
    "    tokenizo ngramas. Para cada token:\n",
    "        si pertenece al vocabulario devuelvo vocab[tk]\n",
    "        si no, devuelvo vocab[unk_token]\n",
    "    agrego la cuenta de todos los tokens a la fila de la matriz        \n",
    "```\n",
    "\n",
    "## Vocabulario cerrado, unk_token = UNK\n",
    "\n",
    "En train:\n",
    "```\n",
    "inicializo el vocabulario en cero\n",
    "Para cada texto:\n",
    "    tokenizo ngramas. Para cada token:\n",
    "        si pertenece al vocabulario, obtengo el índice.\n",
    "        si no, le asigno un nuevo índice y lo agrego al final del vocabulario\n",
    "    agrego la cuenta de todos los tokens a la fila de la matriz\n",
    "agrego un token UNK (columna de ceros en la matriz y último índice en el vocab)\n",
    "me fijo qué columnas suman entre max_freq y min_freq, las elimino y sumo su contenido a columna UNK\n",
    "también me fijo qué columnas de las que quedaron son palabras del vocabulario, y las que no los son también las elimino y las sumo a la columna UNK\n",
    "```\n",
    "\n",
    "En dev:\n",
    "```\n",
    "para cada texto:\n",
    "    tokenizo ngramas. Para cada token:\n",
    "        si pertenece al vocabulario devuelvo vocab[tk]\n",
    "        si no, devuelvo vocab[unk_token]\n",
    "    agrego la cuenta de todos los tokens a la fila de la matriz        \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 0 1 0 0 0 1 0]]\n",
      "{'Esto': 0, 'es': 1, 'una': 7, 'prueba': 5, 'esto': 2, 'también': 6, 'más': 4, 'y': 8, 'la': 3, 'última': 9, 'UNK': 10}\n",
      "[1, 2, 5]\n",
      "[[1 0 0 0 1 0 0 2]\n",
      " [0 0 0 1 0 0 0 1]\n",
      " [1 0 1 1 1 1 0 3]\n",
      " [0 1 0 0 0 0 1 3]]\n",
      "{'Esto': 0, 'prueba': 1, 'esto': 2, 'más': 3, 'y': 4, 'la': 5, 'última': 6, 'UNK': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lestien/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:112: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "train_corpus = ['Esto es una prueba',\n",
    "          'esto también',\n",
    "          'Esto es una prueba más, y esto también',\n",
    "          'esto es la última prueba.']\n",
    "\n",
    "vec = CountVectorizer(lowercase=False,token_pattern=token_pattern,ngram_range=(1,1),vocabulary=None)\n",
    "\n",
    "X = vec.fit_transform(train_corpus)\n",
    "X = csr_matrix((X.data,X.indices,X.indptr),shape=(X.shape[0],X.shape[1]+1))\n",
    "vocab = vec.vocabulary_\n",
    "vocab['UNK'] = len(vocab)\n",
    "print(X.toarray())\n",
    "print(vocab)\n",
    "\n",
    "def get_columns_to_remove(X,min_freq,max_freq):\n",
    "    sums = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "    x1 = sums >= min_freq\n",
    "    x2 = sums <= max_freq\n",
    "    ands = np.logical_not(np.logical_and(x1,x2))\n",
    "    remove_items = list(np.where(ands)[0])\n",
    "    return remove_items\n",
    "\n",
    "def remove_tokens(vocab,X,remove_tokens,unk_idx=-1):\n",
    "    \"\"\"\n",
    "    elimina los tokens de un vocabulario ordenado \n",
    "    \"\"\"\n",
    "    X[:,unk_idx] = X[:,remove_tokens].sum(axis=1)\n",
    "    true_items = sorted(list(set(range(len(vocab))) - set(remove_tokens)))\n",
    "    X = X[:,true_items]\n",
    "    tokens = list(vocab.keys())\n",
    "    vocab = {tokens[i]:idx for idx, i in enumerate(true_items)}\n",
    "    return X, vocab\n",
    "    \n",
    "tokens_to_remove = get_columns_to_remove(X,0,2)\n",
    "print(tokens_to_remove)\n",
    "X, vocab = remove_tokens(vocab,X,tokens_to_remove,unk_idx=-1)\n",
    "print(X.toarray())\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'es': 0, 'esto': 1, 'esto es': 2, 'prueba': 3, 'igual': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "class NgramTextVectorizer(CountVectorizer):\n",
    "    \n",
    "    def __init__(self,token_pattern=r'\\b\\w+\\b',vocabulary=None,unk_token=None,\n",
    "         min_freq=1,max_freq=np.inf,ngram_range=(1,1)):\n",
    "        \n",
    "        super().__init__(lowercase=False,token_pattern=token_pattern,vocabulary=None,ngram_range=ngram_range)\n",
    "\n",
    "        self.unk_token = unk_token\n",
    "        self.min_freq = min_freq\n",
    "        self.max_freq = max_freq\n",
    "        \n",
    "        if vocabulary is None or isinstance(vocabulary,list) or isinstance(vocabulary,set) or hasattr(vocabulary,'__iter__'):\n",
    "            self.vocab = vocabulary\n",
    "        elif isinstance(vocabulary,dict):\n",
    "            self.vocab = list(vocabulary.keys())\n",
    "        else:\n",
    "            raise TypeError('El vocabulario tiene que ser un iterable de palabras')\n",
    "        \n",
    "    def fit_transform(self,corpus):\n",
    "        X = super().fit_transform(corpus)\n",
    "\n",
    "        vocab = self.vocab\n",
    "        if vocab is not None:\n",
    "            n_appended = 0\n",
    "            for tk in vocab:\n",
    "                if tk not in self.vocabulary_:\n",
    "                    self.vocabulary_[tk] = len(self.vocabulary_)\n",
    "                    n_appended += 1\n",
    "            X.resize((X.shape[0],X.shape[1]+n_appended))\n",
    "        \n",
    "        remove_items = self.get_columns_to_remove(X,self.min_freq,self.max_freq)\n",
    "        if vocab is not None:\n",
    "            stay_items = [self.vocabulary_[tk] for tk in self.vocabulary_.keys() if all([sub_tk in vocab for sub_tk in tk.split(' ')])]\n",
    "            remove_items = np.unique(np.hstack((remove_items,\n",
    "                  list(set(range(len(self.vocabulary_))) - set(stay_items)))))\n",
    "        \n",
    "        unk_token = self.unk_token\n",
    "        if unk_token is not None:\n",
    "            self.vocabulary_[unk_token] = len(self.vocabulary_)\n",
    "            X.resize(X.shape[0],X.shape[1]+1)\n",
    "        \n",
    "        X = self.remove_tokens(X,remove_items)\n",
    "        return X\n",
    "        \n",
    "    def transform(self,corpus):\n",
    "        \n",
    "        unk_token = self.unk_token\n",
    "        vocab = self.vocabulary_\n",
    "        \n",
    "        indptr = [0]\n",
    "        j_indices = []\n",
    "        values = []\n",
    "        \n",
    "        tokenizer = self.build_analyzer()\n",
    "        if unk_token is None:\n",
    "            for doc in corpus:\n",
    "                feature_counter = {}\n",
    "                for feature in tokenizer(doc):\n",
    "                    try:\n",
    "                        feature_idx = vocab[feature]\n",
    "                        if feature_idx not in feature_counter:\n",
    "                            feature_counter[feature_idx] = 1\n",
    "                        else:\n",
    "                            feature_counter[feature_idx] += 1\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "                \n",
    "        else:\n",
    "            for doc in corpus:\n",
    "                feature_counter = {}\n",
    "                for feature in tokenizer(doc):\n",
    "                    try:\n",
    "                        feature_idx = vocab[feature]\n",
    "                    except KeyError:\n",
    "                        feature_idx = vocab[unk_token]\n",
    "\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "\n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "            \n",
    "        j_indices = np.asarray(j_indices, dtype=self.dtype)\n",
    "        indptr = np.asarray(indptr, dtype=self.dtype)\n",
    "        values = np.asarray(values, dtype=self.dtype)\n",
    "        \n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocab)),\n",
    "                          dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "            \n",
    "        return X\n",
    "        \n",
    "    def fit(self,corpus):\n",
    "        self.fit_transform(corpus)\n",
    "        return self\n",
    "        \n",
    "    def get_columns_to_remove(self,X,min_freq,max_freq):\n",
    "        sums = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "        x1 = sums >= min_freq\n",
    "        x2 = sums <= max_freq\n",
    "        ands = np.logical_not(np.logical_and(x1,x2))\n",
    "        remove_items = np.where(ands)[0]\n",
    "        return remove_items\n",
    "\n",
    "    def remove_tokens(self,X,remove_tokens):\n",
    "        if self.unk_token is not None:\n",
    "            X = X.tolil()\n",
    "            X[:,-1] = X[:,remove_tokens].sum(axis=1)\n",
    "            X = X.tocsr()\n",
    "        true_items = sorted(list(set(range(len(self.vocabulary_))) - set(remove_tokens)))\n",
    "        X = X[:,true_items]\n",
    "        \n",
    "        sorted_idx = np.argsort(list(self.vocabulary_.values()))\n",
    "        tokens = list(self.vocabulary_.keys())\n",
    "        tokens = [tokens[i] for i in sorted_idx]\n",
    "        self.vocabulary_ = {tokens[i]:idx for idx, i in enumerate(true_items)}\n",
    "        return X\n",
    "\n",
    "train_corpus = ['Esto es una prueba',\n",
    "          'esto también',\n",
    "          'Esto es una prueba más, y esto también',\n",
    "          'esto es la última prueba.']\n",
    "\n",
    "dev_corpus = ['Esto es una prueba para dev',\n",
    "              'esto también',\n",
    "              'esto ya no importa pero igual']\n",
    "\n",
    "token_pattern = r'\\b\\w+\\b'\n",
    "ngram_range = (1,2)\n",
    "vocabulary = ['esto', 'es', 'prueba','igual']\n",
    "unk_token = None#'UNK'\n",
    "vec1 = NgramTextVectorizer(token_pattern=token_pattern,ngram_range=ngram_range,\n",
    "                           vocabulary=vocabulary,unk_token=unk_token,min_freq=0,max_freq=np.inf)\n",
    "\n",
    "vec1.fit(train_corpus)\n",
    "print(vec1.vocabulary_)\n",
    "X = vec1.transform(dev_corpus)\n",
    "X.toarray()\n",
    "\n",
    "#print(vec1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "70.49143838882446\n",
      "77.25798797607422\n"
     ]
    }
   ],
   "source": [
    "train_corpus = imdb.train_reader()['comment']\n",
    "dev_corpus = imdb.test_reader()['comment'].sample(n=5000,replace=False,random_state=0)\n",
    "\n",
    "token_pattern = r'\\b\\w+\\b'\n",
    "ngram_range = (1,2)\n",
    "vocabulary = None#['esto', 'es', 'prueba','igual']\n",
    "unk_token = 'UNK'\n",
    "vec1 = NgramTextVectorizer(token_pattern=token_pattern,ngram_range=ngram_range,\n",
    "                           vocabulary=vocabulary,unk_token=unk_token,min_freq=0,max_freq=np.inf)\n",
    "\n",
    "tic = time.time()\n",
    "print('start')\n",
    "vec1.fit(train_corpus)\n",
    "toc1 = time.time()\n",
    "print(toc1-tic)\n",
    "X = vec1.transform(dev_corpus)\n",
    "toc2 = time.time()\n",
    "print(toc2-tic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
