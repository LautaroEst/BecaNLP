{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0].split('BecaNLP')[0],'BecaNLP/Utils'))\n",
    "\n",
    "import NLPUtils as nlp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLPUtils.datasets import imdb\n",
    "\n",
    "#df = imdb.train_reader().sample(n=100,replace=False,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cualquier tarea de NLP tengo que definir antes que nada un vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'esto': 2, 'es': 0, 'prueba': 4, 'esto es': 3, 'es una': 1}\n",
      "{'esto': 2, 'es': 0, 'prueba': 4, 'esto es': 3, 'es una': 1, '<UNK>': 5}\n",
      "[[1 1 1 1 1 0]\n",
      " [1 0 1 1 1 0]\n",
      " [1 1 1 1 1 0]]\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 1 1 1 0]\n",
      " [1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['esto es una prueba',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas']\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False,max_features=5,ngram_range=(1,2),token_pattern=r'[<>\\w]+')\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "vectorizer.vocabulary_['<UNK>'] = len(vectorizer.vocabulary_)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.transform(corpus).toarray())\n",
    "corpus2 = ['esto es una prueba <UNK>',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas']\n",
    "print(vectorizer.transform(corpus2).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esto',\n",
       " 'es',\n",
       " 'una',\n",
       " 'prueba',\n",
       " '<UNK>',\n",
       " 'y',\n",
       " 'esto',\n",
       " 'es',\n",
       " 'una',\n",
       " 'prueba',\n",
       " 'también',\n",
       " 'esto es',\n",
       " 'es una',\n",
       " 'una prueba',\n",
       " 'prueba <UNK>',\n",
       " '<UNK> y',\n",
       " 'y esto',\n",
       " 'esto es',\n",
       " 'es una',\n",
       " 'una prueba',\n",
       " 'prueba también']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an = vectorizer.build_analyzer()\n",
    "an('esto es una prueba <UNK> y esto es una prueba también')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<esto> ', 'es ', 'una ', 'prueba']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall(r'\\s*[<\\w>]+\\s*','<esto> es una prueba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import array\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import numbers\n",
    "\n",
    "def _make_int_array():\n",
    "    \"\"\"Construct an array.array of a type suitable for scipy.sparse indices.\"\"\"\n",
    "    return array.array(str(\"i\"))\n",
    "\n",
    "\n",
    "class myCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):\n",
    "        try:\n",
    "            self.unk_token = kwargs['unk_token']\n",
    "            kwargs.pop('unk_token')\n",
    "        except KeyError:\n",
    "            self.unk_token = None\n",
    "        super().__init__(*args,**kwargs)\n",
    "    \n",
    "    def _count_vocab(self, raw_documents, fixed_vocab, unk_token):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n",
    "        \"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        \n",
    "        if unk_token is not None:\n",
    "            for doc in raw_documents:\n",
    "                feature_counter = {}\n",
    "                for feature in analyze(doc):\n",
    "                    try:\n",
    "                        feature_idx = vocabulary[feature]\n",
    "                    except KeyError:\n",
    "                        feature_idx = vocabulary[unk_token]\n",
    "                        \n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                        \n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "        \n",
    "        else:\n",
    "            for doc in raw_documents:\n",
    "                feature_counter = {}\n",
    "                for feature in analyze(doc):\n",
    "                    try:\n",
    "                        feature_idx = vocabulary[feature]\n",
    "                        if feature_idx not in feature_counter:\n",
    "                            feature_counter[feature_idx] = 1\n",
    "                        else:\n",
    "                            feature_counter[feature_idx] += 1\n",
    "                    except KeyError:\n",
    "                        # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                        continue\n",
    "    \n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
    "                                 \" contain stop words\")\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "            if _IS_32BIT:\n",
    "                raise ValueError(('sparse CSR array has {} non-zero '\n",
    "                                  'elements and requires 64 bit indexing, '\n",
    "                                  'which is unsupported with 32 bit Python.')\n",
    "                                 .format(indptr[-1]))\n",
    "            indices_dtype = np.int64\n",
    "\n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocabulary)),\n",
    "                          dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn the vocabulary dictionary and return document-term matrix.\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array of shape (n_samples, n_features)\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        # We intentionally don't call the transform method to make\n",
    "        # fit_transform overridable without unwanted side effects in\n",
    "        # TfidfVectorizer.\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        self._validate_params()\n",
    "        self._validate_vocabulary()\n",
    "        max_df = self.max_df\n",
    "        min_df = self.min_df\n",
    "        max_features = self.max_features\n",
    "\n",
    "        vocabulary, X = self._count_vocab(raw_documents,\n",
    "                                          self.fixed_vocabulary_,self.unk_token)\n",
    "\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "\n",
    "        if not self.fixed_vocabulary_:\n",
    "            n_doc = X.shape[0]\n",
    "            max_doc_count = (max_df\n",
    "                             if isinstance(max_df, numbers.Integral)\n",
    "                             else max_df * n_doc)\n",
    "            min_doc_count = (min_df\n",
    "                             if isinstance(min_df, numbers.Integral)\n",
    "                             else min_df * n_doc)\n",
    "            if max_doc_count < min_doc_count:\n",
    "                raise ValueError(\n",
    "                    \"max_df corresponds to < documents than min_df\")\n",
    "            X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
    "                                                       max_doc_count,\n",
    "                                                       min_doc_count,\n",
    "                                                       max_features)\n",
    "\n",
    "            X = self._sort_features(X, vocabulary)\n",
    "\n",
    "            self.vocabulary_ = vocabulary\n",
    "\n",
    "        return X\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "        Extract token counts out of raw text documents using the vocabulary\n",
    "        fitted with fit or the one provided to the constructor.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix of shape (n_samples, n_features)\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "        self._check_vocabulary()\n",
    "\n",
    "        # use the same matrix-building strategy as fit_transform\n",
    "        _, X = self._count_vocab(raw_documents, fixed_vocab=True, unk_token=self.unk_token)\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'esto': 0, 'es': 1, 'UNK': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  5],\n",
       "       [ 1,  1,  5],\n",
       "       [ 2,  2, 13]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['esto es una prueba',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas y esto es otra']\n",
    "\n",
    "vectorizer = myCountVectorizer(lowercase=False,max_features=5,ngram_range=(1,2),\n",
    "               token_pattern=r'\\b\\w+\\b',unk_token='UNK',vocabulary=['esto','es','UNK'])\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(corpus)\n",
    "X.toarray()\n",
    "# vectorizer.vocabulary_['<UNK>'] = len(vectorizer.vocabulary_)\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.transform(corpus).toarray())\n",
    "# corpus2 = ['esto es una prueba <UNK>',\n",
    "#          'esto es otra prueba',\n",
    "#          'esto es una prueba mas']\n",
    "# print(vectorizer.transform(corpus2).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'esto': 2, 'es': 0, 'prueba': 4, 'esto es': 3, 'es otra': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [2, 1, 2, 2, 1]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False,max_features=5,ngram_range=(1,2),\n",
    "               token_pattern=r'\\b\\w+\\b')\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of NLPUtils.datasets.utils failed: Traceback (most recent call last):\n",
      "  File \"/home/lestien/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/lestien/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 434, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/lestien/anaconda3/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/lestien/anaconda3/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/lestien/Documents/BecaNLP/Utils/NLPUtils/datasets/utils.py\", line 74\n",
      "    def _build_tokenizer(self,token_pattern):\n",
      "    ^\n",
      "IndentationError: expected an indented block\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-e03b3e7fb78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "dict([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.296125173568726\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import tee, islice\n",
    "import time\n",
    "\n",
    "def ngrams(lst, n):\n",
    "    tlst = lst\n",
    "    while True:\n",
    "        a, b = tee(tlst)\n",
    "        l = tuple(islice(a, n))\n",
    "        if len(l) == n:\n",
    "            yield l\n",
    "            next(b)\n",
    "            tlst = b\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "tokenizer = lambda x: re.findall(r'\\b\\w+\\b',x)\n",
    "corpus = ['esto es una prueba',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas'] * 10000\n",
    "\n",
    "# caso en que vocabulary == None\n",
    "\n",
    "corpus = imdb.train_reader()['comment']\n",
    "\n",
    "# tic = time.time()\n",
    "# tk_to_freq = Counter()\n",
    "# for text in corpus:\n",
    "#     tk_to_freq += Counter(tokenizer(text))\n",
    "# toc = time.time()\n",
    "# #print(tk_to_freq)\n",
    "# print(toc-tic)\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "tk_to_freq = Counter(ngrams([tk for text in corpus for tk in tokenizer(text)],2))\n",
    "toc = time.time()\n",
    "#print(tk_to_freq)\n",
    "print(toc-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.36709547042847\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "tk_to_freq2 = {}\n",
    "for text in corpus:\n",
    "    for tk in ngrams(tokenizer(text),2):\n",
    "        try:\n",
    "            tk_to_freq2[tk] += 1\n",
    "        except KeyError:\n",
    "            tk_to_freq2[tk] = 1\n",
    "toc = time.time()\n",
    "#print(tk_to_freq)\n",
    "print(toc-tic)\n",
    "print(tk_to_freq2 == tk_to_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vocab = defaultdict()\n",
    "vocab.default_factory = vocab.__len__\n",
    "\n",
    "vocab[0] = 0\n",
    "dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9278197288513184\n",
      "{'esto': 0, 'es': 1, 'una': 2, 'prueba': 3, ('esto', 'esto'): 4, ('esto', 'es'): 5, ('es', 'una'): 6, ('una', 'prueba'): 7, 'otra': 8, ('es', 'otra'): 9, ('otra', 'prueba'): 10, 'mas': 11, ('prueba', 'mas'): 12}\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "corpus = ['esto esto es una prueba',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas'] * 10000\n",
    "\n",
    "def ngrams(lst, n):\n",
    "    if n == 1:\n",
    "        for i in lst:\n",
    "            yield i\n",
    "    else:\n",
    "        tlst = lst\n",
    "        for i in range(len(lst)-n+1):\n",
    "            a, b = tee(tlst)\n",
    "            yield tuple(islice(a, n))\n",
    "            next(b)\n",
    "            tlst = b\n",
    "\n",
    "\n",
    "vocabulary = defaultdict()\n",
    "vocabulary.default_factory = vocabulary.__len__\n",
    "unk_token = 'UNK'\n",
    "#vocabulary = defaultdict(None,{'esto':0, 'es': 1, 'prueba':2, unk_token:3})\n",
    "#vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "\n",
    "def tokenizer(doc,vocab):\n",
    "    tokens = re.findall(r'\\b\\w+\\b',doc)\n",
    "    return [tk if tk in vocab else unk_token for tk in tokens]\n",
    "\n",
    "def tokenizer(doc,vocab):\n",
    "    return re.findall(r'\\b\\w+\\b',doc)\n",
    "\n",
    "\n",
    "ngram_range = (1,2)\n",
    "\n",
    "tic = time.time()\n",
    "for doc in corpus:\n",
    "    feature_counter = {}\n",
    "    for i in range(ngram_range[0],ngram_range[1]+1):\n",
    "        tokens = tokenizer(doc,vocabulary)\n",
    "        for feature in ngrams(tokens,i):\n",
    "            try:\n",
    "                feature_idx = vocabulary[feature]\n",
    "            except KeyError:\n",
    "                feature_idx = vocabulary[unk_token]\n",
    "            if feature_idx not in feature_counter:\n",
    "                feature_counter[feature_idx] = 1\n",
    "            else:\n",
    "                feature_counter[feature_idx] += 1\n",
    "            \n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "print(dict(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7]\n",
    "\n",
    "def ngrams(lst, n):\n",
    "    if n == 1:\n",
    "        for i in lst:\n",
    "            yield i\n",
    "    else:\n",
    "        tlst = lst\n",
    "        for i in range(len(lst)-n+1):\n",
    "            a, b = tee(tlst)\n",
    "            yield tuple(islice(a, n))\n",
    "            next(b)\n",
    "            tlst = b\n",
    "\n",
    "b, c = tee(a)\n",
    "list(islice(b,3))\n",
    "list(ngrams(a,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(lst,n):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import array\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import numbers\n",
    "\n",
    "def _make_int_array():\n",
    "    \"\"\"Construct an array.array of a type suitable for scipy.sparse indices.\"\"\"\n",
    "    return array.array(str(\"i\"))\n",
    "\n",
    "\n",
    "class myCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    def __init__(self,token_pattern=r'\\b\\w+\\b',ngram_range=(1,1),min_df=1,max_df=1.0\n",
    "         max_features=None,vocabulary=None,unk_token=None,dtype=np.int):\n",
    "        kwargs = {'input':'content',\n",
    "                  'encoding':'utf-8', \n",
    "                  'decode_error':'strict',\n",
    "                  'strip_accents':None, \n",
    "                  'lowercase':False,\n",
    "                  'preprocessor':None, \n",
    "                  'tokenizer':None\n",
    "                  'stop_words':None,\n",
    "                  'token_pattern':None,\n",
    "                  'ngram_range':ngram_range, \n",
    "                  'analyzer':self.my_analyzer, \n",
    "                  'max_df':max_df, \n",
    "                  'min_df':min_df,\n",
    "                  'max_features':max_features,\n",
    "                  'vocabulary':vocabulary, \n",
    "                  'binary':False, \n",
    "                  'dtype':np.int}\n",
    "        \n",
    "        self.unk_token = unk_token\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def my_analyzer(self,token_pattern,vocabulary):\n",
    "        tokens = re.findall(r'\\b\\w+\\b',doc)\n",
    "        if vocabulary is None:\n",
    "            return tokens\n",
    "        return [tk if tk in vocabulary else self.unk_token for tk in tokens]\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _count_vocab(self, raw_documents, fixed_vocab, unk_token):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n",
    "        \"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        analyze = self.build_analyzer()\n",
    "        j_indices = []\n",
    "        indptr = []\n",
    "\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "        \n",
    "        if unk_token is not None:\n",
    "            for doc in raw_documents:\n",
    "                feature_counter = {}\n",
    "                for feature in analyze(doc,vocabulary):\n",
    "                    try:\n",
    "                        feature_idx = vocabulary[feature]\n",
    "                    except KeyError:\n",
    "                        feature_idx = vocabulary[unk_token]\n",
    "                        \n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                        \n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "        \n",
    "        else:\n",
    "            for doc in raw_documents:\n",
    "                feature_counter = {}\n",
    "                for feature in analyze(doc):\n",
    "                    try:\n",
    "                        feature_idx = vocabulary[feature]\n",
    "                        if feature_idx not in feature_counter:\n",
    "                            feature_counter[feature_idx] = 1\n",
    "                        else:\n",
    "                            feature_counter[feature_idx] += 1\n",
    "                    except KeyError:\n",
    "                        # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                        continue\n",
    "    \n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
    "                                 \" contain stop words\")\n",
    "\n",
    "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "            if _IS_32BIT:\n",
    "                raise ValueError(('sparse CSR array has {} non-zero '\n",
    "                                  'elements and requires 64 bit indexing, '\n",
    "                                  'which is unsupported with 32 bit Python.')\n",
    "                                 .format(indptr[-1]))\n",
    "            indices_dtype = np.int64\n",
    "\n",
    "        else:\n",
    "            indices_dtype = np.int32\n",
    "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "        values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocabulary)),\n",
    "                          dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn the vocabulary dictionary and return document-term matrix.\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array of shape (n_samples, n_features)\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        # We intentionally don't call the transform method to make\n",
    "        # fit_transform overridable without unwanted side effects in\n",
    "        # TfidfVectorizer.\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        self._validate_params()\n",
    "        self._validate_vocabulary()\n",
    "        max_df = self.max_df\n",
    "        min_df = self.min_df\n",
    "        max_features = self.max_features\n",
    "\n",
    "        vocabulary, X = self._count_vocab(raw_documents,\n",
    "                                          self.fixed_vocabulary_,self.unk_token)\n",
    "\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "\n",
    "        if not self.fixed_vocabulary_:\n",
    "            n_doc = X.shape[0]\n",
    "            max_doc_count = (max_df\n",
    "                             if isinstance(max_df, numbers.Integral)\n",
    "                             else max_df * n_doc)\n",
    "            min_doc_count = (min_df\n",
    "                             if isinstance(min_df, numbers.Integral)\n",
    "                             else min_df * n_doc)\n",
    "            if max_doc_count < min_doc_count:\n",
    "                raise ValueError(\n",
    "                    \"max_df corresponds to < documents than min_df\")\n",
    "            X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
    "                                                       max_doc_count,\n",
    "                                                       min_doc_count,\n",
    "                                                       max_features)\n",
    "\n",
    "            X = self._sort_features(X, vocabulary)\n",
    "\n",
    "            self.vocabulary_ = vocabulary\n",
    "\n",
    "        return X\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "        Extract token counts out of raw text documents using the vocabulary\n",
    "        fitted with fit or the one provided to the constructor.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix of shape (n_samples, n_features)\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "        self._check_vocabulary()\n",
    "\n",
    "        # use the same matrix-building strategy as fit_transform\n",
    "        _, X = self._count_vocab(raw_documents, fixed_vocab=True, unk_token=self.unk_token)\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 1, 4: 1, 2: 1, 5: 1, 6: 1, 7: 1, 8: 1}\n",
      "{0: 1, 1: 1, 4: 1, 2: 1, 6: 1, 7: 1, 8: 1}\n",
      "{0: 1, 1: 1, 4: 2, 2: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'esto': 0, 'es': 1, 'prueba': 2, ('una', 'prueba'): 3, 'UNK': 4, ('esto', 'esto'): 5, ('esto', 'es'): 6, ('es', 'UNK'): 7, ('UNK', 'prueba'): 8, ('prueba', 'UNK'): 9}\n",
      "[[2 1 1 0 1 1 1 1 1 0]\n",
      " [1 1 1 0 1 0 1 1 1 0]\n",
      " [1 1 1 0 2 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from itertools import tee,islice\n",
    "from NLPUtils.datasets import imdb\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def ngrams(lst, n):\n",
    "    if n == 1:\n",
    "        for i in lst:\n",
    "            yield i\n",
    "    else:\n",
    "        tlst = lst\n",
    "        for i in range(len(lst)-n+1):\n",
    "            a, b = tee(tlst)\n",
    "            yield tuple(islice(a, n))\n",
    "            next(b)\n",
    "            tlst = b\n",
    "\n",
    "\n",
    "class myCountVectorizer(object):\n",
    "    \n",
    "    def __init__(self,token_pattern=r'\\bw+\\b',ngram_range=(1,1),\n",
    "     vocabulary=None,unk_token=None,min_df=1,max_df=1.0,dtype=np.int64):\n",
    "        \n",
    "        self.token_pattern = token_pattern\n",
    "        self.ngram_range = ngram_range\n",
    "        self.unk_token = unk_token\n",
    "        self.vocabulary = vocabulary\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def tokenizer(self,doc,token_pattern,vocabulary,fixed_vocab):\n",
    "        tokens = re.findall(r'\\b\\w+\\b',doc)\n",
    "        unk_token = self.unk_token\n",
    "        if not fixed_vocab:\n",
    "            return tokens\n",
    "        return [tk if tk in vocabulary else unk_token for tk in tokens]\n",
    "        \n",
    "        \n",
    "    def _count_vocab(self,corpus,fixed_vocab):\n",
    "        \n",
    "        vocab = defaultdict(None,self.vocabulary) if fixed_vocab else defaultdict()\n",
    "        vocab.default_factory = vocab.__len__\n",
    "        \n",
    "        min_ngram, max_ngram = self.ngram_range\n",
    "        tokenizer = self.tokenizer\n",
    "        token_pattern = self.token_pattern\n",
    "        unk_token = self.unk_token\n",
    "        \n",
    "        indptr = [0]\n",
    "        j_indices = []\n",
    "        values = []\n",
    "        \n",
    "        for doc in corpus:\n",
    "            feature_counter = {}\n",
    "            for i in range(min_ngram,max_ngram+1):\n",
    "                tokens = tokenizer(doc,token_pattern,vocab,fixed_vocab)\n",
    "                for feature in ngrams(tokens,i):\n",
    "                    try:\n",
    "                        feature_idx = vocab[feature]\n",
    "                    except KeyError:\n",
    "                        feature_idx = vocab[unk_token]\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "            print(feature_counter)\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "               \n",
    "        vocab = dict(vocab)\n",
    "        if unk_token not in vocab and unk_token is not None:\n",
    "            vocab[unk_token] = len(vocab)\n",
    "                \n",
    "        j_indices = np.asarray(j_indices, dtype=self.dtype)\n",
    "        indptr = np.asarray(indptr, dtype=self.dtype)\n",
    "        values = np.asarray(values, dtype=self.dtype)\n",
    "        \n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocab)),\n",
    "                          dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "        \n",
    "        true_indices = [vocab[ngram] for ngram in vocab if ngram is not None and (False if isinstance(ngram,tuple) and None in ngram else True)]\n",
    "        vocab = {tk:idx for idx, tk in enumerate(vocab) if idx in true_indices}\n",
    "        X = X[:,true_indices]\n",
    "        \n",
    "        return X, vocab\n",
    "        \n",
    "    def fit(self,corpus):\n",
    "        \n",
    "        fixed_vocab = False if self.vocabulary is None else True\n",
    "        X, vocab = self._count_vocab(corpus,fixed_vocab)\n",
    "        self.vocabulary = vocab\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self,corpus):\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "corpus = ['esto esto es una prueba',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas']\n",
    "\n",
    "vocabulary = {'esto':0, 'es':1, 'prueba':2}\n",
    "#vocabulary = None\n",
    "vectorizer = myCountVectorizer(token_pattern=r'\\bw+\\b',ngram_range=(1,2),\n",
    "             vocabulary=vocabulary,unk_token='UNK',min_df=1,max_df=1.0)\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'otra': 0, 'mas': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from itertools import tee,islice\n",
    "from NLPUtils.datasets import imdb\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def ngrams(lst, n):\n",
    "    if n == 1:\n",
    "        for i in lst:\n",
    "            yield i\n",
    "    else:\n",
    "        tlst = lst\n",
    "        for i in range(len(lst)-n+1):\n",
    "            a, b = tee(tlst)\n",
    "            yield tuple(islice(a, n))\n",
    "            next(b)\n",
    "            tlst = b\n",
    "\n",
    "\n",
    "class myCountVectorizer(object):\n",
    "    \n",
    "    def __init__(self,token_pattern=r'\\bw+\\b',ngram_range=(1,1),\n",
    "     vocabulary=None,unk_token=None,min_freq=1,max_freq=np.inf,dtype=np.int64):\n",
    "        \n",
    "        self.token_pattern = token_pattern\n",
    "        self.ngram_range = ngram_range\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        if isinstance(vocabulary,list) or isinstance(vocabulary,set) or hasattr(vocabulary,'__iter__'):\n",
    "            self.vocabulary = {tk: idx for idx, tk in enumerate(vocabulary)}\n",
    "        elif isinstance(vocabulary,dict) or vocabulary is None:\n",
    "            self.vocabulary = vocabulary\n",
    "        else:\n",
    "            raise TypeError('El vocabulario tiene que ser un iterable de palabras')\n",
    "            \n",
    "        self.dtype = dtype\n",
    "        self.min_freq, self.max_freq = min_freq, max_freq\n",
    "        \n",
    "    def tokenizer(self,doc,token_pattern,vocabulary,fixed_vocab,ngram_range):\n",
    "        tokens = re.findall(r'\\b\\w+\\b',doc)\n",
    "        unk_token = self.unk_token\n",
    "        min_ngram, max_ngram = ngram_range\n",
    "        \n",
    "        if not fixed_vocab:\n",
    "            return [tk for i in range(min_ngram,max_ngram+1) for tk in ngrams(tokens,i)]\n",
    "            \n",
    "        return [tk if all([sub_tk in vocabulary for sub_tk in tk]) or tk in vocabulary else unk_token for i in range(min_ngram,max_ngram+1) for tk in ngrams(tokens,i)]\n",
    "        \n",
    "        \n",
    "    def _count_vocab(self,corpus,fixed_vocab):\n",
    "        \n",
    "        vocab = defaultdict(None,self.vocabulary) if fixed_vocab else defaultdict()\n",
    "        vocab.default_factory = vocab.__len__\n",
    "        \n",
    "        ngram_range = self.ngram_range\n",
    "        tokenizer = self.tokenizer\n",
    "        token_pattern = self.token_pattern\n",
    "        unk_token = self.unk_token\n",
    "        \n",
    "        indptr = [0]\n",
    "        j_indices = []\n",
    "        values = []\n",
    "        \n",
    "        for doc in corpus:\n",
    "            \n",
    "            feature_counter = {}\n",
    "            tokens = tokenizer(doc,token_pattern,vocab,fixed_vocab,ngram_range)\n",
    "            for feature in tokens:\n",
    "                try:\n",
    "                    feature_idx = vocab[feature]\n",
    "                except KeyError:\n",
    "                    feature_idx = vocab[unk_token]\n",
    "                if feature_idx not in feature_counter:\n",
    "                    feature_counter[feature_idx] = 1\n",
    "                else:\n",
    "                    feature_counter[feature_idx] += 1\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "               \n",
    "        vocab = dict(vocab)\n",
    "        if unk_token not in vocab and unk_token is not None:\n",
    "            vocab[unk_token] = len(vocab)\n",
    "                \n",
    "        j_indices = np.asarray(j_indices, dtype=self.dtype)\n",
    "        indptr = np.asarray(indptr, dtype=self.dtype)\n",
    "        values = np.asarray(values, dtype=self.dtype)\n",
    "        \n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocab)),\n",
    "                          dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "        \n",
    "        if self.max_freq < np.inf or self.min_freq > 0:\n",
    "            sums = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "            x1 = sums >= self.min_freq\n",
    "            x2 = sums <= self.max_freq\n",
    "            ands = np.logical_and(x1,x2)\n",
    "            true_indices = list(np.where(ands)[0])\n",
    "            if unk_token is not None:\n",
    "                unk_idx = vocab[unk_token]\n",
    "                if unk_idx not in true_indices:\n",
    "                    true_indices.append(unk_idx)\n",
    "\n",
    "        if unk_token is None and unk_token in vocab.keys():\n",
    "            unk_idx = vocab[unk_token] \n",
    "            if unk_idx in true_indices:\n",
    "                true_indices.remove(unk_idx)\n",
    "        \n",
    "        true_indices = sorted(true_indices)\n",
    "        X = X[:,true_indices]\n",
    "        tokens = list(vocab.keys())\n",
    "        vocab = {tokens[i]:idx for idx, i in enumerate(true_indices)}\n",
    "        \n",
    "        return X, vocab\n",
    "        \n",
    "    def fit(self,corpus):\n",
    "        \n",
    "        fixed_vocab = False if self.vocabulary is None else True\n",
    "        X, vocab = self._count_vocab(corpus,fixed_vocab)\n",
    "        self.vocabulary = vocab\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self,corpus):\n",
    "        \n",
    "        token_pattern = self.token_pattern\n",
    "        vocab = self.vocabulary\n",
    "        ngram_range = self.ngram_range\n",
    "        unk_token = self.unk_token\n",
    "        \n",
    "        indptr = [0]\n",
    "        j_indices = []\n",
    "        values = []\n",
    "        \n",
    "        if unk_token is None:\n",
    "            for doc in corpus:\n",
    "                feature_counter = {}\n",
    "                tokens = self.tokenizer(doc,token_pattern,vocab,True,ngram_range)\n",
    "                for feature in tokens:\n",
    "                    try:\n",
    "                        feature_idx = vocab[feature]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "        else:\n",
    "            for doc in corpus:\n",
    "                feature_counter = {}\n",
    "                tokens = self.tokenizer(doc,token_pattern,vocab,True,ngram_range)\n",
    "                for feature in tokens:\n",
    "                    try:\n",
    "                        feature_idx = vocab[feature]\n",
    "                    except KeyError:\n",
    "                        feature_idx = vocab[unk_token]\n",
    "                    if feature_idx not in feature_counter:\n",
    "                        feature_counter[feature_idx] = 1\n",
    "                    else:\n",
    "                        feature_counter[feature_idx] += 1\n",
    "                j_indices.extend(feature_counter.keys())\n",
    "                values.extend(feature_counter.values())\n",
    "                indptr.append(len(j_indices))\n",
    "            \n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocab)),\n",
    "                          dtype=self.dtype)\n",
    "        X.sort_indices()\n",
    "        \n",
    "        return X\n",
    "\n",
    "corpus_train = ['esto esto es una prueba',\n",
    "         'esto es otra prueba',\n",
    "         'esto es una prueba mas']\n",
    "\n",
    "corpus_dev = ['esto es una prueba para dev',\n",
    "             'esto también',\n",
    "             'esto es otra prueba y esto es una prueba más']\n",
    "\n",
    "#corpus = imdb.train_reader()['comment']\n",
    "#vocabulary = {'esto':0, 'es':1, ('una','prueba'):2, ('esto','es','una'):3}\n",
    "vocabulary = None\n",
    "vectorizer = myCountVectorizer(token_pattern=r'\\bw+\\b',ngram_range=(1,1),\n",
    "             vocabulary=vocabulary,unk_token=None,min_freq=1,max_freq=1)\n",
    "\n",
    "X_train = vectorizer.fit(corpus_train)\n",
    "X_dev = vectorizer.transform(corpus_dev)\n",
    "print(vectorizer.vocabulary)\n",
    "X_dev.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lógica es esta:\n",
    "\n",
    "* Yo puedo pedir deducir el vocabulario a partir de un corpus. Cuando hago esto, puedo pedir que el programa cuente n-gramas y que se quede con los que tienen una frecuencia dada. En este caso, poner unk_token=None o unk_token='UNK' lo único que cambia es que el token forme parte o no del vocabulario para futuras vectorizaciones. Además, aunque ponga una frecuencia máxima y el unk_token tenga más de esa cuenta, no se elimina del vocabulario. Es decir, se prioriza que haya unk_token a que se elimine esa cantidad, para después trabajar en el dev ser.\n",
    "\n",
    "* También puedo dar un vocabulario cerrado y pedir que se agregue un unk_token opcional."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
